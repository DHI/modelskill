[
  {
    "objectID": "user-guide/plotting.html",
    "href": "user-guide/plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "PointObservations and PointModelResults can be plotted using their plot accessor:\n&gt;&gt;&gt; o.plot.timeseries()\n&gt;&gt;&gt; mr.plot.timeseries()\n&gt;&gt;&gt; mr.plot.hist()\nOnly the observation time series is shown here:\n\n\n\nTimeseries plot\n\n\n\n\n\nThe temporal coverage of observations and model results can be plotted using the temporal_coverage function in the plotting module:\n&gt;&gt;&gt; o1 = ms.PointObservation('HKNA.dfs0', item=0, x=4.2420, y=52.6887)\n&gt;&gt;&gt; o2 = ms.PointObservation('EPL.dfs0', item=0, x=3.2760, y=51.9990)\n&gt;&gt;&gt; o3 = ms.TrackObservation(\"Alti_c2.dfs0\", item=3)\n&gt;&gt;&gt; mr = ms.DfsuModelResult('HKZN_local.dfsu', item=0)\n&gt;&gt;&gt; ms.plotting.temporal_coverage(obs=[o1, o2, o3], mod=mr)\n\n\n\nTemporal coverage\n\n\n\n\n\nThe spatial coverage of observations and model results can be plotted using the spatial_overview function in the plotting module:\n&gt;&gt;&gt; ms.plotting.spatial_overview([o1, o2, o3], mr)\n\n\n\nSpatial overview\n\n\n\n\n\nThe plot accessor on a Comparer or ComparerCollection object can be used to plot the compared data:\n&gt;&gt;&gt; cmp.plot.timeseries()\n&gt;&gt;&gt; cc.plot.timeseries()\n&gt;&gt;&gt; cc.plot.scatter()\n\n\n\nA Taylor diagram shows how well a model result matches an observation in terms of correlation, standard deviation and root mean square error. The taylor plot can be accessed through the Comparer plot accessor or the ComparerCollection plot accessor:\n&gt;&gt;&gt; cc = ms.match([o1, o2, o3], [mr_CMEMS, mr_ERA5, mr_MIKE21SW])\n&gt;&gt;&gt; cc.plot.taylor()\n\n\n\nTaylor diagram\n\n\nThe radial distance from the point to the observation point is the standard deviation ratio, the angle is the correlation coefficient and the distance from the observation point to the model point is the root mean square error ratio. The closer the model point is to the observation point, the better the model result matches the observation. The closer the model point is to the origin, the better the model result matches the observation in terms of standard deviation and root mean square error. The closer the model point is to the horizontal axis, the better the model result matches the observation in terms of correlation.\n\n\n\nDirectional data can be plotted using the wind_rose function in the plotting module. The function takes an array-like structure with speed and direction as columns (from one or two sources) and plots a wind rose:\n&gt;&gt;&gt; df = pd.read_csv('wind.csv', index_col=0, parse_dates=True)\n&gt;&gt;&gt; ms.plotting.wind_rose(df)\n\n\n\nWind rose"
  },
  {
    "objectID": "user-guide/plotting.html#plotting-observations-and-model-results",
    "href": "user-guide/plotting.html#plotting-observations-and-model-results",
    "title": "Plotting",
    "section": "",
    "text": "PointObservations and PointModelResults can be plotted using their plot accessor:\n&gt;&gt;&gt; o.plot.timeseries()\n&gt;&gt;&gt; mr.plot.timeseries()\n&gt;&gt;&gt; mr.plot.hist()\nOnly the observation time series is shown here:\n\n\n\nTimeseries plot"
  },
  {
    "objectID": "user-guide/plotting.html#plotting-temporal-coverage",
    "href": "user-guide/plotting.html#plotting-temporal-coverage",
    "title": "Plotting",
    "section": "",
    "text": "The temporal coverage of observations and model results can be plotted using the temporal_coverage function in the plotting module:\n&gt;&gt;&gt; o1 = ms.PointObservation('HKNA.dfs0', item=0, x=4.2420, y=52.6887)\n&gt;&gt;&gt; o2 = ms.PointObservation('EPL.dfs0', item=0, x=3.2760, y=51.9990)\n&gt;&gt;&gt; o3 = ms.TrackObservation(\"Alti_c2.dfs0\", item=3)\n&gt;&gt;&gt; mr = ms.DfsuModelResult('HKZN_local.dfsu', item=0)\n&gt;&gt;&gt; ms.plotting.temporal_coverage(obs=[o1, o2, o3], mod=mr)\n\n\n\nTemporal coverage"
  },
  {
    "objectID": "user-guide/plotting.html#plotting-spatial-overview",
    "href": "user-guide/plotting.html#plotting-spatial-overview",
    "title": "Plotting",
    "section": "",
    "text": "The spatial coverage of observations and model results can be plotted using the spatial_overview function in the plotting module:\n&gt;&gt;&gt; ms.plotting.spatial_overview([o1, o2, o3], mr)\n\n\n\nSpatial overview"
  },
  {
    "objectID": "user-guide/plotting.html#plotting-compared-data",
    "href": "user-guide/plotting.html#plotting-compared-data",
    "title": "Plotting",
    "section": "",
    "text": "The plot accessor on a Comparer or ComparerCollection object can be used to plot the compared data:\n&gt;&gt;&gt; cmp.plot.timeseries()\n&gt;&gt;&gt; cc.plot.timeseries()\n&gt;&gt;&gt; cc.plot.scatter()"
  },
  {
    "objectID": "user-guide/plotting.html#plotting-taylor-diagrams",
    "href": "user-guide/plotting.html#plotting-taylor-diagrams",
    "title": "Plotting",
    "section": "",
    "text": "A Taylor diagram shows how well a model result matches an observation in terms of correlation, standard deviation and root mean square error. The taylor plot can be accessed through the Comparer plot accessor or the ComparerCollection plot accessor:\n&gt;&gt;&gt; cc = ms.match([o1, o2, o3], [mr_CMEMS, mr_ERA5, mr_MIKE21SW])\n&gt;&gt;&gt; cc.plot.taylor()\n\n\n\nTaylor diagram\n\n\nThe radial distance from the point to the observation point is the standard deviation ratio, the angle is the correlation coefficient and the distance from the observation point to the model point is the root mean square error ratio. The closer the model point is to the observation point, the better the model result matches the observation. The closer the model point is to the origin, the better the model result matches the observation in terms of standard deviation and root mean square error. The closer the model point is to the horizontal axis, the better the model result matches the observation in terms of correlation."
  },
  {
    "objectID": "user-guide/plotting.html#plotting-directional-data-e.g.-wind-or-currents",
    "href": "user-guide/plotting.html#plotting-directional-data-e.g.-wind-or-currents",
    "title": "Plotting",
    "section": "",
    "text": "Directional data can be plotted using the wind_rose function in the plotting module. The function takes an array-like structure with speed and direction as columns (from one or two sources) and plots a wind rose:\n&gt;&gt;&gt; df = pd.read_csv('wind.csv', index_col=0, parse_dates=True)\n&gt;&gt;&gt; ms.plotting.wind_rose(df)\n\n\n\nWind rose"
  },
  {
    "objectID": "user-guide/selecting-data.html",
    "href": "user-guide/selecting-data.html",
    "title": "Selecting/filtering data",
    "section": "",
    "text": "The primary data filtering method of ModelSkill is the sel() method which is accesible on most ModelSkill data structures. The sel() method is a wrapper around xarray.Dataset.sel() and can be used to select data based on time, location and/or variable. The sel() method returns a new data structure of the same type with the selected data.\n\n\nPoint and track timeseries data of both observation and model result kinds are stored in TimeSeries objects which uses xarray.Dataset as data container. The sel() method can be used to select data based on time and returns a new TimeSeries object with the selected data.\n&gt;&gt;&gt; o = ms.observation('obs.nc', item='waterlevel')\n&gt;&gt;&gt; o_1month = o.sel(time=slice('2018-01-01', '2018-02-01'))\n\n\n\nComparer and ComparerCollection contain matched data from observations and model results. The sel() method can be used to select data based on time, model, quantity or other criteria and returns a new comparer object with the selected data.\n&gt;&gt;&gt; cmp = ms.match(o, [m1, m2])\n&gt;&gt;&gt; cmp_1month = cmp.sel(time=slice('2018-01-01', '2018-02-01'))\n&gt;&gt;&gt; cmp_m1 = cmp.sel(model='m1')\n\n\n\nThe skill() and mean_skill() methods return a SkillTable object with skill scores from comparing observation and model result data using different metrics (e.g. root mean square error). The data of the SkillTable object is stored in a (MultiIndex) pandas.DataFrame which can be accessed via the data attribute. The sel() method can be used to select specific rows and returns a new SkillTable object with the selected data.\n&gt;&gt;&gt; sk = cmp.skill()\n&gt;&gt;&gt; sk_m1 = sk.sel(model='m1')",
    "crumbs": [
      "Home",
      "User Guide",
      "Selecting/filtering data"
    ]
  },
  {
    "objectID": "user-guide/selecting-data.html#timeseries-data",
    "href": "user-guide/selecting-data.html#timeseries-data",
    "title": "Selecting/filtering data",
    "section": "",
    "text": "Point and track timeseries data of both observation and model result kinds are stored in TimeSeries objects which uses xarray.Dataset as data container. The sel() method can be used to select data based on time and returns a new TimeSeries object with the selected data.\n&gt;&gt;&gt; o = ms.observation('obs.nc', item='waterlevel')\n&gt;&gt;&gt; o_1month = o.sel(time=slice('2018-01-01', '2018-02-01'))",
    "crumbs": [
      "Home",
      "User Guide",
      "Selecting/filtering data"
    ]
  },
  {
    "objectID": "user-guide/selecting-data.html#comparer-objects",
    "href": "user-guide/selecting-data.html#comparer-objects",
    "title": "Selecting/filtering data",
    "section": "",
    "text": "Comparer and ComparerCollection contain matched data from observations and model results. The sel() method can be used to select data based on time, model, quantity or other criteria and returns a new comparer object with the selected data.\n&gt;&gt;&gt; cmp = ms.match(o, [m1, m2])\n&gt;&gt;&gt; cmp_1month = cmp.sel(time=slice('2018-01-01', '2018-02-01'))\n&gt;&gt;&gt; cmp_m1 = cmp.sel(model='m1')",
    "crumbs": [
      "Home",
      "User Guide",
      "Selecting/filtering data"
    ]
  },
  {
    "objectID": "user-guide/selecting-data.html#skill-objects",
    "href": "user-guide/selecting-data.html#skill-objects",
    "title": "Selecting/filtering data",
    "section": "",
    "text": "The skill() and mean_skill() methods return a SkillTable object with skill scores from comparing observation and model result data using different metrics (e.g. root mean square error). The data of the SkillTable object is stored in a (MultiIndex) pandas.DataFrame which can be accessed via the data attribute. The sel() method can be used to select specific rows and returns a new SkillTable object with the selected data.\n&gt;&gt;&gt; sk = cmp.skill()\n&gt;&gt;&gt; sk_m1 = sk.sel(model='m1')",
    "crumbs": [
      "Home",
      "User Guide",
      "Selecting/filtering data"
    ]
  },
  {
    "objectID": "user-guide/skill.html",
    "href": "user-guide/skill.html",
    "title": "Skill",
    "section": "",
    "text": "Skill\nMatched data can be analysed statistically using the skill() function. The function returns a Skill object which contains the statistical results. The Skill object can be printed to the console or saved to a file using the save() function.\n```python"
  },
  {
    "objectID": "user-guide/matching.html",
    "href": "user-guide/matching.html",
    "title": "Matching",
    "section": "",
    "text": "Once observations and model results have been defined, the next step is to match them. This is done using the match() function which handles the allignment of the observation and model result data in space and time. Note that if the data is already matched, the from_matched() function can be used to create a Comparer directly from the matched data and the matching described here is not needed.\nThe observation is considered the truth and the model result data is therefore interpolated to the observation data positions.\nThe matching process will be different depending on the geometry of observation and model result:\n\nGeometries are the same (e.g. both are point time series): only temporal matching is needed\nGeometries are different (e.g. observation is a point time series and model result is a grid): data is first spatially extracted from the model result and then matched in time.\n\n\n\nTemporal matching is done by interpolating the model result data to the observation data time points; it is carried out after spatial matching when applicable. The interpolation is linear in time and done inside the match() function.\n\n\n\nIf observation and model result are of the same geometry, the matching is done one observation at a time. Several model results can be matched to the same observation. The result of the matching process is a Comparer object which contains the matched data.\nIn the most simple cases, one observation to one model result, the match() function can be used directly, without creating Observation and ModelResult objects first:\n&gt;&gt;&gt; cmp = ms.match('obs.dfs0', 'model.dfs0', obs_item='obs_WL', mod_item='WL')\nIn all other cases, the observations and model results needs to be defined first.\n&gt;&gt;&gt; o = ms.observation('obs.dfs0', item='waterlevel')\n&gt;&gt;&gt; mr1 = ms.model_result('model1.dfs0', item='WL1')\n&gt;&gt;&gt; mr2 = ms.model_result('model2.dfs0', item='WL2')\n&gt;&gt;&gt; cmp = ms.match(o, [mr1, mr2])\nIn most cases, several observations needs to matched with several model results. This can be done by constructing a list of Comparer objects and then combining them into a ComparerCollection:\n&gt;&gt;&gt; cmps = []\n&gt;&gt;&gt; for o in observations:\n&gt;&gt;&gt;     mr1 = ...\n&gt;&gt;&gt;     mr2 = ...\n&gt;&gt;&gt;     cmps.append(ms.match(o, [mr1, mr2]))\n&gt;&gt;&gt; cc = ms.ComparerCollection(cmps)\n\n\n\nIf the model result is a SpatialField, i.e., either a GridModelResult or a DfsuModelResult, and the observation is of lower dimension (e.g. point), then the model result needs to be extracted before matching can be done. This can be done “offline” before using ModelSkill, e.g., using MIKE tools or MIKE IO, or as part of the matching process using ModelSkill. We will here focus on the latter.\nIn this situation, multiple observations can be matched to the same model result, in which case the match function returns a ComparerCollection instead of a Comparer which is the returned object for single observation matching.\n&gt;&gt;&gt; o1 = ms.observation('obs1.dfs0', item='waterlevel')\n&gt;&gt;&gt; o2 = ms.observation('obs2.dfs0', item='waterlevel')\n&gt;&gt;&gt; mr = ms.model_result('model.dfsu', item='WaterLevel')\n&gt;&gt;&gt; cc = ms.match([o1, o2], mr)   # returns a ComparerCollection\nMatching PointObservation with SpatialField model results consists of two steps:\n\nExtracting data from the model result at the spatial position of the observation, which returns a PointModelResult\nMatching the extracted data with the observation data in time\n\nMatching TrackObservation with SpatialField model results is for technical reasons handled in one step, i.e., the data is extracted in both space and time.\nThe spatial matching method (selection or interpolation) can be specified using the spatial_method argument of the match() function. The default method depends on the type of observation and model result as specified in the sections below.\n\n\nExtracting data for a specific point position from the flexible mesh dfsu files can be done in several ways (specified by the spatial_method argument of the match() function):\n\nSelection of the “contained” element\nSelection of the “nearest” element (often the same as the contained element, but not always)\nInterpolation with “inverse_distance” weighting (IDW) using the five nearest elements (default)\n\nThe default (inverse_distance) is not necessarily the best method in all cases. When the extracted position is close to the model boundary, “contained” may be a better choice.\n&gt;&gt;&gt; cc = ms.match([o1, o2], mr_dfsu, spatial_method='contained')   \nNote that extraction of track data does not currently support the “contained” method.\nNote that the extraction of point data from 3D dfsu files is not yet fully supported. It is recommended to extract the data “offline” prior to using ModelSkill.\n\n\n\nExtracting data from a GridModelResult is done through xarray’s interp() function. The spatial_method argument of the match() function is passed on to the interp() function as the method argument. The default method is “linear” which is the recommended method for most cases. Close to land where the grid model result data is often missing, “nearest” may be a better choice.\n&gt;&gt;&gt; cc = ms.match([o1, o2], mr_netcdf, spatial_method='nearest')   \n\n\n\n\nIf the model result data contains gaps either because only events are stored or because of missing data, the max_model_gap argument of the match() function can be used to specify the maximum allowed gap (in seconds) in the model result data. This will avoid interpolating model data over long gaps in the model result data!\n\n\n\nIf the model results have different temporal coverage, the match() function will only match the overlapping time period to ensure that the model results are comparable. The Comparer object will contain the matched data for the overlapping period only."
  },
  {
    "objectID": "user-guide/matching.html#temporal-matching",
    "href": "user-guide/matching.html#temporal-matching",
    "title": "Matching",
    "section": "",
    "text": "Temporal matching is done by interpolating the model result data to the observation data time points; it is carried out after spatial matching when applicable. The interpolation is linear in time and done inside the match() function."
  },
  {
    "objectID": "user-guide/matching.html#matching-of-time-series",
    "href": "user-guide/matching.html#matching-of-time-series",
    "title": "Matching",
    "section": "",
    "text": "If observation and model result are of the same geometry, the matching is done one observation at a time. Several model results can be matched to the same observation. The result of the matching process is a Comparer object which contains the matched data.\nIn the most simple cases, one observation to one model result, the match() function can be used directly, without creating Observation and ModelResult objects first:\n&gt;&gt;&gt; cmp = ms.match('obs.dfs0', 'model.dfs0', obs_item='obs_WL', mod_item='WL')\nIn all other cases, the observations and model results needs to be defined first.\n&gt;&gt;&gt; o = ms.observation('obs.dfs0', item='waterlevel')\n&gt;&gt;&gt; mr1 = ms.model_result('model1.dfs0', item='WL1')\n&gt;&gt;&gt; mr2 = ms.model_result('model2.dfs0', item='WL2')\n&gt;&gt;&gt; cmp = ms.match(o, [mr1, mr2])\nIn most cases, several observations needs to matched with several model results. This can be done by constructing a list of Comparer objects and then combining them into a ComparerCollection:\n&gt;&gt;&gt; cmps = []\n&gt;&gt;&gt; for o in observations:\n&gt;&gt;&gt;     mr1 = ...\n&gt;&gt;&gt;     mr2 = ...\n&gt;&gt;&gt;     cmps.append(ms.match(o, [mr1, mr2]))\n&gt;&gt;&gt; cc = ms.ComparerCollection(cmps)"
  },
  {
    "objectID": "user-guide/matching.html#matching-with-dfsu-or-grid-model-result",
    "href": "user-guide/matching.html#matching-with-dfsu-or-grid-model-result",
    "title": "Matching",
    "section": "",
    "text": "If the model result is a SpatialField, i.e., either a GridModelResult or a DfsuModelResult, and the observation is of lower dimension (e.g. point), then the model result needs to be extracted before matching can be done. This can be done “offline” before using ModelSkill, e.g., using MIKE tools or MIKE IO, or as part of the matching process using ModelSkill. We will here focus on the latter.\nIn this situation, multiple observations can be matched to the same model result, in which case the match function returns a ComparerCollection instead of a Comparer which is the returned object for single observation matching.\n&gt;&gt;&gt; o1 = ms.observation('obs1.dfs0', item='waterlevel')\n&gt;&gt;&gt; o2 = ms.observation('obs2.dfs0', item='waterlevel')\n&gt;&gt;&gt; mr = ms.model_result('model.dfsu', item='WaterLevel')\n&gt;&gt;&gt; cc = ms.match([o1, o2], mr)   # returns a ComparerCollection\nMatching PointObservation with SpatialField model results consists of two steps:\n\nExtracting data from the model result at the spatial position of the observation, which returns a PointModelResult\nMatching the extracted data with the observation data in time\n\nMatching TrackObservation with SpatialField model results is for technical reasons handled in one step, i.e., the data is extracted in both space and time.\nThe spatial matching method (selection or interpolation) can be specified using the spatial_method argument of the match() function. The default method depends on the type of observation and model result as specified in the sections below.\n\n\nExtracting data for a specific point position from the flexible mesh dfsu files can be done in several ways (specified by the spatial_method argument of the match() function):\n\nSelection of the “contained” element\nSelection of the “nearest” element (often the same as the contained element, but not always)\nInterpolation with “inverse_distance” weighting (IDW) using the five nearest elements (default)\n\nThe default (inverse_distance) is not necessarily the best method in all cases. When the extracted position is close to the model boundary, “contained” may be a better choice.\n&gt;&gt;&gt; cc = ms.match([o1, o2], mr_dfsu, spatial_method='contained')   \nNote that extraction of track data does not currently support the “contained” method.\nNote that the extraction of point data from 3D dfsu files is not yet fully supported. It is recommended to extract the data “offline” prior to using ModelSkill.\n\n\n\nExtracting data from a GridModelResult is done through xarray’s interp() function. The spatial_method argument of the match() function is passed on to the interp() function as the method argument. The default method is “linear” which is the recommended method for most cases. Close to land where the grid model result data is often missing, “nearest” may be a better choice.\n&gt;&gt;&gt; cc = ms.match([o1, o2], mr_netcdf, spatial_method='nearest')"
  },
  {
    "objectID": "user-guide/matching.html#event-based-matching-and-handling-of-gaps",
    "href": "user-guide/matching.html#event-based-matching-and-handling-of-gaps",
    "title": "Matching",
    "section": "",
    "text": "If the model result data contains gaps either because only events are stored or because of missing data, the max_model_gap argument of the match() function can be used to specify the maximum allowed gap (in seconds) in the model result data. This will avoid interpolating model data over long gaps in the model result data!"
  },
  {
    "objectID": "user-guide/matching.html#multiple-model-results-with-different-temporal-coverage",
    "href": "user-guide/matching.html#multiple-model-results-with-different-temporal-coverage",
    "title": "Matching",
    "section": "",
    "text": "If the model results have different temporal coverage, the match() function will only match the overlapping time period to ensure that the model results are comparable. The Comparer object will contain the matched data for the overlapping period only."
  },
  {
    "objectID": "user-guide/getting-started.html",
    "href": "user-guide/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "This page describes the typical ModelSkill workflow for comparing model results and observations.\n\n\nThe typical ModelSkill workflow consists of these four steps:\n\nDefine Observations\nDefine ModelResults\nMatch observations and ModelResults in space and time\nDo analysis, plotting, etc with a Comparer\n\n\n\nThe first step is to define the measurements to be used for the skill assessment. Two types of observation are available:\n\nPointObservation\nTrackObservation\n\nLet’s assume that we have one PointObservation and one TrackObservation (name is used to identify the observation, similar to the name of the model above).\npython hl_lines=\"3 5\" hkna = ms.PointObservation(\"HKNA_Hm0.dfs0\", item=0,                             x=4.2420, y=52.6887,                             name=\"HKNA\") c2 = ms.TrackObservation(\"Alti_c2_Dutch.dfs0\", item=3,                           name=\"c2\")\nIn this case both observations are provided as .dfs0 files but pandas dataframes are also supported in case data are stored in another file format.\nBoth PointObservation and TrackObservation need the path of the data file, the item number (or item name) and a name. A PointObservation further needs to be initialized with it's x-, y-position.\n\n\n\nThe result of a simulation is stored in one or more result files, e.g. dfsu, dfs0, nc, csv.\nThe name is used to identify the model result in the plots and tables.\npython hl_lines=\"4\" import modelskill as ms mr = ms.DfsuModelResult(\"SW/HKZN_local_2017_DutchCoast.dfsu\",                           item=\"Sign. Wave Height\",                          name='HKZN_local')\n\n\n\nThis match() method returns a Comparer (a single observation) or a ComparerCollection (multiple observations) for further analysis and plotting.\ncc = ms.match([hkna, c2], mr)\n\n\n\nThe object returned by the match() method is a Comparer/ComparerCollection. It holds the matched observation and model data and has methods for plotting and skill assessment.\nThe primary comparer methods are:\n\nskill() which returns a SkillTable with the skill scores\nvarious plot methods of the comparer objects (e.g. plot.scatter(), plot.timeseries())\nsel() method for selecting data\n\n\n\n\nIt can be useful to save the comparer collection for later use. This can be done using the save() method:\ncc.save(\"my_comparer_collection.msk\")\nThe comparer collection can be loaded again from disk, using the load() method:\ncc = ms.load(\"my_comparer_collection.msk\")\n\n\n\nIn order to select only a subset of the data for analysis, the comparer has a sel() method which returns a new comparer with the selected data.\nThis method allow filtering of the data in several ways:\n\non observation by specifying name or index of one or more observations\non model (if more than one is compared) by giving name or index\ntemporal using the time (or start and end) arguments\nspatial using the area argument given as a bounding box or a polygon"
  },
  {
    "objectID": "user-guide/getting-started.html#workflow",
    "href": "user-guide/getting-started.html#workflow",
    "title": "Getting started",
    "section": "",
    "text": "The typical ModelSkill workflow consists of these four steps:\n\nDefine Observations\nDefine ModelResults\nMatch observations and ModelResults in space and time\nDo analysis, plotting, etc with a Comparer\n\n\n\nThe first step is to define the measurements to be used for the skill assessment. Two types of observation are available:\n\nPointObservation\nTrackObservation\n\nLet’s assume that we have one PointObservation and one TrackObservation (name is used to identify the observation, similar to the name of the model above).\npython hl_lines=\"3 5\" hkna = ms.PointObservation(\"HKNA_Hm0.dfs0\", item=0,                             x=4.2420, y=52.6887,                             name=\"HKNA\") c2 = ms.TrackObservation(\"Alti_c2_Dutch.dfs0\", item=3,                           name=\"c2\")\nIn this case both observations are provided as .dfs0 files but pandas dataframes are also supported in case data are stored in another file format.\nBoth PointObservation and TrackObservation need the path of the data file, the item number (or item name) and a name. A PointObservation further needs to be initialized with it's x-, y-position.\n\n\n\nThe result of a simulation is stored in one or more result files, e.g. dfsu, dfs0, nc, csv.\nThe name is used to identify the model result in the plots and tables.\npython hl_lines=\"4\" import modelskill as ms mr = ms.DfsuModelResult(\"SW/HKZN_local_2017_DutchCoast.dfsu\",                           item=\"Sign. Wave Height\",                          name='HKZN_local')\n\n\n\nThis match() method returns a Comparer (a single observation) or a ComparerCollection (multiple observations) for further analysis and plotting.\ncc = ms.match([hkna, c2], mr)\n\n\n\nThe object returned by the match() method is a Comparer/ComparerCollection. It holds the matched observation and model data and has methods for plotting and skill assessment.\nThe primary comparer methods are:\n\nskill() which returns a SkillTable with the skill scores\nvarious plot methods of the comparer objects (e.g. plot.scatter(), plot.timeseries())\nsel() method for selecting data\n\n\n\n\nIt can be useful to save the comparer collection for later use. This can be done using the save() method:\ncc.save(\"my_comparer_collection.msk\")\nThe comparer collection can be loaded again from disk, using the load() method:\ncc = ms.load(\"my_comparer_collection.msk\")\n\n\n\nIn order to select only a subset of the data for analysis, the comparer has a sel() method which returns a new comparer with the selected data.\nThis method allow filtering of the data in several ways:\n\non observation by specifying name or index of one or more observations\non model (if more than one is compared) by giving name or index\ntemporal using the time (or start and end) arguments\nspatial using the area argument given as a bounding box or a polygon"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "ModelSkill",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024 DHI\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "examples/Prematched_with_auxiliary.html",
    "href": "examples/Prematched_with_auxiliary.html",
    "title": "Pre-matched data with auxiliary data",
    "section": "",
    "text": "import modelskill as ms\nimport numpy as np\nimport pandas as pd\nimport mikeio\nfn = \"../tests/testdata/SW/eur_matched.dfs0\"\nmikeio.read(fn)\n\n&lt;mikeio.Dataset&gt;\ndims: (time:67)\ntime: 2017-10-27 00:00:00 - 2017-10-29 18:00:00 (67 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  Hm0, model &lt;Significant wave height&gt; (meter)\n  1:  Hm0, obs &lt;Significant wave height&gt; (meter)\n  2:  Wind speed &lt;Wind speed&gt; (meter per sec)\n  3:  Wind Direction &lt;Wind Direction&gt; (degree)\nThe function from_matched() takes a dataframe, a dfs0 or a mikeio.Dataset of already matched data and returns a Comparer object.\ncmp = ms.from_matched(fn, obs_item=1, mod_items=0, aux_items=[2,3])\ncmp.aux_names\n\n['Wind speed', 'Wind Direction']\n# NOTE: we rename data_vars to avoid spaces in names\ncmp = cmp.rename({\"Wind speed\": \"wind_speed\", \"Wind Direction\": \"wind_dir\"})\ncmp.aux_names\n\n['wind_speed', 'wind_dir']\ncmp\n\n&lt;Comparer&gt;\nQuantity: Significant wave height [m]\nObservation: Hm0, obs, n_points=67\n Model: Hm0, model, rmse=0.228\n Auxiliary: wind_speed\n Auxiliary: wind_dir\ncmp.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\nHm0, obs\n67\n0.052239\n0.22824\n0.222181\n0.174851\n0.968321\n0.085898\n0.929767\ncmp.plot.scatter(quantiles=0, figsize=(6,6));\ncmp.plot.timeseries();",
    "crumbs": [
      "Home",
      "Examples",
      "Prematched with auxiliary"
    ]
  },
  {
    "objectID": "examples/Prematched_with_auxiliary.html#filter",
    "href": "examples/Prematched_with_auxiliary.html#filter",
    "title": "Pre-matched data with auxiliary data",
    "section": "Filter",
    "text": "Filter\nFilter on auxiliary data using query() or where(). Below, we consider only wave data when the wind speed is above 15 m/s.\n\ncmp.query(\"wind_speed &gt; 15.0\")\n\n&lt;Comparer&gt;\nQuantity: Significant wave height [m]\nObservation: Hm0, obs, n_points=19\n Model: Hm0, model, rmse=0.201\n Auxiliary: wind_speed\n Auxiliary: wind_dir\n\n\n\ncmp2 = cmp.where(cmp.data.wind_speed&gt;15.0)\ncmp2\n\n&lt;Comparer&gt;\nQuantity: Significant wave height [m]\nObservation: Hm0, obs, n_points=19\n Model: Hm0, model, rmse=0.201\n Auxiliary: wind_speed\n Auxiliary: wind_dir\n\n\n\n# notice that the model data is kept, but the observations are filtered\ncmp2.plot.timeseries();\n\n\n\n\n\n\n\n\nMore auxiliary data can be added, e.g. as derived data from the original data.\n\ncmp.data[\"residual\"] = cmp.data[\"Hm0, model\"] - cmp.data[\"Observation\"]\n\n\nlarge_residuals = np.abs(cmp.data.residual)&gt;0.1\ncmp3 = cmp.where(large_residuals)\ncmp3.plot.scatter(figsize=(6,6));\ncmp3.plot.timeseries();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncmp3.data.data_vars\n\nData variables:\n    Observation  (time) float64 0.92 1.03 1.24 1.34 1.55 ... 3.46 3.37 3.24 3.23\n    Hm0, model   (time) float64 1.43 1.655 1.789 1.834 ... 3.634 3.531 3.473\n    wind_speed   (time) float64 9.754 11.06 11.42 10.93 ... 13.3 13.3 13.54\n    wind_dir     (time) float64 327.4 331.5 333.3 330.2 ... 343.0 340.8 343.6\n    residual     (time) float64 0.5101 0.6253 0.5495 ... 0.2637 0.2907 0.2427\n\n\n\ncmp3.data.Observation.values\n\narray([0.92000002, 1.02999997, 1.24000001, 1.34000003, 1.54999995,\n       1.65999997, 1.79999995, 2.1500001 , 2.20000005, 2.1500001 ,\n       2.1500001 , 2.08999991, 2.01999998, 2.02999997, 1.88999999,\n       1.76999998, 2.1099999 , 2.27999997, 2.31999993, 2.77999997,\n       2.72000003, 2.61999989, 2.79999995, 2.91000009, 2.96000004,\n       3.31999993, 2.86999989, 3.3599999 , 4.13000011, 4.01000023,\n       3.97000003, 3.8900001 , 4.17999983, 3.63000011, 3.79999995,\n       3.47000003, 3.46000004, 3.36999989, 3.24000001, 3.23000002])",
    "crumbs": [
      "Home",
      "Examples",
      "Prematched with auxiliary"
    ]
  },
  {
    "objectID": "examples/Prematched_with_auxiliary.html#aggregate",
    "href": "examples/Prematched_with_auxiliary.html#aggregate",
    "title": "Pre-matched data with auxiliary data",
    "section": "Aggregate",
    "text": "Aggregate\nLet’s split the data based on wind direction sector and aggregate the skill calculation of the significant wave height predition for each sector.\n\n# Note: in this short example wind direction is between 274 and 353 degrees\ndf = cmp.data.wind_dir.to_dataframe()\ncmp.data[\"windsector\"] = pd.cut(df.wind_dir,\n                                [255, 285, 315, 345, 360],\n                                labels=[\"W\", \"WNW\", \"NNW\", \"N\"])\n\n\ncmp.skill(by=\"windsector\")\n\n\n\n\n\n\n\n\nobservation\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nwindsector\n\n\n\n\n\n\n\n\n\n\n\n\n\nNNW\nHm0, obs\n28\n0.115715\n0.285428\n0.260920\n0.230681\n0.969837\n0.103408\n0.927645\n\n\nN\nHm0, obs\n7\n0.070214\n0.252445\n0.242484\n0.222582\n0.991219\n0.082961\n0.859219\n\n\nWNW\nHm0, obs\n15\n-0.044628\n0.141796\n0.134590\n0.107524\n0.984303\n0.049652\n0.965368\n\n\nW\nHm0, obs\n17\n0.025762\n0.164749\n0.162723\n0.122650\n0.962449\n0.066609\n0.903978\n\n\n\n\n\n\n\n\ncmp.skill(by=\"windsector\").rmse.plot.bar(title=\"Hm0 RMSE by wind sector\");\n\n\n\n\n\n\n\n\n\ncmp.where(cmp.data.windsector==\"W\").plot.timeseries();",
    "crumbs": [
      "Home",
      "Examples",
      "Prematched with auxiliary"
    ]
  },
  {
    "objectID": "examples/Hydrology_Vistula_Catchment.html",
    "href": "examples/Hydrology_Vistula_Catchment.html",
    "title": "Hydrology example from the Vistula catchment in Poland",
    "section": "",
    "text": "The Vistula catchment is the largest catchment in Poland, with an area of 194,424 km2. This notebook shows how a hydrolgoical model can evaluated using ModelSkill.\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport modelskill as ms\nfldr = Path(\"../tests/testdata/Vistula\")\ndf = pd.read_csv(fldr / \"stations.csv\", index_col=0)\ndf\n\n\n\n\n\n\n\n\nStation\nLong\nLat\nArea\n\n\nId\n\n\n\n\n\n\n\n\n6458010\nVISTULA (TCZEW)\n18.80556\n54.08722\n193922.9\n\n\n6458500\nVISTULA (WARSAW (WARSZAWA))\n21.03250\n52.24750\n84945.1\n\n\n6458753\nPILICA (PRZEDBORZ)\n19.87528\n51.08944\n2550.1\n\n\n6458715\nWIEPRZ (LUBARTOW)\n22.64361\n51.49806\n6389.8\n\n\n6458713\nWIEPRZ (KRASNYSTAW)\n23.17667\n50.98528\n3010.2\n\n\n6458520\nKAMIENNA (KUNOW)\n21.27889\n50.96194\n1110.4\n\n\n6458460\nVISTULA (SANDOMIERZ)\n21.74611\n50.67250\n31809.9\n\n\n6458450\nVISTULA (SZCZUCIN)\n21.07722\n50.32694\n23869.3\n\n\n6458406\nDUNAJEC (NOWY SACZ)\n20.68722\n49.62722\n4337.4\n\n\n6158100\nPOPRAD (CHMELNICA)\n20.73023\n49.28918\n1262.0\n\n\n6458950\nWISLOK (TRYNCZA)\n22.54722\n50.16222\n3523.6\n\n\n6458863\nPISA (PTAKI)\n21.79250\n53.39306\n3575.7\n\n\n6458805\nNAREW (SURAZ)\n22.95500\n52.94889\n3425.3\n\n\n6458924\nLIWIEC (LOCHOW)\n21.67833\n52.51000\n2471.4\n\n\n6458555\nKRZNA (MALOWA GORA)\n23.46750\n52.10361\n3041.9\ndef get_comparer(df, mods, id):\n    \"\"\"Get a Comparer object for a given model and station id\"\"\"\n    q = ms.Quantity(name=\"Discharge\", unit=\"m3/s\")\n\n    # create Observation object\n    fp = fldr / mods[0] / f\"{id}.csv\"\n    dfd = pd.read_csv(fp, index_col=0, parse_dates=True)\n    o = ms.PointObservation(dfd, item=\"Qobs\", name=df.loc[id].Station,\n                            x=df.loc[id].Long, y=df.loc[id].Lat, quantity=q)\n    \n    # create ModelResult objects\n    mm = []\n    for m in mods:\n        fp = fldr / m / f\"{id}.csv\"\n        dfd = pd.read_csv(fp, index_col=0, parse_dates=True)\n        mm.append(ms.PointModelResult(dfd, item=\"Qsim\", name=m, quantity=q))\n    \n    return ms.match(obs=o, mod=mm)",
    "crumbs": [
      "Home",
      "Examples",
      "Hydrology Vistula Catchment"
    ]
  },
  {
    "objectID": "examples/Hydrology_Vistula_Catchment.html#compare-a-single-observation-with-two-model-results",
    "href": "examples/Hydrology_Vistula_Catchment.html#compare-a-single-observation-with-two-model-results",
    "title": "Hydrology example from the Vistula catchment in Poland",
    "section": "Compare a single observation with two model results",
    "text": "Compare a single observation with two model results\n\ncmp = get_comparer(df, [\"sim1\",\"sim2\"], df.index[0])\ncmp\n\n&lt;Comparer&gt;\nQuantity: Discharge [m3/s]\nObservation: VISTULA (TCZEW), n_points=3653\n Model: sim1, rmse=442.686\n Model: sim2, rmse=331.543\n\n\n\nPlots\ntimeseries, scatter, boxplot, hist, kde, qq, taylor\n\ncmp.plot.timeseries();\n\n\n\n\n\n\n\n\n\ncmp.sel(model=\"sim1\").plot.scatter();\n\n\n\n\n\n\n\n\n\n\nSummary statistics\n\n# set default metrics\nms.options.metrics.list = [\"kge\", \"cc\"]\n\n\ncmp.skill().round(3)\n\n\n\n\n\n\n\n\n\nn\nkge\ncc\n\n\nmodel\nobservation\n\n\n\n\n\n\n\nsim1\nVISTULA (TCZEW)\n3653\n0.617\n0.794\n\n\nsim2\nVISTULA (TCZEW)\n3653\n0.809\n0.829\n\n\n\n\n\n\n\n\n\nStatistics aggregated by month\n\ncmp.data[\"month\"] = cmp.time.to_series().dt.month\n\n\ncmp.skill(by=[\"model\",\"month\"]) #[\"kge\"].plot.bar();\n\n\n\n\n\n\n\n\n\nobservation\nn\nkge\ncc\n\n\nmodel\nmonth\n\n\n\n\n\n\n\n\nsim1\n1\nVISTULA (TCZEW)\n310\n0.385138\n0.440905\n\n\n2\nVISTULA (TCZEW)\n283\n0.578280\n0.803975\n\n\n3\nVISTULA (TCZEW)\n310\n0.454622\n0.730561\n\n\n4\nVISTULA (TCZEW)\n300\n0.509017\n0.834517\n\n\n5\nVISTULA (TCZEW)\n310\n0.575944\n0.697614\n\n\n6\nVISTULA (TCZEW)\n300\n0.344820\n0.493330\n\n\n7\nVISTULA (TCZEW)\n310\n0.052140\n0.298390\n\n\n8\nVISTULA (TCZEW)\n310\n0.219232\n0.667376\n\n\n9\nVISTULA (TCZEW)\n300\n0.376622\n0.552289\n\n\n10\nVISTULA (TCZEW)\n310\n0.626824\n0.717061\n\n\n11\nVISTULA (TCZEW)\n300\n0.643888\n0.741710\n\n\n12\nVISTULA (TCZEW)\n310\n0.288263\n0.452674\n\n\nsim2\n1\nVISTULA (TCZEW)\n310\n0.580579\n0.622109\n\n\n2\nVISTULA (TCZEW)\n283\n0.844779\n0.887547\n\n\n3\nVISTULA (TCZEW)\n310\n0.669936\n0.767161\n\n\n4\nVISTULA (TCZEW)\n300\n0.764530\n0.809858\n\n\n5\nVISTULA (TCZEW)\n310\n0.487875\n0.655435\n\n\n6\nVISTULA (TCZEW)\n300\n0.496136\n0.695364\n\n\n7\nVISTULA (TCZEW)\n310\n0.235510\n0.540092\n\n\n8\nVISTULA (TCZEW)\n310\n0.269188\n0.773781\n\n\n9\nVISTULA (TCZEW)\n300\n0.526274\n0.741995\n\n\n10\nVISTULA (TCZEW)\n310\n0.719504\n0.863399\n\n\n11\nVISTULA (TCZEW)\n300\n0.721616\n0.840079\n\n\n12\nVISTULA (TCZEW)\n310\n0.506460\n0.560932\n\n\n\n\n\n\n\n\ncmp.skill(by=[\"model\",\"month\"])[\"kge\"].plot.line()\nplt.xlabel(\"Month\")\nplt.xticks(np.arange(1,13), [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]);",
    "crumbs": [
      "Home",
      "Examples",
      "Hydrology Vistula Catchment"
    ]
  },
  {
    "objectID": "examples/Hydrology_Vistula_Catchment.html#compare-multiple-observations-with-two-model-results",
    "href": "examples/Hydrology_Vistula_Catchment.html#compare-multiple-observations-with-two-model-results",
    "title": "Hydrology example from the Vistula catchment in Poland",
    "section": "Compare multiple observations with two model results",
    "text": "Compare multiple observations with two model results\n\n# loop through all stations in df and create a Comparer for each\ncmps = []\nfor id in df.index:\n   try:\n      cmps.append(get_comparer(df, [\"sim1\",\"sim2\"], id))\n   except ValueError as e:\n      print(\"Failed to create comparer for station\", id, \":\", e)\ncc = ms.ComparerCollection(cmps)\ncc   \n\nFailed to create comparer for station 6458500 : No data left after trimming to 2000-01-01 23:59:59 - 2010-01-01 00:00:01\n\n\n&lt;ComparerCollection&gt;\nComparer: VISTULA (TCZEW)\nComparer: PILICA (PRZEDBORZ)\nComparer: WIEPRZ (LUBARTOW)\nComparer: WIEPRZ (KRASNYSTAW)\nComparer: KAMIENNA (KUNOW)\nComparer: VISTULA (SANDOMIERZ)\nComparer: VISTULA (SZCZUCIN)\nComparer: DUNAJEC (NOWY SACZ)\nComparer: POPRAD (CHMELNICA)\nComparer: WISLOK (TRYNCZA)\nComparer: PISA (PTAKI)\nComparer: NAREW (SURAZ)\nComparer: LIWIEC (LOCHOW)\nComparer: KRZNA (MALOWA GORA)\n\n\n\ncc.skill(by=[\"model\",\"observation\"], metrics=\"kge\")[\"kge\"].plot.barh();\n\n\n\n\n\n\n\n\n\n# Average skill over all stations, weighted by sqrt(area)\narea = df.set_index(\"Station\").loc[cc.obs_names].Area\ncc.mean_skill(weights=np.sqrt(area)).round(3)\n\n\n\n\n\n\n\n\nn\nkge\ncc\n\n\nmodel\n\n\n\n\n\n\n\nsim1\n51142\n0.430\n0.649\n\n\nsim2\n51142\n0.456\n0.649",
    "crumbs": [
      "Home",
      "Examples",
      "Hydrology Vistula Catchment"
    ]
  },
  {
    "objectID": "api/plotting.html",
    "href": "api/plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "Plotting\n::: modelskill.plotting"
  },
  {
    "objectID": "api/metrics.html",
    "href": "api/metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "Metrics\n::: modelskill.metrics"
  },
  {
    "objectID": "api/model/grid.html",
    "href": "api/model/grid.html",
    "title": "GridModelResult",
    "section": "",
    "text": "GridModelResult\n::: modelskill.GridModelResult"
  },
  {
    "objectID": "api/model/track.html",
    "href": "api/model/track.html",
    "title": "TrackModelResult",
    "section": "",
    "text": "TrackModelResult\n::: modelskill.TrackModelResult\n::: modelskill.timeseries._plotter.MatplotlibTimeSeriesPlotter"
  },
  {
    "objectID": "api/model/index.html",
    "href": "api/model/index.html",
    "title": "Model Result",
    "section": "",
    "text": "Model Result\nA model result can either be a simple point/track, or spatial field (e.g. 2d dfsu file) from which data can be extracted at the observation positions by spatial interpolation. The following types are available:\n\nTimeseries\n\nPointModelResult - a point result from a dfs0/nc file or a DataFrame\nTrackModelResult - a track (moving point) result from a dfs0/nc file or a DataFrame\n\nSpatialField (extractable)\n\nGridModelResult - a spatial field from a dfs2/nc file or a Xarray Dataset\nDfsuModelResult - a spatial field from a dfsu file\n\n\nA model result can be created by explicitly invoking one of the above classes or using the model_result() function which will return the appropriate type based on the input data (if possible)."
  },
  {
    "objectID": "api/model/dfsu.html",
    "href": "api/model/dfsu.html",
    "title": "DfsuModelResult",
    "section": "",
    "text": "DfsuModelResult\n::: modelskill.DfsuModelResult"
  },
  {
    "objectID": "api/observation/track.html",
    "href": "api/observation/track.html",
    "title": "TrackObservation",
    "section": "",
    "text": "TrackObservation\n::: modelskill.TrackObservation\n::: modelskill.timeseries._plotter.MatplotlibTimeSeriesPlotter",
    "crumbs": [
      "Home",
      "API Reference",
      "Observation",
      "TrackObservation"
    ]
  },
  {
    "objectID": "api/observation/point.html",
    "href": "api/observation/point.html",
    "title": "PointObservation",
    "section": "",
    "text": "PointObservation\n::: modelskill.PointObservation\n::: modelskill.timeseries._plotter.MatplotlibTimeSeriesPlotter",
    "crumbs": [
      "Home",
      "API Reference",
      "Observation",
      "PointObservation"
    ]
  },
  {
    "objectID": "api/matching.html",
    "href": "api/matching.html",
    "title": "matching",
    "section": "",
    "text": "matching\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_matched\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\nmatch\nMatch observation and model result data in space and time\n\n\nmatch_space_time\nMatch observation with one or more model results in time domain\n\n\n\n\n\nmatching.from_matched(data, *, obs_item=0, mod_items=None, aux_items=None, quantity=None, name=None, weight=1.0, x=None, y=None, z=None, x_item=None, y_item=None)\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n[pandas.pandas.DataFrame, str, pathlib.Path, mikeio.mikeio.Dfs0, mikeio.mikeio.Dataset]\nDataFrame (or object that can be converted to a DataFrame e.g. dfs0) with columns obs_item, mod_items, aux_items\nrequired\n\n\nobs_item\n[str, int]\nName or index of observation item, by default first item\n0\n\n\nmod_items\ntyping.Iterable[str, int]\nNames or indicies of model items, if None all remaining columns are model items, by default None\nNone\n\n\naux_items\ntyping.Iterable[str, int]\nNames or indicies of auxiliary items, by default None\nNone\n\n\nquantity\nmodelskill.Quantity\nQuantity of the observation and model results, by default Quantity(name=“Undefined”, unit=“Undefined”)\nNone\n\n\nname\nstr\nName of the comparer, by default None (will be set to obs_item)\nNone\n\n\nx\nfloat\nx-coordinate of observation, by default None\nNone\n\n\ny\nfloat\ny-coordinate of observation, by default None\nNone\n\n\nz\nfloat\nz-coordinate of observation, by default None\nNone\n\n\nx_item\nstr | int | None\nName of x item, only relevant for track data\nNone\n\n\ny_item\nstr | int | None\nName of y item, only relevant for track data\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a') # remaining columns are model results\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n Model: local, rmse=0.100\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1], 'global': [1.2,2.2,3.2], 'nonsense':[1,2,3]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a', mod_items=['local', 'global'])\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n    Model: local, rmse=0.100\n    Model: global, rmse=0.200\n\n\n\n\nmatching.match(obs, mod, *, obs_item=None, mod_item=None, gtype=None, max_model_gap=None, spatial_method=None)\nMatch observation and model result data in space and time\nNOTE: In case of multiple model results with different time coverage, only the overlapping time period will be used! (intersection)\nNOTE: In case of multiple observations, multiple models can only be matched if they are all of SpatialField type, e.g. DfsuModelResult or GridModelResult.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.obs.Observation, typing.Sequence[modelskill.obs.Observation])\nObservation(s) to be compared\nrequired\n\n\nmod\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.model.factory.ModelResult, typing.Sequence[modelskill.model.factory.ModelResult])\nModel result(s) to be compared\nrequired\n\n\nobs_item\nint or str\nobservation item if obs is a file/dataframe, by default None\nNone\n\n\nmod_item\n(int, str)\nmodel item if mod is a file/dataframe, by default None\nNone\n\n\ngtype\n(str, optional)\nGeometry type of the model result (if mod is a file/dataframe). If not specified, it will be guessed.\nNone\n\n\nmax_model_gap\n(float, optional)\nMaximum time gap (s) in the model result (e.g. for event-based model results), by default None\nNone\n\n\nspatial_method\nstr\nFor Dfsu- and GridModelResult, spatial interpolation/selection method. - For DfsuModelResult, one of: ‘contained’ (=isel), ‘nearest’, ‘inverse_distance’ (with 5 nearest points), by default “inverse_distance”. - For GridModelResult, passed to xarray.interp() as method argument, by default ‘linear’.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nmodelskill.comparison.Comparer\nIn case of a single observation\n\n\nmodelskill.comparison.ComparerCollection\nIn case of multiple observations\n\n\n\n\n\n\n[from_matched][modelskill.from_matched] Create a Comparer from observation and model results that are already matched\n\n\n\n\nmatching.match_space_time(observation, raw_mod_data, max_model_gap=None, spatial_tolerance=0.001)\nMatch observation with one or more model results in time domain and return as xr.Dataset in the format used by modelskill.Comparer\nWill interpolate model results to observation time.\nNote: assumes that observation and model data are already matched in space. But positions of track observations will be checked.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobservation\nmodelskill.obs.Observation\nObservation to be matched\nrequired\n\n\nraw_mod_data\ntyping.Dict[str, modelskill.model.point.PointModelResult | modelskill.model.track.TrackModelResult]\nDictionary of model results ready for interpolation\nrequired\n\n\nmax_model_gap\ntyping.Optional[modelskill.matching.TimeDeltaTypes]\nIn case of non-equidistant model results (e.g. event data), max_model_gap can be given e.g. as seconds, by default None\nNone\n\n\nspatial_tolerance\nfloat\nTolerance for spatial matching, by default 1e-3\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.xarray.Dataset\nMatched data in the format used by modelskill.Comparer"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Observation classes and functions\n\n\n\nobservation\nA factory function for creating an appropriate observation object\n\n\nPointObservation\nClass for observations of fixed locations\n\n\nTrackObservation\nClass for observation with locations moving in space, e.g. satellite altimetry\n\n\n\n\n\n\nModel result classes and functions\n\n\n\nmodel_result\nA factory function for creating an appropriate object based on the data input.\n\n\nPointModelResult\nConstruct a PointModelResult from a 0d data source:\n\n\nTrackModelResult\nConstruct a TrackModelResult from a dfs0 file,\n\n\nDfsuModelResult\nConstruct a DfsuModelResult from a dfsu file or mikeio.Dataset/DataArray.\n\n\nGridModelResult\nConstruct a GridModelResult from a file or xarray.Dataset.\n\n\nDummyModelResult\n\n\n\n\n\n\n\nMatching classes and functions\n\n\n\nmatching\n\n\n\n\n\n\n\nComparer classes and functions\n\n\n\nComparer\nComparer class for comparing model and observation data.\n\n\nComparerCollection\nCollection of comparers, constructed by calling the modelskill.match\n\n\n\n\n\n\nMetrics classes and functions\n\n\n\nmetrics\nThe metrics module contains different skill metrics for evaluating the"
  },
  {
    "objectID": "api/comparer.html",
    "href": "api/comparer.html",
    "title": "Comparer",
    "section": "",
    "text": "Comparer\nThe Comparer class is the main class of the ModelSkill package. It is returned by match(), from_matched() or as an element in a ComparerCollection. It holds the matched observation and model data for a single observation and has methods for plotting and skill assessment.\nMain functionality:\n\nselecting/filtering data\n\nsel()\nquery()\n\nskill assessment\n\nskill()\ngridded_skill() (for track observations)\n\nplotting\n\nplot.timeseries()\nplot.scatter()\nplot.kde()\nplot.qq()\nplot.hist()\nplot.box()\n\nload/save/export data\n\nload()\nsave()\nto_dataframe()\n\n\n::: modelskill.Comparer\n::: modelskill.comparison._comparer_plotter.ComparerPlotter"
  },
  {
    "objectID": "api/settings.html",
    "href": "api/settings.html",
    "title": "Settings",
    "section": "",
    "text": "Settings\n::: modelskill.settings"
  },
  {
    "objectID": "api/comparercollection.html",
    "href": "api/comparercollection.html",
    "title": "ComparerCollection",
    "section": "",
    "text": "ComparerCollection\nThe ComparerCollection is one of the main objects of the modelskill package. It is a collection of Comparer objects and created either by the match() method, by passing a list of Comparers to the ComparerCollection constructor, or by reading a config file using the from_config() function.\nMain functionality:\n\nselecting/filtering data\n\n__get_item__() - get a single Comparer, e.g., cc[0] or cc['obs1']\nsel()\nquery()\n\nskill assessment\n\nskill()\nmean_skill()\ngridded_skill() (for track observations)\n\nplotting\n\nplot.scatter()\nplot.kde()\nplot.hist()\n\nload/save/export data\n\nload()\nsave()\n\n\n::: modelskill.ComparerCollection\n::: modelskill.comparison._collection_plotter.ComparerCollectionPlotter",
    "crumbs": [
      "Home",
      "API Reference",
      "ComparerCollection"
    ]
  },
  {
    "objectID": "api/skill.html",
    "href": "api/skill.html",
    "title": "Skill",
    "section": "",
    "text": "Skill\n::: modelskill.skill.SkillTable\n::: modelskill.skill.SkillArray\n::: modelskill.skill.SkillArrayPlotter",
    "crumbs": [
      "Home",
      "API Reference",
      "Skill"
    ]
  },
  {
    "objectID": "api/observation/index.html",
    "href": "api/observation/index.html",
    "title": "Observations",
    "section": "",
    "text": "Observations\nModelSkill supports two types of observations:\n\nPointObservation - a point timeseries from a dfs0/nc file or a DataFrame\nTrackObservation - a track (moving point) timeseries from a dfs0/nc file or a DataFrame\n\nAn observation can be created by explicitly invoking one of the above classes or using the observation() function which will return the appropriate type based on the input data (if possible).",
    "crumbs": [
      "Home",
      "API Reference",
      "Observation",
      "Observations"
    ]
  },
  {
    "objectID": "api/observation/observation.html",
    "href": "api/observation/observation.html",
    "title": "observation()",
    "section": "",
    "text": "observation()\n::: modelskill.observation"
  },
  {
    "objectID": "api/model/point.html",
    "href": "api/model/point.html",
    "title": "PointModelResult",
    "section": "",
    "text": "PointModelResult\n::: modelskill.PointModelResult\n::: modelskill.timeseries._plotter.MatplotlibTimeSeriesPlotter"
  },
  {
    "objectID": "api/model/dummy.html",
    "href": "api/model/dummy.html",
    "title": "DummyModelResult",
    "section": "",
    "text": "DummyModelResult\n::: modelskill.DummyModelResult"
  },
  {
    "objectID": "api/model/model_result.html",
    "href": "api/model/model_result.html",
    "title": "model_result()",
    "section": "",
    "text": "model_result()\n::: modelskill.model_result"
  },
  {
    "objectID": "api/quantity.html",
    "href": "api/quantity.html",
    "title": "Quantity",
    "section": "",
    "text": "Quantity\n::: modelskill.quantity.Quantity"
  },
  {
    "objectID": "api/gridded_skill.html",
    "href": "api/gridded_skill.html",
    "title": "Gridded Skill",
    "section": "",
    "text": "Gridded Skill\n::: modelskill.skill_grid.SkillGrid\n::: modelskill.skill_grid.SkillGridArray",
    "crumbs": [
      "Home",
      "API Reference",
      "Gridded Skill"
    ]
  },
  {
    "objectID": "examples/Metocean_track_comparison.html",
    "href": "examples/Metocean_track_comparison.html",
    "title": "Metocean track comparison",
    "section": "",
    "text": "Comparing MIKE 21 HD dfsu model result with satellite track observation of surface elevation.\nThis notebook also includes gridded spatial skill assessments.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport modelskill as ms",
    "crumbs": [
      "Home",
      "Examples",
      "Metocean track comparison"
    ]
  },
  {
    "objectID": "examples/Metocean_track_comparison.html#extract-track-data",
    "href": "examples/Metocean_track_comparison.html#extract-track-data",
    "title": "Metocean track comparison",
    "section": "Extract track data",
    "text": "Extract track data\n\nmr = ms.model_result('../tests/testdata/NorthSeaHD_and_windspeed.dfsu',\n                     name='HD', item=0)\nmr\n\n&lt;DfsuModelResult&gt;: HD\nTime: 2017-10-27 00:00:00 - 2017-10-29 18:00:00\nQuantity: Surface Elevation [m]\n\n\nIn this case, the track observations are stored in a csv file, which we can read in using pandas. Any file format that can be read into a pandas dataframe can be used here.\n\ndf = pd.read_csv('../tests/testdata/altimetry_NorthSea_20171027.csv',\n                  index_col=0, parse_dates=True)\ndf.head()\n\n\n\n\n\n\n\n\nlon\nlat\nsurface_elevation\nsignificant_wave_height\nwind_speed\n\n\ndate\n\n\n\n\n\n\n\n\n\n2017-10-26 04:37:37\n8.757272\n53.926136\n1.6449\n0.426\n6.100000\n\n\n2017-10-26 04:37:54\n8.221631\n54.948459\n1.1200\n1.634\n9.030000\n\n\n2017-10-26 04:37:55\n8.189390\n55.008547\n1.0882\n1.717\n9.370000\n\n\n2017-10-26 04:37:56\n8.157065\n55.068627\n1.0309\n1.869\n9.559999\n\n\n2017-10-26 04:37:58\n8.124656\n55.128700\n1.0369\n1.939\n9.980000\n\n\n\n\n\n\n\nCsv files have no metadata on which quantity it contains, we add this manually, consistent with the model result.\n\no1 = ms.TrackObservation(df, item=\"surface_elevation\", name='alti',\n                         quantity=ms.Quantity(name=\"Surface Elevation\", unit=\"meter\")) \no1\n\n/home/jan/src/modelskill/modelskill/timeseries/_track.py:135: UserWarning: Removed 22 duplicate timestamps with keep=first\n  warnings.warn(\n\n\n&lt;TrackObservation&gt;: alti\nTime: 2017-10-26 04:37:37 - 2017-10-30 20:54:47\nQuantity: Surface Elevation [meter]\n\n\n\nms.plotting.spatial_overview(o1, mr);\n\n\n\n\n\n\n\n\n\ncmp = ms.match(o1, mr)\ncmp\n\n&lt;Comparer&gt;\nQuantity: Surface Elevation [meter]\nObservation: alti, n_points=532\nModel(s):\n0: HD\n\n\n\ncmp.plot.scatter();",
    "crumbs": [
      "Home",
      "Examples",
      "Metocean track comparison"
    ]
  },
  {
    "objectID": "examples/Metocean_track_comparison.html#extract-track-from-dfs0",
    "href": "examples/Metocean_track_comparison.html#extract-track-from-dfs0",
    "title": "Metocean track comparison",
    "section": "Extract track from dfs0",
    "text": "Extract track from dfs0\nUsing the TrackModelResult class.\n\nmr = ms.TrackModelResult('../tests/testdata/NorthSeaHD_extracted_track.dfs0',\n                          name='HD', item=2)\nmr\n\n/home/jan/src/modelskill/modelskill/timeseries/_track.py:135: UserWarning: Removed 22 duplicate timestamps with keep=first\n  warnings.warn(\n\n\n&lt;TrackModelResult&gt;: HD\nTime: 2017-10-26 04:37:37 - 2017-10-30 20:54:47\nQuantity: Undefined [undefined]\n\n\n\ndf = pd.read_csv('../tests/testdata/altimetry_NorthSea_20171027.csv',\n                  index_col=0, parse_dates=True)\no1 = ms.TrackObservation(df, item=2, name='alti',\n                         quantity=ms.Quantity(name=\"Surface Elevation\", unit=\"meter\"))\no1\n\n/home/jan/src/modelskill/modelskill/timeseries/_track.py:135: UserWarning: Removed 22 duplicate timestamps with keep=first\n  warnings.warn(\n\n\n&lt;TrackObservation&gt;: alti\nTime: 2017-10-26 04:37:37 - 2017-10-30 20:54:47\nQuantity: Surface Elevation [meter]\n\n\n\ncmp = ms.match(o1, mr)\ncmp\n\n&lt;Comparer&gt;\nQuantity: Surface Elevation [meter]\nObservation: alti, n_points=532\nModel(s):\n0: HD\n\n\n\ncmp.plot.scatter();",
    "crumbs": [
      "Home",
      "Examples",
      "Metocean track comparison"
    ]
  },
  {
    "objectID": "examples/Metocean_track_comparison.html#gridded-skill",
    "href": "examples/Metocean_track_comparison.html#gridded-skill",
    "title": "Metocean track comparison",
    "section": "Gridded skill",
    "text": "Gridded skill\nLoad model, load observation, add observation to model and extract.\n\nmr = ms.model_result('../tests/testdata/NorthSeaHD_and_windspeed.dfsu',\n                     name='HD', item=0)\n\ndf = pd.read_csv('../tests/testdata/altimetry_NorthSea_20171027.csv',\n                 index_col=0, parse_dates=True)\no1 = ms.TrackObservation(df, item=2, name='alti',\n                         quantity=ms.Quantity(name=\"Surface Elevation\", unit=\"meter\"))\ncmp = ms.match(o1, mr)\ncmp\n\n/home/jan/src/modelskill/modelskill/timeseries/_track.py:135: UserWarning: Removed 22 duplicate timestamps with keep=first\n  warnings.warn(\n\n\n&lt;Comparer&gt;\nQuantity: Surface Elevation [meter]\nObservation: alti, n_points=532\nModel(s):\n0: HD\n\n\nGet metrics binned by a regular spatial grid, returns xarray Dataset\n\ngs = cmp.gridded_skill(metrics=['bias'])\ngs\n\n&lt;SkillGrid&gt;\nDimensions: (x: 5, y: 5)\n\n\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize = (10, 5))\ngs.n.plot(ax=axes[0])\ngs.bias.plot(ax=axes[1]);",
    "crumbs": [
      "Home",
      "Examples",
      "Metocean track comparison"
    ]
  },
  {
    "objectID": "examples/Metocean_track_comparison.html#minimum-number-of-observations",
    "href": "examples/Metocean_track_comparison.html#minimum-number-of-observations",
    "title": "Metocean track comparison",
    "section": "Minimum number of observations",
    "text": "Minimum number of observations\n\ngs = cmp.gridded_skill(metrics=['bias'], n_min=25)\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 5))\ngs.n.plot(ax=axes[0])\ngs.bias.plot(ax=axes[1]);",
    "crumbs": [
      "Home",
      "Examples",
      "Metocean track comparison"
    ]
  },
  {
    "objectID": "examples/Metocean_track_comparison.html#multiple-bins---gridded-skill-for-water-level-categories",
    "href": "examples/Metocean_track_comparison.html#multiple-bins---gridded-skill-for-water-level-categories",
    "title": "Metocean track comparison",
    "section": "Multiple bins - gridded skill for water level categories",
    "text": "Multiple bins - gridded skill for water level categories\nGet data from comparer as dataframe and add a water level category as a new column.\n\ndftmp = cmp.data.to_dataframe()\ndftmp[\"wl category\"] = 'high'\ndftmp.loc[dftmp['HD']&lt;0, \"wl category\"] = 'low'\n\nAdd the “wl category” to the comparer’s data structure.\n\ncmp.data[\"wl category\"] = dftmp[\"wl category\"]\n\nNow aggregate the data by the new column (and x and y):\n\ngs = cmp.gridded_skill(by=['wl category'], metrics=['bias'], n_min=5)\ngs\n\n\ngs.bias.plot();",
    "crumbs": [
      "Home",
      "Examples",
      "Metocean track comparison"
    ]
  },
  {
    "objectID": "examples/Metocean_track_comparison.html#multiple-observations",
    "href": "examples/Metocean_track_comparison.html#multiple-observations",
    "title": "Metocean track comparison",
    "section": "Multiple observations",
    "text": "Multiple observations\nAdd fake 2nd observation to model\n\nimport warnings\n\ndf2 = df.copy()\ndf2['surface_elevation'] = df2['surface_elevation'] - 0.2\no2 = ms.TrackObservation(df2, item=2, name='alti2')\n\nwarnings.filterwarnings('ignore', message=\"duplicate\")\ncmp2 = ms.match(o2, mr)\n\nExtract, gridded skill, add attrs, plot.\n\ncmp = cmp + cmp2\ngs = cmp.gridded_skill(metrics=['bias'], n_min=20)\ngs.bias.data.attrs = dict(long_name=\"Bias of surface elevation\", units=\"m\")\ngs.bias.plot(figsize=(10,5));",
    "crumbs": [
      "Home",
      "Examples",
      "Metocean track comparison"
    ]
  },
  {
    "objectID": "examples/Simple_MIKE21HD_dfsu.html",
    "href": "examples/Simple_MIKE21HD_dfsu.html",
    "title": "MIKE21 HD",
    "section": "",
    "text": "import modelskill as ms\n\n\nmr = ms.model_result('../tests/testdata/Oresund2D.dfsu',\n                     item='Surface elevation')\nmr\n\n&lt;DfsuModelResult&gt;: Oresund2D\nTime: 2018-03-04 00:00:00 - 2018-03-10 22:40:00\nQuantity: Surface Elevation [m]\n\n\n\nfn = '../tests/testdata/smhi_2095_klagshamn.dfs0'\no1 = ms.PointObservation(fn, x=366844.15, y=6154291.6, item=0) \no1\n\n&lt;PointObservation&gt;: smhi_2095_klagshamn\nLocation: 366844.15, 6154291.6\nTime: 2015-01-01 01:00:00 - 2020-09-28 00:00:00\nQuantity: Water Level [m]\n\n\n\nms.plotting.spatial_overview(o1, mr, figsize=(8, 8));\n\n\n\n\n\n\n\n\n\ncmp = ms.match(o1, mr)\ncmp\n\n&lt;Comparer&gt;\nQuantity: Water Level [m]\nObservation: smhi_2095_klagshamn, n_points=167\nModel(s):\n0: Oresund2D\n\n\nMost use cases will compare many observed locations to a one or more models.\nIn this case we only have one observed location.\n\ncmp.plot.timeseries(figsize=(10,4));\n\n\n\n\n\n\n\n\n\nub_cmp = cmp.remove_bias()\nub_cmp.plot.timeseries(figsize=(10,4));\n\n\n\n\n\n\n\n\n\nub_cmp.score(\"bias\")\n\n{'Oresund2D': -2.9251385079944247e-17}\n\n\n\ncmp.score(\"bias\")\n\n{'Oresund2D': 0.1868744370781926}\n\n\n\ncmp.plot.timeseries()\n\n\n\n\n\n\n\n\nGet skill for a commonly used set of metrics\n\ncmp.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\nsmhi_2095_klagshamn\n167\n0.186874\n0.191303\n0.040924\n0.186874\n0.838306\n0.378995\n-5.505521\n\n\n\n\n\n\n\nOr choose specific metrics\n\ncmp.metrics = [\"bias\",\"rmse\"]\ncmp.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\nsmhi_2095_klagshamn\n167\n0.186874\n0.191303\n0.040924\n0.186874\n0.838306\n0.378995\n-5.505521\n\n\n\n\n\n\n\n\ncmp.plot.scatter(bins=0.02, cmap='YlOrRd', show_points=True);\n\n\n\n\n\n\n\n\n\ncmp.plot.scatter(skill_table=True, show_points=True);",
    "crumbs": [
      "Home",
      "Examples",
      "Simple MIKE21HD dfsu"
    ]
  },
  {
    "objectID": "index.html#set-up-in-5-minutes",
    "href": "index.html#set-up-in-5-minutes",
    "title": "ModelSkill: Assess the skill of your simulations",
    "section": "Set up in 5 minutes",
    "text": "Set up in 5 minutes\nInstall ModelSkill with pip and get up and running in minutes\nGetting started"
  },
  {
    "objectID": "user-guide/overview.html",
    "href": "user-guide/overview.html",
    "title": "Overview",
    "section": "",
    "text": "ModelSkill compares model results with observations. The workflow can be split in two phases:\n\nMatching - making sure that observations and model results are in the same space and time\nAnalysis - plots and statistics of the matched data\n\nIf the observations and model results are already matched (i.e. are stored in the same data source), the from_matched() function can be used to go directly to the analysis phase. If not, the match() function can be used to match the observations and model results in space and time.\n\n\nIf the observations and model results are not in the same data source (e.g. dfs0 file), they will need to be defined and then matched in space and time with the match() function. In simple cases, observations and model results can be defined directly in the match() function:\nimport modelskill as ms\ncmp = ms.match(\"obs.dfs0\", \"model.dfs0\", obs_item=\"obs_WL\", mod_item=\"WL\")\nBut in most cases, the observations and model results will need to be defined separately first.\n\n\nThe observations can be defined as either a PointObservation or a TrackObservation (a moving point).\no1 = ms.PointObservation(\"stn1.dfs0\", item=\"obs_WL\")\no2 = ms.PointObservation(\"stn2.dfs0\", item=\"obs_WL\")\nThe item needs to be specified as either the item number or the item name if the input file contains multiple items. Several other parameters can be specified, such as the name of the observation, the x- and y-position, and the quantity type and unit of the observation.\n\n\n\nA model result will either be a simple point/track like the observations, or spatial field (e.g. 2d dfsu file) from which the model results will be extracted at the observation positions. The following types are available:\n\nPointModelResult - a point result from a dfs0/nc file or a DataFrame\nTrackModelResult - a track result from a dfs0/nc file or a DataFrame\nGridModelResult - a spatial field from a dfs2/nc file or a Xarray Dataset\nDfsuModelResult - a spatial field from a dfsu file\n\nmr1 = ms.PointModelResult(\"model.dfs0\", item=\"WL_stn1\")\nmr2 = ms.PointModelResult(\"model.dfs0\", item=\"WL_stn2\")\n\n\n\nThe match() function will interpolate the model results to the time (and space) of the observations and return a collection of Comparer objects that can be used for analysis.\ncc1 = ms.match(o1, mr1)\ncc2 = ms.match(o2, mr2)\ncc = cc1 + cc2\n\n\n\n\nOnce the observations and model results are matched, the Comparer object can be used for analysis and plotting."
  },
  {
    "objectID": "user-guide/overview.html#matching",
    "href": "user-guide/overview.html#matching",
    "title": "Overview",
    "section": "",
    "text": "If the observations and model results are not in the same data source (e.g. dfs0 file), they will need to be defined and then matched in space and time with the match() function. In simple cases, observations and model results can be defined directly in the match() function:\nimport modelskill as ms\ncmp = ms.match(\"obs.dfs0\", \"model.dfs0\", obs_item=\"obs_WL\", mod_item=\"WL\")\nBut in most cases, the observations and model results will need to be defined separately first.\n\n\nThe observations can be defined as either a PointObservation or a TrackObservation (a moving point).\no1 = ms.PointObservation(\"stn1.dfs0\", item=\"obs_WL\")\no2 = ms.PointObservation(\"stn2.dfs0\", item=\"obs_WL\")\nThe item needs to be specified as either the item number or the item name if the input file contains multiple items. Several other parameters can be specified, such as the name of the observation, the x- and y-position, and the quantity type and unit of the observation.\n\n\n\nA model result will either be a simple point/track like the observations, or spatial field (e.g. 2d dfsu file) from which the model results will be extracted at the observation positions. The following types are available:\n\nPointModelResult - a point result from a dfs0/nc file or a DataFrame\nTrackModelResult - a track result from a dfs0/nc file or a DataFrame\nGridModelResult - a spatial field from a dfs2/nc file or a Xarray Dataset\nDfsuModelResult - a spatial field from a dfsu file\n\nmr1 = ms.PointModelResult(\"model.dfs0\", item=\"WL_stn1\")\nmr2 = ms.PointModelResult(\"model.dfs0\", item=\"WL_stn2\")\n\n\n\nThe match() function will interpolate the model results to the time (and space) of the observations and return a collection of Comparer objects that can be used for analysis.\ncc1 = ms.match(o1, mr1)\ncc2 = ms.match(o2, mr2)\ncc = cc1 + cc2"
  },
  {
    "objectID": "user-guide/overview.html#analysis",
    "href": "user-guide/overview.html#analysis",
    "title": "Overview",
    "section": "",
    "text": "Once the observations and model results are matched, the Comparer object can be used for analysis and plotting."
  },
  {
    "objectID": "user-guide/index.html",
    "href": "user-guide/index.html",
    "title": "User Guide",
    "section": "",
    "text": "User Guide\nModelSkill compares model results with observations. The workflow can be split in two phases:\n\nMatching - making sure that observations and model results are in the same space and time\nAnalysis - plots and statistics of the matched data\n\nIf the observations and model results are already matched (i.e. are stored in the same data source), the from_matched() function can be used to go directly to the analysis phase. If not, the match() function can be used to match the observations and model results in space and time.",
    "crumbs": [
      "Home",
      "User Guide",
      "User Guide"
    ]
  },
  {
    "objectID": "user-guide/terminology.html",
    "href": "user-guide/terminology.html",
    "title": "Terminology",
    "section": "",
    "text": "ModelSkill is a library for assessing the skill of numerical models. It provides tools for comparing model results with observations, plotting the results and calculating validation metrics. This page defines some of the key terms used in the documentation.\n\n\nSkill refers to the ability of a numerical model to accurately represent the real-world phenomenon it aims to simulate. It is a measure of how well the model performs in reproducing the observed system. Skill can be assessed using various metrics, such as accuracy, precision, and reliability, depending on the specific goals of the model and the nature of the data. In ModelSkill, skill is also a specific method on Comparer objects that returns a SkillTable with aggregated skill scores per observation and model for a list of selected metrics.\n\n\n\nValidation is the process of assessing the model’s performance by comparing its output to real-world observations or data collected from the system being modeled. It helps ensure that the model accurately represents the system it simulates. Validation is typically performed before the model is used for prediction or decision-making.\n\n\n\nCalibration is the process of adjusting the model’s parameters or settings to improve its performance. It involves fine-tuning the model to better match observed data. Calibration aims to reduce discrepancies between model predictions and actual measurements. At the end of the calibration process, the calibrated model should be validated with independent data.\n\n\n\nPerformance is a measure of how well a numerical model operates in reproducing the observed system. It can be assessed using various metrics, such as accuracy, precision, and reliability, depending on the specific goals of the model and the nature of the data. In this context, performance is synonymous with skill.\n\n\n\nA timeseries is a sequence of data points in time. In ModelSkill, The data can either be from observations or model results. Timeseries can univariate or multivariate; ModelSkill primarily supports univariate timeseries. Multivariate timeseries can be assessed one variable at a time. Timeseries can also have different spatial dimensions, such as point, track, line, or area.\n\n\n\nAn observation refers to real-world data or measurements collected from the system you are modeling. Observations serve as a reference for assessing the model’s performance. These data points are used to compare with the model’s predictions during validation and calibration. Observations are usually based on field measurements or laboratory experiments, but for the purposes of model validation, they can also be derived from other models (e.g. a reference model). ModelSkill supports point and track observation types.\n\n\n\nA measurement is called observation in ModelSkill.\n\n\n\nA model result is the output of any type of numerical model. It is the data generated by the model during a simulation. Model results can be compared with observations to assess the model’s performance. In the context of validation, the term “model result” is often used interchangeably with “model output” or “model prediction”. ModelSkill supports point, track, dfsu and grid model result types.\n\n\n\nA metric is a quantitative measure (a mathematical expression) used to evaluate the performance of a numerical model. Metrics provide a standardized way to assess the model’s accuracy, precision, and other attributes. A metric aggregates the skill of a model into a single number. See list of metrics supported by ModelSkill.\n\n\n\nA score is a numerical value that summarizes the model’s performance based on chosen metrics. Scores can be used to rank or compare different models or model configurations. In the context of validation, the “skill score” or “validation score” often quantifies the model’s overall performance. The score of a model is a single number, calculated as a weighted average for all time-steps, observations and variables. If you want to perform automated calibration, you can use the score as the objective function. In ModelSkill, score is also a specific method on Comparer objects that returns a single number aggregated score using a specific metric.\n\n\n\nIn ModelSkill, observations and model results are matched when they refer to the same positions in space and time. If the observations and model results are already matched, the from_matched function can be used to create a Comparer directly. Otherwise, the match function can be used to match the observations and model results in space and time.\n\n\n\nThe function match is used to match a model result with observations. It returns a Comparer object or a ComparerCollection object.\n\n\n\nA Comparer is an object that stores the matched observation and model result data for a single observation. It is used to calculate validation metrics and generate plots. A Comparer can be created using the match function.\n\n\n\nA ComparerCollection is a collection of Comparers. It is used to compare multiple observations with one or more model results. A ComparerCollection can be created using the match function or by passing a list of Comparers to the ComparerCollection constructor.\n\n\n\nIn past versions of FMSkill/ModelSkill, the Connector class was used to connect observations and model results. This class has been deprecated and is no longer in use.\n\n\n\n\n\n\nAbbreviation\nMeaning\n\n\n\n\nms\nModelSkill\n\n\no or obs\nObservation\n\n\nmr or mod\nModel result\n\n\ncmp\nComparer\n\n\ncc\nComparerCollection\n\n\nsk\nSkillTable\n\n\nmtr\nMetric\n\n\nq\nQuantity",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#skill",
    "href": "user-guide/terminology.html#skill",
    "title": "Terminology",
    "section": "",
    "text": "Skill refers to the ability of a numerical model to accurately represent the real-world phenomenon it aims to simulate. It is a measure of how well the model performs in reproducing the observed system. Skill can be assessed using various metrics, such as accuracy, precision, and reliability, depending on the specific goals of the model and the nature of the data. In ModelSkill, skill is also a specific method on Comparer objects that returns a SkillTable with aggregated skill scores per observation and model for a list of selected metrics.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#validation",
    "href": "user-guide/terminology.html#validation",
    "title": "Terminology",
    "section": "",
    "text": "Validation is the process of assessing the model’s performance by comparing its output to real-world observations or data collected from the system being modeled. It helps ensure that the model accurately represents the system it simulates. Validation is typically performed before the model is used for prediction or decision-making.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#calibration",
    "href": "user-guide/terminology.html#calibration",
    "title": "Terminology",
    "section": "",
    "text": "Calibration is the process of adjusting the model’s parameters or settings to improve its performance. It involves fine-tuning the model to better match observed data. Calibration aims to reduce discrepancies between model predictions and actual measurements. At the end of the calibration process, the calibrated model should be validated with independent data.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#performance",
    "href": "user-guide/terminology.html#performance",
    "title": "Terminology",
    "section": "",
    "text": "Performance is a measure of how well a numerical model operates in reproducing the observed system. It can be assessed using various metrics, such as accuracy, precision, and reliability, depending on the specific goals of the model and the nature of the data. In this context, performance is synonymous with skill.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#timeseries",
    "href": "user-guide/terminology.html#timeseries",
    "title": "Terminology",
    "section": "",
    "text": "A timeseries is a sequence of data points in time. In ModelSkill, The data can either be from observations or model results. Timeseries can univariate or multivariate; ModelSkill primarily supports univariate timeseries. Multivariate timeseries can be assessed one variable at a time. Timeseries can also have different spatial dimensions, such as point, track, line, or area.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#observation",
    "href": "user-guide/terminology.html#observation",
    "title": "Terminology",
    "section": "",
    "text": "An observation refers to real-world data or measurements collected from the system you are modeling. Observations serve as a reference for assessing the model’s performance. These data points are used to compare with the model’s predictions during validation and calibration. Observations are usually based on field measurements or laboratory experiments, but for the purposes of model validation, they can also be derived from other models (e.g. a reference model). ModelSkill supports point and track observation types.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#measurement",
    "href": "user-guide/terminology.html#measurement",
    "title": "Terminology",
    "section": "",
    "text": "A measurement is called observation in ModelSkill.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#model-result",
    "href": "user-guide/terminology.html#model-result",
    "title": "Terminology",
    "section": "",
    "text": "A model result is the output of any type of numerical model. It is the data generated by the model during a simulation. Model results can be compared with observations to assess the model’s performance. In the context of validation, the term “model result” is often used interchangeably with “model output” or “model prediction”. ModelSkill supports point, track, dfsu and grid model result types.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#metric",
    "href": "user-guide/terminology.html#metric",
    "title": "Terminology",
    "section": "",
    "text": "A metric is a quantitative measure (a mathematical expression) used to evaluate the performance of a numerical model. Metrics provide a standardized way to assess the model’s accuracy, precision, and other attributes. A metric aggregates the skill of a model into a single number. See list of metrics supported by ModelSkill.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#score",
    "href": "user-guide/terminology.html#score",
    "title": "Terminology",
    "section": "",
    "text": "A score is a numerical value that summarizes the model’s performance based on chosen metrics. Scores can be used to rank or compare different models or model configurations. In the context of validation, the “skill score” or “validation score” often quantifies the model’s overall performance. The score of a model is a single number, calculated as a weighted average for all time-steps, observations and variables. If you want to perform automated calibration, you can use the score as the objective function. In ModelSkill, score is also a specific method on Comparer objects that returns a single number aggregated score using a specific metric.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#matched-data",
    "href": "user-guide/terminology.html#matched-data",
    "title": "Terminology",
    "section": "",
    "text": "In ModelSkill, observations and model results are matched when they refer to the same positions in space and time. If the observations and model results are already matched, the from_matched function can be used to create a Comparer directly. Otherwise, the match function can be used to match the observations and model results in space and time.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#match",
    "href": "user-guide/terminology.html#match",
    "title": "Terminology",
    "section": "",
    "text": "The function match is used to match a model result with observations. It returns a Comparer object or a ComparerCollection object.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#comparer",
    "href": "user-guide/terminology.html#comparer",
    "title": "Terminology",
    "section": "",
    "text": "A Comparer is an object that stores the matched observation and model result data for a single observation. It is used to calculate validation metrics and generate plots. A Comparer can be created using the match function.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#comparercollection",
    "href": "user-guide/terminology.html#comparercollection",
    "title": "Terminology",
    "section": "",
    "text": "A ComparerCollection is a collection of Comparers. It is used to compare multiple observations with one or more model results. A ComparerCollection can be created using the match function or by passing a list of Comparers to the ComparerCollection constructor.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#connector",
    "href": "user-guide/terminology.html#connector",
    "title": "Terminology",
    "section": "",
    "text": "In past versions of FMSkill/ModelSkill, the Connector class was used to connect observations and model results. This class has been deprecated and is no longer in use.",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/terminology.html#abbreviations",
    "href": "user-guide/terminology.html#abbreviations",
    "title": "Terminology",
    "section": "",
    "text": "Abbreviation\nMeaning\n\n\n\n\nms\nModelSkill\n\n\no or obs\nObservation\n\n\nmr or mod\nModel result\n\n\ncmp\nComparer\n\n\ncc\nComparerCollection\n\n\nsk\nSkillTable\n\n\nmtr\nMetric\n\n\nq\nQuantity",
    "crumbs": [
      "Home",
      "User Guide",
      "Terminology"
    ]
  },
  {
    "objectID": "user-guide/data-structures.html",
    "href": "user-guide/data-structures.html",
    "title": "Data Structures",
    "section": "",
    "text": "The main data structures in ModelSkill can be grouped into three categories:\n\nPrimary data (observations and model results)\nComparer objects\nSkill objects\n\nAll objects share some common principles:\n\nThe data container is accesssible via the data attribute.\nThe data container is an xarray object (except for the SkillTable object, which is a pandas object).\nThe main data selection method is sel, which is a wrapper around xarray.Dataset.sel.\nAll plotting are accessible via the plot accessor of the object.\n\n\n\nThe primary data of ModelSkill are the data that needs to be compared: observations and model results. The underlying data structures are very similar and can be grouped according to the spatial dimensionality (gtype) of the data:\n\npoint: 0D time series data\ntrack: 0D time series data at moving locations (trajectories)\ngrid: gridded 2D data\ndfsu: flexible mesh 2D data\n\nPoint and track data are both TimeSeries objects, while grid and dfsu data are both SpatialField objects. TimeSeries objects are ready to be compared whereas data from SpatialField object needs to be extracted first (the extracted object will be of the TimeSeries type).\nTimeSeries objects contains its data in an xarray.Dataset with the actual data in the first DataArray and optional auxilliary data in the following DataArrays. The DataArrays have a kind attribute with either observation or model.\n\n\n\nComparer objects are results of a matching procedure (between observations and model results) or constructed directly from already matched data. A comparison of a single observation and one or more model results are stored in a Comparer object. A comparison of multiple observations and one or more model results are stored in a ComparerCollection object which is a collection of Comparer objects.\nThe matched data in a Comparer is stored in an xarray.Dataset which can be accessed via the data attribute. The Dataset has an attribute gtype which is a string describing the type of data (e.g. point, track). The first DataArray in the Dataset is the observation data, the next DataArrays are model result data and optionally additional DataArrays are auxilliarye data. Each of the DataArrays have a kind attribute with either observation, model or aux.\nBoth Comparer and ComparerCollection have a plot accessor for plotting the data (e.g. cmp.plot.timeseries() or cmp.plot.scatter()).\n\n\n\nCalling a skill method on a comparer object will return a skill object with skill scores (statistics) from comparing observation and model result data using different metrics (e.g. root mean square error). Two skill objects are currently implemented: SkillTable and SkillGrid. The first is relevant for all ModelSkill users while the latter is relevant for users of the track data (e.g. MetOcean studies using satellite altimetry data).\nIf c is a comparer object, then the following skill methods are available:\n\nc.skill() -&gt; SkillTable\nc.mean_skill() -&gt; SkillTable\nc.gridded_skill() -&gt; SkillGrid",
    "crumbs": [
      "Home",
      "User Guide",
      "Data Structures"
    ]
  },
  {
    "objectID": "user-guide/data-structures.html#observations-and-model-results",
    "href": "user-guide/data-structures.html#observations-and-model-results",
    "title": "Data Structures",
    "section": "",
    "text": "The primary data of ModelSkill are the data that needs to be compared: observations and model results. The underlying data structures are very similar and can be grouped according to the spatial dimensionality (gtype) of the data:\n\npoint: 0D time series data\ntrack: 0D time series data at moving locations (trajectories)\ngrid: gridded 2D data\ndfsu: flexible mesh 2D data\n\nPoint and track data are both TimeSeries objects, while grid and dfsu data are both SpatialField objects. TimeSeries objects are ready to be compared whereas data from SpatialField object needs to be extracted first (the extracted object will be of the TimeSeries type).\nTimeSeries objects contains its data in an xarray.Dataset with the actual data in the first DataArray and optional auxilliary data in the following DataArrays. The DataArrays have a kind attribute with either observation or model.",
    "crumbs": [
      "Home",
      "User Guide",
      "Data Structures"
    ]
  },
  {
    "objectID": "user-guide/data-structures.html#comparer-objects",
    "href": "user-guide/data-structures.html#comparer-objects",
    "title": "Data Structures",
    "section": "",
    "text": "Comparer objects are results of a matching procedure (between observations and model results) or constructed directly from already matched data. A comparison of a single observation and one or more model results are stored in a Comparer object. A comparison of multiple observations and one or more model results are stored in a ComparerCollection object which is a collection of Comparer objects.\nThe matched data in a Comparer is stored in an xarray.Dataset which can be accessed via the data attribute. The Dataset has an attribute gtype which is a string describing the type of data (e.g. point, track). The first DataArray in the Dataset is the observation data, the next DataArrays are model result data and optionally additional DataArrays are auxilliarye data. Each of the DataArrays have a kind attribute with either observation, model or aux.\nBoth Comparer and ComparerCollection have a plot accessor for plotting the data (e.g. cmp.plot.timeseries() or cmp.plot.scatter()).",
    "crumbs": [
      "Home",
      "User Guide",
      "Data Structures"
    ]
  },
  {
    "objectID": "user-guide/data-structures.html#skill-objects",
    "href": "user-guide/data-structures.html#skill-objects",
    "title": "Data Structures",
    "section": "",
    "text": "Calling a skill method on a comparer object will return a skill object with skill scores (statistics) from comparing observation and model result data using different metrics (e.g. root mean square error). Two skill objects are currently implemented: SkillTable and SkillGrid. The first is relevant for all ModelSkill users while the latter is relevant for users of the track data (e.g. MetOcean studies using satellite altimetry data).\nIf c is a comparer object, then the following skill methods are available:\n\nc.skill() -&gt; SkillTable\nc.mean_skill() -&gt; SkillTable\nc.gridded_skill() -&gt; SkillGrid",
    "crumbs": [
      "Home",
      "User Guide",
      "Data Structures"
    ]
  },
  {
    "objectID": "user-guide/vision.html",
    "href": "user-guide/vision.html",
    "title": "Vision",
    "section": "",
    "text": "ModelSkill would like to be your modelling companion. It should be indispensable good such that you want to use it every time you do a MIKE simulation.\n\n\nWe want ModelSkill to make it easy to\n\nassess the skill of a model by comparing with measurements\nassess model skill also when result is split on several files (2d, 3d, yearly, ...)\ncompare the skill of different calibration runs\ncompare your model with other models\nuse a wide range of common evaluation metrics\ncreate common plots such as time series, scatter and taylor diagrams\ndo aggregations - assess for all observations, geographic areas, monthly, ...\ndo filtering - assess for a subset of observations, geographic areas, ...\nmake fast comparisons (optimized code)\n\nAnd it should be\n\nDifficult to make mistakes by verifying input\nTrustworthy by having &gt;95% test coverage\nEasy to install ($ pip install modelskill)\nEasy to get started by providing many notebook examples and documentation\n\n\n\n\nModelSkill wants to balance general and specific needs:\n\nIt should be general enough to cover &gt;90% of MIKE simulations\nIt should be general enough to cover generic modelling irrespective of software.\nBut specific enough to be useful\n\nSupport dfs files (using mikeio)\nHandle circular variables such as wave direction\n\n\n\n\n\nModelSkill does not wish to cover\n\nExtreme value analysis\nDeterministic wave analysis such as crossing analysis\nRare alternative file types\nRarely used model result types\nRare observation types\nAnything project specific\n\n\n\n\n\n\nIt should be possible to compare forecasts with observations using forecast lead time as a dimension. Planned 2024.\n\n\n\nCurrently 3D data is supported only as point data and only if data has already been extracted from model result files. It should be possible to extract date from 3D files directly. Furthermore, vertical columns data should be supported as an observation type with z as a dimension. Planned 2024.\n\n\n\nCreate a web app that wraps this library.\n\n\n\nBoth static as markdown, docx, pptx and interactive as html."
  },
  {
    "objectID": "user-guide/vision.html#objective",
    "href": "user-guide/vision.html#objective",
    "title": "Vision",
    "section": "",
    "text": "We want ModelSkill to make it easy to\n\nassess the skill of a model by comparing with measurements\nassess model skill also when result is split on several files (2d, 3d, yearly, ...)\ncompare the skill of different calibration runs\ncompare your model with other models\nuse a wide range of common evaluation metrics\ncreate common plots such as time series, scatter and taylor diagrams\ndo aggregations - assess for all observations, geographic areas, monthly, ...\ndo filtering - assess for a subset of observations, geographic areas, ...\nmake fast comparisons (optimized code)\n\nAnd it should be\n\nDifficult to make mistakes by verifying input\nTrustworthy by having &gt;95% test coverage\nEasy to install ($ pip install modelskill)\nEasy to get started by providing many notebook examples and documentation"
  },
  {
    "objectID": "user-guide/vision.html#scope",
    "href": "user-guide/vision.html#scope",
    "title": "Vision",
    "section": "",
    "text": "ModelSkill wants to balance general and specific needs:\n\nIt should be general enough to cover &gt;90% of MIKE simulations\nIt should be general enough to cover generic modelling irrespective of software.\nBut specific enough to be useful\n\nSupport dfs files (using mikeio)\nHandle circular variables such as wave direction"
  },
  {
    "objectID": "user-guide/vision.html#limitations",
    "href": "user-guide/vision.html#limitations",
    "title": "Vision",
    "section": "",
    "text": "ModelSkill does not wish to cover\n\nExtreme value analysis\nDeterministic wave analysis such as crossing analysis\nRare alternative file types\nRarely used model result types\nRare observation types\nAnything project specific"
  },
  {
    "objectID": "user-guide/vision.html#future",
    "href": "user-guide/vision.html#future",
    "title": "Vision",
    "section": "",
    "text": "It should be possible to compare forecasts with observations using forecast lead time as a dimension. Planned 2024.\n\n\n\nCurrently 3D data is supported only as point data and only if data has already been extracted from model result files. It should be possible to extract date from 3D files directly. Furthermore, vertical columns data should be supported as an observation type with z as a dimension. Planned 2024.\n\n\n\nCreate a web app that wraps this library.\n\n\n\nBoth static as markdown, docx, pptx and interactive as html."
  },
  {
    "objectID": "index.html#its-just-python",
    "href": "index.html#its-just-python",
    "title": "ModelSkill: Assess the skill of your simulations",
    "section": "It’s just Python",
    "text": "It’s just Python\nFocus on your modelling and less on generate a validation report\nAPI Reference"
  },
  {
    "objectID": "index.html#made-to-measure",
    "href": "index.html#made-to-measure",
    "title": "ModelSkill: Assess the skill of your simulations",
    "section": "Made to measure",
    "text": "Made to measure\nChoose between different skill metrics and customizable tables and charts\nMetrics"
  },
  {
    "objectID": "index.html#open-source-mit",
    "href": "index.html#open-source-mit",
    "title": "ModelSkill: Assess the skill of your simulations",
    "section": "Open Source, MIT",
    "text": "Open Source, MIT\nModelSkill is licensed under MIT and available on GitHub\nLicense"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Examples",
    "section": "",
    "text": "Examples",
    "crumbs": [
      "Home",
      "Examples",
      "Overview"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Observation classes and functions\n\n\n\nobservation\nA factory function for creating an appropriate observation object\n\n\nPointObservation\nClass for observations of fixed locations\n\n\nTrackObservation\nClass for observation with locations moving in space, e.g. satellite altimetry\n\n\n\n\n\n\nModel result classes and functions\n\n\n\nmodel_result\nA factory function for creating an appropriate object based on the data input.\n\n\nPointModelResult\nConstruct a PointModelResult from a 0d data source:\n\n\nTrackModelResult\nConstruct a TrackModelResult from a dfs0 file,\n\n\nDfsuModelResult\nConstruct a DfsuModelResult from a dfsu file or mikeio.Dataset/DataArray.\n\n\nGridModelResult\nConstruct a GridModelResult from a file or xarray.Dataset.\n\n\nDummyModelResult\n\n\n\n\n\n\n\nMatching classes and functions\n\n\n\nmatching\n\n\n\n\n\n\n\nComparer classes and functions\n\n\n\nComparer\nComparer class for comparing model and observation data.\n\n\nComparerCollection\nCollection of comparers, constructed by calling the modelskill.match\n\n\n\n\n\n\nMetrics classes and functions\n\n\n\nmetrics\nThe metrics module contains different skill metrics for evaluating the"
  },
  {
    "objectID": "reference/index.html#observation",
    "href": "reference/index.html#observation",
    "title": "Function reference",
    "section": "",
    "text": "Observation classes and functions\n\n\n\nobservation\nA factory function for creating an appropriate observation object\n\n\nPointObservation\nClass for observations of fixed locations\n\n\nTrackObservation\nClass for observation with locations moving in space, e.g. satellite altimetry"
  },
  {
    "objectID": "reference/PointObservation.html",
    "href": "reference/PointObservation.html",
    "title": "PointObservation",
    "section": "",
    "text": "PointObservation(self, data, *, item=None, x=None, y=None, z=None, name=None, weight=1.0, quantity=None, aux_items=None, attrs=None)\nClass for observations of fixed locations\nCreate a PointObservation from a dfs0 file or a pd.DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nitem\n(int, str)\nindex or name of the wanted item/column, by default None if data contains more than one item, item must be given\nNone\n\n\nx\nfloat\nx-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\ny\nfloat\ny-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nz-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"klagshamn.dfs0\", item=\"Water Level\", x=366844, y=6154291)\n&gt;&gt;&gt; o3 = ms.PointObservation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o4 = ms.PointObservation(df[\"Water Level\"], x=366844, y=6154291)"
  },
  {
    "objectID": "reference/PointObservation.html#parameters",
    "href": "reference/PointObservation.html#parameters",
    "title": "PointObservation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nitem\n(int, str)\nindex or name of the wanted item/column, by default None if data contains more than one item, item must be given\nNone\n\n\nx\nfloat\nx-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\ny\nfloat\ny-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nz-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0"
  },
  {
    "objectID": "reference/PointObservation.html#examples",
    "href": "reference/PointObservation.html#examples",
    "title": "PointObservation",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"klagshamn.dfs0\", item=\"Water Level\", x=366844, y=6154291)\n&gt;&gt;&gt; o3 = ms.PointObservation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o4 = ms.PointObservation(df[\"Water Level\"], x=366844, y=6154291)"
  },
  {
    "objectID": "reference/TrackObservation.html",
    "href": "reference/TrackObservation.html",
    "title": "TrackObservation",
    "section": "",
    "text": "TrackObservation(self, data, *, item=None, name=None, weight=1.0, x_item=0, y_item=1, keep_duplicates='first', offset_duplicates=0.001, quantity=None, aux_items=None, attrs=None)\nClass for observation with locations moving in space, e.g. satellite altimetry\nThe data needs in addition to the datetime of each single observation point also, x and y coordinates.\nCreate TrackObservation from dfs0 or DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, pandas.pandas.DataFrame, xarray.xarray.Dataset)\npath to dfs0 file or object with track data\nrequired\n\n\nitem\n(str, int)\nitem name or index of values, by default None if data contains more than one item, item must be given\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nx_item\n(str, int)\nitem name or index of x-coordinate, by default 0\n0\n\n\ny_item\n(str, int)\nitem name or index of y-coordinate, by default 1\n1\n\n\nkeep_duplicates\n(str, bool)\nstrategy for handling duplicate timestamps (xarray.Dataset.drop_duplicates): “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=2, name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=\"wind_speed\", name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track_wl.dfs0\", item=\"wl\", x_item=\"lon\", y_item=\"lat\")\n&gt;&gt;&gt; df = pd.DataFrame(\n...         {\n...             \"t\": pd.date_range(\"2010-01-01\", freq=\"10s\", periods=n),\n...             \"x\": np.linspace(0, 10, n),\n...             \"y\": np.linspace(45000, 45100, n),\n...             \"swh\": [0.1, 0.3, 0.4, 0.5, 0.3],\n...         }\n... )\n&gt;&gt;&gt; df = df.set_index(\"t\")\n&gt;&gt;&gt; df\n                    x        y  swh\nt\n2010-01-01 00:00:00   0.0  45000.0  0.1\n2010-01-01 00:00:10   2.5  45025.0  0.3\n2010-01-01 00:00:20   5.0  45050.0  0.4\n2010-01-01 00:00:30   7.5  45075.0  0.5\n2010-01-01 00:00:40  10.0  45100.0  0.3\n&gt;&gt;&gt; t1 = TrackObservation(df, name=\"fake\")\n&gt;&gt;&gt; t1.n_points\n5\n&gt;&gt;&gt; t1.values\narray([0.1, 0.3, 0.4, 0.5, 0.3])\n&gt;&gt;&gt; t1.time\nDatetimeIndex(['2010-01-01 00:00:00', '2010-01-01 00:00:10',\n           '2010-01-01 00:00:20', '2010-01-01 00:00:30',\n           '2010-01-01 00:00:40'],\n          dtype='datetime64[ns]', name='t', freq=None)\n&gt;&gt;&gt; t1.x\narray([ 0. ,  2.5,  5. ,  7.5, 10. ])\n&gt;&gt;&gt; t1.y\narray([45000., 45025., 45050., 45075., 45100.])"
  },
  {
    "objectID": "reference/TrackObservation.html#parameters",
    "href": "reference/TrackObservation.html#parameters",
    "title": "TrackObservation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, pandas.pandas.DataFrame, xarray.xarray.Dataset)\npath to dfs0 file or object with track data\nrequired\n\n\nitem\n(str, int)\nitem name or index of values, by default None if data contains more than one item, item must be given\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nx_item\n(str, int)\nitem name or index of x-coordinate, by default 0\n0\n\n\ny_item\n(str, int)\nitem name or index of y-coordinate, by default 1\n1\n\n\nkeep_duplicates\n(str, bool)\nstrategy for handling duplicate timestamps (xarray.Dataset.drop_duplicates): “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0"
  },
  {
    "objectID": "reference/TrackObservation.html#examples",
    "href": "reference/TrackObservation.html#examples",
    "title": "TrackObservation",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=2, name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=\"wind_speed\", name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track_wl.dfs0\", item=\"wl\", x_item=\"lon\", y_item=\"lat\")\n&gt;&gt;&gt; df = pd.DataFrame(\n...         {\n...             \"t\": pd.date_range(\"2010-01-01\", freq=\"10s\", periods=n),\n...             \"x\": np.linspace(0, 10, n),\n...             \"y\": np.linspace(45000, 45100, n),\n...             \"swh\": [0.1, 0.3, 0.4, 0.5, 0.3],\n...         }\n... )\n&gt;&gt;&gt; df = df.set_index(\"t\")\n&gt;&gt;&gt; df\n                    x        y  swh\nt\n2010-01-01 00:00:00   0.0  45000.0  0.1\n2010-01-01 00:00:10   2.5  45025.0  0.3\n2010-01-01 00:00:20   5.0  45050.0  0.4\n2010-01-01 00:00:30   7.5  45075.0  0.5\n2010-01-01 00:00:40  10.0  45100.0  0.3\n&gt;&gt;&gt; t1 = TrackObservation(df, name=\"fake\")\n&gt;&gt;&gt; t1.n_points\n5\n&gt;&gt;&gt; t1.values\narray([0.1, 0.3, 0.4, 0.5, 0.3])\n&gt;&gt;&gt; t1.time\nDatetimeIndex(['2010-01-01 00:00:00', '2010-01-01 00:00:10',\n           '2010-01-01 00:00:20', '2010-01-01 00:00:30',\n           '2010-01-01 00:00:40'],\n          dtype='datetime64[ns]', name='t', freq=None)\n&gt;&gt;&gt; t1.x\narray([ 0. ,  2.5,  5. ,  7.5, 10. ])\n&gt;&gt;&gt; t1.y\narray([45000., 45025., 45050., 45075., 45100.])"
  },
  {
    "objectID": "reference/PointModelResult.html",
    "href": "reference/PointModelResult.html",
    "title": "PointModelResult",
    "section": "",
    "text": "PointModelResult(self, data, *, name=None, x=None, y=None, z=None, item=None, quantity=None, aux_items=None)\nConstruct a PointModelResult from a 0d data source: dfs0 file, mikeio.Dataset/DataArray, pandas.DataFrame/Series or xarray.Dataset/DataArray\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nx\nfloat\nfirst coordinate of point position, inferred from data if not given, else None\nNone\n\n\ny\nfloat\nsecond coordinate of point position, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nthird coordinate of point position, inferred from data if not given, else None\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "reference/PointModelResult.html#parameters",
    "href": "reference/PointModelResult.html#parameters",
    "title": "PointModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nx\nfloat\nfirst coordinate of point position, inferred from data if not given, else None\nNone\n\n\ny\nfloat\nsecond coordinate of point position, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nthird coordinate of point position, inferred from data if not given, else None\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "reference/index.html#model-result",
    "href": "reference/index.html#model-result",
    "title": "Function reference",
    "section": "",
    "text": "Model result classes and functions\n\n\n\nmodel_result\nA factory function for creating an appropriate object based on the data input.\n\n\nPointModelResult\nConstruct a PointModelResult from a 0d data source:\n\n\nTrackModelResult\nConstruct a TrackModelResult from a dfs0 file,\n\n\nDfsuModelResult\nConstruct a DfsuModelResult from a dfsu file or mikeio.Dataset/DataArray.\n\n\nGridModelResult\nConstruct a GridModelResult from a file or xarray.Dataset.\n\n\nDummyModelResult"
  },
  {
    "objectID": "reference/index.html#matching",
    "href": "reference/index.html#matching",
    "title": "Function reference",
    "section": "",
    "text": "Matching classes and functions\n\n\n\nmatching"
  },
  {
    "objectID": "reference/index.html#comparer",
    "href": "reference/index.html#comparer",
    "title": "Function reference",
    "section": "",
    "text": "Comparer classes and functions\n\n\n\nComparer\nComparer class for comparing model and observation data.\n\n\nComparerCollection\nCollection of comparers, constructed by calling the modelskill.match"
  },
  {
    "objectID": "reference/index.html#metrics",
    "href": "reference/index.html#metrics",
    "title": "Function reference",
    "section": "",
    "text": "Metrics classes and functions\n\n\n\nmetrics\nThe metrics module contains different skill metrics for evaluating the"
  },
  {
    "objectID": "reference/DfsuModelResult.html",
    "href": "reference/DfsuModelResult.html",
    "title": "DfsuModelResult",
    "section": "",
    "text": "DfsuModelResult(self, data, *, name=None, item=None, quantity=None, aux_items=None)\nConstruct a DfsuModelResult from a dfsu file or mikeio.Dataset/DataArray.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.UnstructuredType\nthe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "reference/DfsuModelResult.html#parameters",
    "href": "reference/DfsuModelResult.html#parameters",
    "title": "DfsuModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.UnstructuredType\nthe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "reference/DummyModelResult.html",
    "href": "reference/DummyModelResult.html",
    "title": "DummyModelResult",
    "section": "",
    "text": "DummyModelResult\nDummyModelResult(name='dummy', data=None, strategy='constant')"
  },
  {
    "objectID": "reference/matching.html",
    "href": "reference/matching.html",
    "title": "matching",
    "section": "",
    "text": "matching\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_matched\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\nmatch\nMatch observation and model result data in space and time\n\n\nmatch_space_time\nMatch observation with one or more model results in time domain\n\n\n\n\n\nmatching.from_matched(data, *, obs_item=0, mod_items=None, aux_items=None, quantity=None, name=None, weight=1.0, x=None, y=None, z=None, x_item=None, y_item=None)\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n[pandas.pandas.DataFrame, str, pathlib.Path, mikeio.mikeio.Dfs0, mikeio.mikeio.Dataset]\nDataFrame (or object that can be converted to a DataFrame e.g. dfs0) with columns obs_item, mod_items, aux_items\nrequired\n\n\nobs_item\n[str, int]\nName or index of observation item, by default first item\n0\n\n\nmod_items\ntyping.Iterable[str, int]\nNames or indicies of model items, if None all remaining columns are model items, by default None\nNone\n\n\naux_items\ntyping.Iterable[str, int]\nNames or indicies of auxiliary items, by default None\nNone\n\n\nquantity\nmodelskill.Quantity\nQuantity of the observation and model results, by default Quantity(name=“Undefined”, unit=“Undefined”)\nNone\n\n\nname\nstr\nName of the comparer, by default None (will be set to obs_item)\nNone\n\n\nx\nfloat\nx-coordinate of observation, by default None\nNone\n\n\ny\nfloat\ny-coordinate of observation, by default None\nNone\n\n\nz\nfloat\nz-coordinate of observation, by default None\nNone\n\n\nx_item\nstr | int | None\nName of x item, only relevant for track data\nNone\n\n\ny_item\nstr | int | None\nName of y item, only relevant for track data\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a') # remaining columns are model results\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n Model: local, rmse=0.100\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1], 'global': [1.2,2.2,3.2], 'nonsense':[1,2,3]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a', mod_items=['local', 'global'])\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n    Model: local, rmse=0.100\n    Model: global, rmse=0.200\n\n\n\n\nmatching.match(obs, mod, *, obs_item=None, mod_item=None, gtype=None, max_model_gap=None, spatial_method=None)\nMatch observation and model result data in space and time\nNOTE: In case of multiple model results with different time coverage, only the overlapping time period will be used! (intersection)\nNOTE: In case of multiple observations, multiple models can only be matched if they are all of SpatialField type, e.g. DfsuModelResult or GridModelResult.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.obs.Observation, typing.Sequence[modelskill.obs.Observation])\nObservation(s) to be compared\nrequired\n\n\nmod\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.model.factory.ModelResult, typing.Sequence[modelskill.model.factory.ModelResult])\nModel result(s) to be compared\nrequired\n\n\nobs_item\nint or str\nobservation item if obs is a file/dataframe, by default None\nNone\n\n\nmod_item\n(int, str)\nmodel item if mod is a file/dataframe, by default None\nNone\n\n\ngtype\n(str, optional)\nGeometry type of the model result (if mod is a file/dataframe). If not specified, it will be guessed.\nNone\n\n\nmax_model_gap\n(float, optional)\nMaximum time gap (s) in the model result (e.g. for event-based model results), by default None\nNone\n\n\nspatial_method\nstr\nFor Dfsu- and GridModelResult, spatial interpolation/selection method. - For DfsuModelResult, one of: ‘contained’ (=isel), ‘nearest’, ‘inverse_distance’ (with 5 nearest points), by default “inverse_distance”. - For GridModelResult, passed to xarray.interp() as method argument, by default ‘linear’.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nmodelskill.comparison.Comparer\nIn case of a single observation\n\n\nmodelskill.comparison.ComparerCollection\nIn case of multiple observations\n\n\n\n\n\n\n[from_matched][modelskill.from_matched] Create a Comparer from observation and model results that are already matched\n\n\n\n\nmatching.match_space_time(observation, raw_mod_data, max_model_gap=None, spatial_tolerance=0.001)\nMatch observation with one or more model results in time domain and return as xr.Dataset in the format used by modelskill.Comparer\nWill interpolate model results to observation time.\nNote: assumes that observation and model data are already matched in space. But positions of track observations will be checked.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobservation\nmodelskill.obs.Observation\nObservation to be matched\nrequired\n\n\nraw_mod_data\ntyping.Dict[str, modelskill.model.point.PointModelResult | modelskill.model.track.TrackModelResult]\nDictionary of model results ready for interpolation\nrequired\n\n\nmax_model_gap\ntyping.Optional[modelskill.matching.TimeDeltaTypes]\nIn case of non-equidistant model results (e.g. event data), max_model_gap can be given e.g. as seconds, by default None\nNone\n\n\nspatial_tolerance\nfloat\nTolerance for spatial matching, by default 1e-3\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.xarray.Dataset\nMatched data in the format used by modelskill.Comparer"
  },
  {
    "objectID": "reference/matching.html#functions",
    "href": "reference/matching.html#functions",
    "title": "matching",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_matched\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\nmatch\nMatch observation and model result data in space and time\n\n\nmatch_space_time\nMatch observation with one or more model results in time domain\n\n\n\n\n\nmatching.from_matched(data, *, obs_item=0, mod_items=None, aux_items=None, quantity=None, name=None, weight=1.0, x=None, y=None, z=None, x_item=None, y_item=None)\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n[pandas.pandas.DataFrame, str, pathlib.Path, mikeio.mikeio.Dfs0, mikeio.mikeio.Dataset]\nDataFrame (or object that can be converted to a DataFrame e.g. dfs0) with columns obs_item, mod_items, aux_items\nrequired\n\n\nobs_item\n[str, int]\nName or index of observation item, by default first item\n0\n\n\nmod_items\ntyping.Iterable[str, int]\nNames or indicies of model items, if None all remaining columns are model items, by default None\nNone\n\n\naux_items\ntyping.Iterable[str, int]\nNames or indicies of auxiliary items, by default None\nNone\n\n\nquantity\nmodelskill.Quantity\nQuantity of the observation and model results, by default Quantity(name=“Undefined”, unit=“Undefined”)\nNone\n\n\nname\nstr\nName of the comparer, by default None (will be set to obs_item)\nNone\n\n\nx\nfloat\nx-coordinate of observation, by default None\nNone\n\n\ny\nfloat\ny-coordinate of observation, by default None\nNone\n\n\nz\nfloat\nz-coordinate of observation, by default None\nNone\n\n\nx_item\nstr | int | None\nName of x item, only relevant for track data\nNone\n\n\ny_item\nstr | int | None\nName of y item, only relevant for track data\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a') # remaining columns are model results\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n Model: local, rmse=0.100\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1], 'global': [1.2,2.2,3.2], 'nonsense':[1,2,3]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a', mod_items=['local', 'global'])\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n    Model: local, rmse=0.100\n    Model: global, rmse=0.200\n\n\n\n\nmatching.match(obs, mod, *, obs_item=None, mod_item=None, gtype=None, max_model_gap=None, spatial_method=None)\nMatch observation and model result data in space and time\nNOTE: In case of multiple model results with different time coverage, only the overlapping time period will be used! (intersection)\nNOTE: In case of multiple observations, multiple models can only be matched if they are all of SpatialField type, e.g. DfsuModelResult or GridModelResult.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.obs.Observation, typing.Sequence[modelskill.obs.Observation])\nObservation(s) to be compared\nrequired\n\n\nmod\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.model.factory.ModelResult, typing.Sequence[modelskill.model.factory.ModelResult])\nModel result(s) to be compared\nrequired\n\n\nobs_item\nint or str\nobservation item if obs is a file/dataframe, by default None\nNone\n\n\nmod_item\n(int, str)\nmodel item if mod is a file/dataframe, by default None\nNone\n\n\ngtype\n(str, optional)\nGeometry type of the model result (if mod is a file/dataframe). If not specified, it will be guessed.\nNone\n\n\nmax_model_gap\n(float, optional)\nMaximum time gap (s) in the model result (e.g. for event-based model results), by default None\nNone\n\n\nspatial_method\nstr\nFor Dfsu- and GridModelResult, spatial interpolation/selection method. - For DfsuModelResult, one of: ‘contained’ (=isel), ‘nearest’, ‘inverse_distance’ (with 5 nearest points), by default “inverse_distance”. - For GridModelResult, passed to xarray.interp() as method argument, by default ‘linear’.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nmodelskill.comparison.Comparer\nIn case of a single observation\n\n\nmodelskill.comparison.ComparerCollection\nIn case of multiple observations\n\n\n\n\n\n\n[from_matched][modelskill.from_matched] Create a Comparer from observation and model results that are already matched\n\n\n\n\nmatching.match_space_time(observation, raw_mod_data, max_model_gap=None, spatial_tolerance=0.001)\nMatch observation with one or more model results in time domain and return as xr.Dataset in the format used by modelskill.Comparer\nWill interpolate model results to observation time.\nNote: assumes that observation and model data are already matched in space. But positions of track observations will be checked.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobservation\nmodelskill.obs.Observation\nObservation to be matched\nrequired\n\n\nraw_mod_data\ntyping.Dict[str, modelskill.model.point.PointModelResult | modelskill.model.track.TrackModelResult]\nDictionary of model results ready for interpolation\nrequired\n\n\nmax_model_gap\ntyping.Optional[modelskill.matching.TimeDeltaTypes]\nIn case of non-equidistant model results (e.g. event data), max_model_gap can be given e.g. as seconds, by default None\nNone\n\n\nspatial_tolerance\nfloat\nTolerance for spatial matching, by default 1e-3\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.xarray.Dataset\nMatched data in the format used by modelskill.Comparer"
  },
  {
    "objectID": "reference/ComparerCollection.html",
    "href": "reference/ComparerCollection.html",
    "title": "ComparerCollection",
    "section": "",
    "text": "ComparerCollection(self, comparers)\nCollection of comparers, constructed by calling the modelskill.match method or by initializing with a list of comparers.\nNOTE: In case of multiple model results with different time coverage, only the overlapping time period will be used! (intersection)\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; mr = ms.DfsuModelResult(\"Oresund2D.dfsu\", item=0)\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"drogden.dfs0\", item=0, x=355568.0, y=6156863.0)\n&gt;&gt;&gt; cmp1 = ms.match(o1, mr)  # Comparer\n&gt;&gt;&gt; cmp2 = ms.match(o2, mr)  # Comparer\n&gt;&gt;&gt; ccA = ms.ComparerCollection([cmp1, cmp2])\n&gt;&gt;&gt; ccB = ms.match(obs=[o1, o2], mod=mr)\n&gt;&gt;&gt; sk = ccB.skill()\n&gt;&gt;&gt; ccB[\"Klagshamn\"].plot.timeseries()"
  },
  {
    "objectID": "reference/ComparerCollection.html#examples",
    "href": "reference/ComparerCollection.html#examples",
    "title": "ComparerCollection",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; mr = ms.DfsuModelResult(\"Oresund2D.dfsu\", item=0)\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"drogden.dfs0\", item=0, x=355568.0, y=6156863.0)\n&gt;&gt;&gt; cmp1 = ms.match(o1, mr)  # Comparer\n&gt;&gt;&gt; cmp2 = ms.match(o2, mr)  # Comparer\n&gt;&gt;&gt; ccA = ms.ComparerCollection([cmp1, cmp2])\n&gt;&gt;&gt; ccB = ms.match(obs=[o1, o2], mod=mr)\n&gt;&gt;&gt; sk = ccB.skill()\n&gt;&gt;&gt; ccB[\"Klagshamn\"].plot.timeseries()"
  },
  {
    "objectID": "reference/Comparer.html",
    "href": "reference/Comparer.html",
    "title": "Comparer",
    "section": "",
    "text": "Comparer(self, matched_data, raw_mod_data=None)\nComparer class for comparing model and observation data.\nTypically, the Comparer is part of a ComparerCollection, created with the match function.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatched_data\nxarray.xarray.Dataset\nMatched data\nrequired\n\n\nraw_mod_data\ndict of modelskill.TimeSeries\nRaw model data. If None, observation and modeldata must be provided.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; cmp1 = ms.match(observation, modeldata)\n&gt;&gt;&gt; cmp2 = ms.from_matched(matched_data)\n\n\n\nmodelskill.match, modelskill.from_matched"
  },
  {
    "objectID": "reference/Comparer.html#parameters",
    "href": "reference/Comparer.html#parameters",
    "title": "Comparer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatched_data\nxarray.xarray.Dataset\nMatched data\nrequired\n\n\nraw_mod_data\ndict of modelskill.TimeSeries\nRaw model data. If None, observation and modeldata must be provided.\nNone"
  },
  {
    "objectID": "reference/Comparer.html#examples",
    "href": "reference/Comparer.html#examples",
    "title": "Comparer",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; cmp1 = ms.match(observation, modeldata)\n&gt;&gt;&gt; cmp2 = ms.from_matched(matched_data)"
  },
  {
    "objectID": "reference/Comparer.html#see-also",
    "href": "reference/Comparer.html#see-also",
    "title": "Comparer",
    "section": "",
    "text": "modelskill.match, modelskill.from_matched"
  },
  {
    "objectID": "reference/metrics.html",
    "href": "reference/metrics.html",
    "title": "metrics",
    "section": "",
    "text": "metrics\nThe metrics module contains different skill metrics for evaluating the difference between a model and an observation.\n\n[bias][modelskill.metrics.bias]\n[max_error][modelskill.metrics.max_error]\n[root_mean_squared_error (rmse)][modelskill.metrics.root_mean_squared_error]\n\n[urmse][modelskill.metrics.urmse]\n[mean_absolute_error (mae)][modelskill.metrics.mean_absolute_error]\n[mean_absolute_percentage_error (mape)][modelskill.metrics.mean_absolute_percentage_error]\n[kling_gupta_efficiency (kge)][modelskill.metrics.kling_gupta_efficiency]\n[nash_sutcliffe_efficiency (nse)][modelskill.metrics.nash_sutcliffe_efficiency]\n[r2 (r2=nse)][modelskill.metrics.r2]\n[model_efficiency_factor (mef)][modelskill.metrics.model_efficiency_factor]\n[wilmott][modelskill.metrics.willmott]\n[scatter_index (si)][modelskill.metrics.scatter_index]\n[scatter_index2][modelskill.metrics.scatter_index2]\n[corrcoef (cc)][modelskill.metrics.corrcoef]\n[spearmanr (rho)][modelskill.metrics.spearmanr]\n[lin_slope][modelskill.metrics.lin_slope]\n[hit_ratio][modelskill.metrics.hit_ratio]\n[explained_variance (ev)][modelskill.metrics.explained_variance]\n[peak_ratio (pr)][modelskill.metrics.peak_ratio]\n\nCircular metrics (for directional data with units in degrees):\n\n[c_bias][modelskill.metrics.c_bias]\n[c_max_error][modelskill.metrics.c_max_error]\n[c_mean_absolute_error (c_mae)][modelskill.metrics.c_mean_absolute_error]\n[c_root_mean_squared_error (c_rmse)][modelskill.metrics.c_root_mean_squared_error]\n[c_unbiased_root_mean_squared_error (c_urmse)][modelskill.metrics.c_unbiased_root_mean_squared_error]\n\nThe names in parentheses are shorthand aliases for the different metrics.\n\n\n&gt;&gt;&gt; obs = np.array([0.3, 2.1, -1.0])\n&gt;&gt;&gt; mod = np.array([0.0, 2.3, 1.0])\n&gt;&gt;&gt; bias(obs, mod)\n0.6333333333333332\n&gt;&gt;&gt; max_error(obs, mod)\n2.0\n&gt;&gt;&gt; rmse(obs, mod)\n1.173314393786536\n&gt;&gt;&gt; urmse(obs, mod)\n0.9877021593352702\n&gt;&gt;&gt; mae(obs, mod)\n0.8333333333333331\n&gt;&gt;&gt; mape(obs, mod)\n103.17460317460316\n&gt;&gt;&gt; nse(obs, mod)\n0.14786795048143053\n&gt;&gt;&gt; r2(obs, mod)\n0.14786795048143053\n&gt;&gt;&gt; mef(obs, mod)\n0.9231099877688299\n&gt;&gt;&gt; si(obs, mod)\n0.8715019052958266\n&gt;&gt;&gt; spearmanr(obs, mod)\n0.5\n&gt;&gt;&gt; willmott(obs, mod)\n0.7484604452865941\n&gt;&gt;&gt; hit_ratio(obs, mod, a=0.5)\n0.6666666666666666\n&gt;&gt;&gt; ev(obs, mod)\n0.39614855570839064\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_metric\nAdds a metric to the metric list. Useful for custom metrics.\n\n\nbias\nBias (mean error)\n\n\nc_bias\nCircular bias (mean error)\n\n\nc_mae\nalias for circular mean absolute error\n\n\nc_max_error\nCircular max error\n\n\nc_mean_absolute_error\nCircular mean absolute error\n\n\nc_rmse\nalias for circular root mean squared error\n\n\nc_root_mean_squared_error\nCircular root mean squared error\n\n\nc_unbiased_root_mean_squared_error\nCircular unbiased root mean squared error\n\n\nc_urmse\nalias for circular unbiased root mean squared error\n\n\ncc\nalias for corrcoef\n\n\ncorrcoef\nPearson’s Correlation coefficient (CC)\n\n\nev\nalias for explained_variance\n\n\nexplained_variance\nEV: Explained variance\n\n\nhit_ratio\nFraction within obs ± acceptable deviation\n\n\nkge\nalias for kling_gupta_efficiency\n\n\nkling_gupta_efficiency\nKling-Gupta Efficiency (KGE)\n\n\nlin_slope\nSlope of the regression line.\n\n\nmae\nalias for mean_absolute_error\n\n\nmape\nalias for mean_absolute_percentage_error\n\n\nmax_error\nMax (absolute) error\n\n\nmean_absolute_error\nMean Absolute Error (MAE)\n\n\nmean_absolute_percentage_error\nMean Absolute Percentage Error (MAPE)\n\n\nmef\nalias for model_efficiency_factor\n\n\nmetric_has_units\nCheck if a metric has units (dimension).\n\n\nmodel_efficiency_factor\nModel Efficiency Factor (MEF)\n\n\nnash_sutcliffe_efficiency\nNash-Sutcliffe Efficiency (NSE)\n\n\nnse\nalias for nash_sutcliffe_efficiency\n\n\npeak_ratio\nPeak Ratio\n\n\npr\nalias for peak_ratio\n\n\nr2\nCoefficient of determination (R2)\n\n\nrho\nalias for spearmanr\n\n\nrmse\nalias for root_mean_squared_error\n\n\nroot_mean_squared_error\nRoot Mean Squared Error (RMSE)\n\n\nscatter_index\nScatter index (SI)\n\n\nscatter_index2\nAlternative formulation of the scatter index (SI)\n\n\nsi\nalias for scatter_index\n\n\nspearmanr\nSpearman rank correlation coefficient\n\n\nurmse\nUnbiased Root Mean Squared Error (uRMSE)\n\n\nwillmott\nWillmott’s Index of Agreement\n\n\n\n\n\nmetrics.add_metric(metric, has_units=False)\nAdds a metric to the metric list. Useful for custom metrics.\nSome metrics are dimensionless, others have the same dimension as the observations.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetric\nstr or callable\nMetric name or function\nrequired\n\n\nhas_units\nbool\nTrue if metric has a dimension, False otherwise. Default:False\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNone\n\n\n\n\n\n\n\n&gt;&gt;&gt; add_metric(hit_ratio)\n&gt;&gt;&gt; add_metric(rmse,True)\n\n\n\n\nmetrics.bias(obs, model)\nBias (mean error)\n\\[\nbias=\\frac{1}{n}\\sum_{i=1}^n (model_i - obs_i)\n\\]\nRange: \\((-\\infty, \\infty)\\); Best: 0\n\n\n\nmetrics.c_bias(obs, model)\nCircular bias (mean error)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\n\n\n\n\nRange: \\([-180., 180.]\\); Best: 0.\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular bias\n\n\n\n\n\n\n&gt;&gt;&gt; obs = np.array([10., 355., 170.])\n&gt;&gt;&gt; mod = np.array([20., 5., -180.])\n&gt;&gt;&gt; c_bias(obs, mod)\n10.0\n\n\n\n\nmetrics.c_mae(obs, model, weights=None)\nalias for circular mean absolute error\n\n\n\nmetrics.c_max_error(obs, model)\nCircular max error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\n\n\n\n\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular max error\n\n\n\n\n\n\n&gt;&gt;&gt; obs = np.array([10., 350., 10.])\n&gt;&gt;&gt; mod = np.array([20., 10., 350.])\n&gt;&gt;&gt; c_max_error(obs, mod)\n20.0\n\n\n\n\nmetrics.c_mean_absolute_error(obs, model, weights=None)\nCircular mean absolute error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular mean absolute error\n\n\n\n\n\n\n\nmetrics.c_rmse(obs, model, weights=None)\nalias for circular root mean squared error\n\n\n\nmetrics.c_root_mean_squared_error(obs, model, weights=None)\nCircular root mean squared error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular root mean squared error\n\n\n\n\n\n\n\nmetrics.c_unbiased_root_mean_squared_error(obs, model, weights=None)\nCircular unbiased root mean squared error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular unbiased root mean squared error\n\n\n\n\n\n\n\nmetrics.c_urmse(obs, model, weights=None)\nalias for circular unbiased root mean squared error\n\n\n\nmetrics.cc(obs, model, weights=None)\nalias for corrcoef\n\n\n\nmetrics.corrcoef(obs, model, weights=None)\nPearson’s Correlation coefficient (CC)\n\\[\nCC = \\frac{\\sum_{i=1}^n (model_i - \\overline{model})(obs_i - \\overline{obs}) }\n               {\\sqrt{\\sum_{i=1}^n (model_i - \\overline{model})^2}\n                \\sqrt{\\sum_{i=1}^n (obs_i - \\overline{obs})^2} }\n\\]\nRange: [-1, 1]; Best: 1\n\n\nspearmanr np.corrcoef\n\n\n\n\nmetrics.ev(obs, model)\nalias for explained_variance\n\n\n\nmetrics.explained_variance(obs, model)\nEV: Explained variance\nEV is the explained variance and measures the proportion [0 - 1] to which the model accounts for the variation (dispersion) of the observations.\nIn cases with no bias, EV is equal to r2\n\\[\n\\frac{ \\sum_{i=1}^n (obs_i - \\overline{obs})^2 -\n\\sum_{i=1}^n \\left( (obs_i - \\overline{obs}) -\n(model_i - \\overline{model}) \\right)^2}{\\sum_{i=1}^n\n(obs_i - \\overline{obs})^2}\n\\]\nRange: [0, 1]; Best: 1\n\n\nr2\n\n\n\n\nmetrics.hit_ratio(obs, model, a=0.1)\nFraction within obs ± acceptable deviation\n\\[\nHR = \\frac{1}{n}\\sum_{i=1}^n I_{|(model_i - obs_i)|} &lt; a\n\\]\nRange: [0, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.array([1.0, 1.1, 1.2, 1.3, 1.4, 1.4, 1.3])\n&gt;&gt;&gt; model = np.array([1.02, 1.16, 1.3, 1.38, 1.49, 1.45, 1.32])\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.05)\n0.2857142857142857\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.1)\n0.8571428571428571\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.15)\n1.0\n\n\n\n\nmetrics.kge(obs, model)\nalias for kling_gupta_efficiency\n\n\n\nmetrics.kling_gupta_efficiency(obs, model)\nKling-Gupta Efficiency (KGE)\n\\[\nKGE = 1 - \\sqrt{(r-1)^2 + \\left(\\frac{\\sigma_{mod}}{\\sigma_{obs}} - 1\\right)^2 +\n                            \\left(\\frac{\\mu_{mod}}{\\mu_{obs}} - 1\\right)^2 }\n\\]\nwhere \\(r\\) is the pearson correlation coefficient, \\(\\mu_{obs},\\mu_{mod}\\) and \\(\\sigma_{obs},\\sigma_{mod}\\) is the mean and standard deviation of observations and model.\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nGupta, H. V., Kling, H., Yilmaz, K. K. and Martinez, G. F., (2009), Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling, J. Hydrol., 377(1-2), 80-91 https://doi.org/10.1016/j.jhydrol.2009.08.003\nKnoben, W. J. M., Freer, J. E., and Woods, R. A. (2019) Technical note: Inherent benchmark or not? Comparing Nash–Sutcliffe and Kling–Gupta efficiency scores, Hydrol. Earth Syst. Sci., 23, 4323-4331 https://doi.org/10.5194/hess-23-4323-2019\n\n\n\n\nmetrics.lin_slope(obs, model, reg_method='ols')\nSlope of the regression line.\n\\[\nslope = \\frac{\\sum_{i=1}^n (model_i - \\overline {model})(obs_i - \\overline {obs})}\n                {\\sum_{i=1}^n (obs_i - \\overline {obs})^2}\n\\]\nRange: \\((-\\infty, \\infty )\\); Best: 1\n\n\n\nmetrics.mae(obs, model, weights=None)\nalias for mean_absolute_error\n\n\n\nmetrics.mape(obs, model)\nalias for mean_absolute_percentage_error\n\n\n\nmetrics.max_error(obs, model)\nMax (absolute) error\n\\[\nmax_{error} = max(|model_i - obs_i|)\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mean_absolute_error(obs, model, weights=None)\nMean Absolute Error (MAE)\n\\[\nMAE=\\frac{1}{n}\\sum_{i=1}^n|model_i - obs_i|\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mean_absolute_percentage_error(obs, model)\nMean Absolute Percentage Error (MAPE)\n\\[\nMAPE=\\frac{1}{n}\\sum_{i=1}^n\\frac{|model_i - obs_i|}{obs_i}*100\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mef(obs, model)\nalias for model_efficiency_factor\n\n\n\nmetrics.metric_has_units(metric)\nCheck if a metric has units (dimension).\nSome metrics are dimensionless, others have the same dimension as the observations.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetric\nstr or callable\nMetric name or function\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbool\nTrue if metric has a dimension, False otherwise\n\n\n\n\n\n\n&gt;&gt;&gt; metric_has_units(\"rmse\")\nTrue\n&gt;&gt;&gt; metric_has_units(\"kge\")\nFalse\n\n\n\n\nmetrics.model_efficiency_factor(obs, model)\nModel Efficiency Factor (MEF)\nScale independent RMSE, standardized by Stdev of observations\n\\[\nMEF = \\frac{RMSE}{STDEV}=\\frac{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(model_i - obs_i)^2}}\n                                {\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(obs_i - \\overline{obs})^2}}=\\sqrt{1-NSE}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\nnash_sutcliffe_efficiency root_mean_squared_error\n\n\n\n\nmetrics.nash_sutcliffe_efficiency(obs, model)\nNash-Sutcliffe Efficiency (NSE)\n\\[\nNSE = 1 - \\frac {\\sum _{i=1}^{n}\\left(model_{i} - obs_{i}\\right)^{2}}\n                {\\sum_{i=1}^{n}\\left(obs_{i} - {\\overline{obs}}\\right)^{2}}\n\\]\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nr2 = nash_sutcliffe_efficiency(nse)\n\n\n\nNash, J. E.; Sutcliffe, J. V. (1970). “River flow forecasting through conceptual models part I — A discussion of principles”. Journal of Hydrology. 10 (3): 282–290. https://doi.org/10.1016/0022-1694(70)90255-6\n\n\n\n\nmetrics.nse(obs, model)\nalias for nash_sutcliffe_efficiency\n\n\n\nmetrics.peak_ratio(obs, model, inter_event_level=0.7, AAP=2, inter_event_time='36h')\nPeak Ratio\nPR is the mean of the individual ratios of identified peaks in the model / identified peaks in the measurements. PR is calculated only for the joint-events, ie, events that ocurr simulateneously within a window +/- 0.5*inter_event_time.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninter_event_level\nfloat\nInter-event level threshold (default: 0.7).\n0.7\n\n\nAAP\nint\nAverage Annual Peaks (ie, Number of peaks per year, on average). (default: 2)\n2\n\n\ninter_event_time\n\nMaximum time interval between peaks (default: 36 hours).\n'36h'\n\n\n\n\n\n\n\\(\\frac{\\sum_{i=1}^{N_{joint-peaks}} (\\frac{Peak_{model_i}}{Peak_{obs_i}} )}{N_{joint-peaks}}\\)\nRange: \\([0, \\infty)\\); Best: 1.0\n\n\n\n\nmetrics.pr(obs, model, inter_event_level=0.7, AAP=2, inter_event_time='36h')\nalias for peak_ratio\n\n\n\nmetrics.r2(obs, model)\nCoefficient of determination (R2)\nPronounced ‘R-squared’; the proportion of the variation in the dependent variable that is predictable from the independent variable(s), i.e. the proportion of explained variance.\n\\[\nR^2 = 1 - \\frac{\\sum_{i=1}^n (model_i - obs_i)^2}\n                {\\sum_{i=1}^n (obs_i - \\overline {obs})^2}\n\\]\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nr2 = nash_sutcliffe_efficiency(nse)\n\n\n\n&gt;&gt;&gt; obs = np.array([1.0,1.1,1.2,1.3,1.4])\n&gt;&gt;&gt; model = np.array([1.09, 1.16, 1.3 , 1.38, 1.49])\n&gt;&gt;&gt; r2(obs,model)\n0.6379999999999998\n\n\n\n\nmetrics.rho(obs, model)\nalias for spearmanr\n\n\n\nmetrics.rmse(obs, model, weights=None, unbiased=False)\nalias for root_mean_squared_error\n\n\n\nmetrics.root_mean_squared_error(obs, model, weights=None, unbiased=False)\nRoot Mean Squared Error (RMSE)\n\\[\nres_i = model_i - obs_i\n\\]\n\\[\nRMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_i^2}\n\\]\nUnbiased version:\n\\[\nres_{u,i} = res_i - \\overline {res}\n\\]\n\\[\nuRMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_{u,i}^2}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.scatter_index(obs, model)\nScatter index (SI)\nWhich is the same as the unbiased-RMSE normalized by the absolute mean of the observations.\n\\[\n\\frac{ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left( (model_i - \\overline {model}) - (obs_i - \\overline {obs}) \\right)^2} }\n{\\frac{1}{n} \\sum_{i=1}^n | obs_i | }\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.scatter_index2(obs, model)\nAlternative formulation of the scatter index (SI)\n\\[\n\\sqrt {\\frac{\\sum_{i=1}^n \\left( (model_i - \\overline {model}) - (obs_i - \\overline {obs}) \\right)^2}\n{\\sum_{i=1}^n obs_i^2}}\n\\]\nRange: [0, 100]; Best: 0\n\n\n\nmetrics.si(obs, model)\nalias for scatter_index\n\n\n\nmetrics.spearmanr(obs, model)\nSpearman rank correlation coefficient\nThe rank correlation coefficient is similar to the Pearson correlation coefficient but applied to ranked quantities and is useful to quantify a monotonous relationship\n\\[\n\\rho = \\frac{\\sum_{i=1}^n (rmodel_i - \\overline{rmodel})(robs_i - \\overline{robs}) }\n                {\\sqrt{\\sum_{i=1}^n (rmodel_i - \\overline{rmodel})^2}\n                \\sqrt{\\sum_{i=1}^n (robs_i - \\overline{robs})^2} }\n\\]\nRange: [-1, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.linspace(-20, 20, 100)\n&gt;&gt;&gt; mod = np.tanh(obs)\n&gt;&gt;&gt; rho(obs, mod)\n0.9999759973116955\n&gt;&gt;&gt; spearmanr(obs, mod)\n0.9999759973116955\n\n\n\ncorrcoef\n\n\n\n\nmetrics.urmse(obs, model, weights=None)\nUnbiased Root Mean Squared Error (uRMSE)\n\\[\nres_i = model_i - obs_i\n\\]\n\\[\nres_{u,i} = res_i - \\overline {res}\n\\]\n\\[\nuRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_{u,i}^2}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\nroot_mean_squared_error\n\n\n\n\nmetrics.willmott(obs, model)\nWillmott’s Index of Agreement\nA scaled representation of the predictive accuracy of the model against observations. A value of 1 indicates a perfect match, and 0 indicates no agreement at all.\n\\[\nwillmott = 1 - \\frac{\\frac{1}{n} \\sum_{i=1}^n(model_i - obs_i)^2}\n                    {\\frac{1}{n} \\sum_{i=1}^n(|model_i - \\overline{obs}| + |obs_i - \\overline{obs}|)^2}\n\\]\nRange: [0, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.array([1.0, 1.1, 1.2, 1.3, 1.4, 1.4, 1.3])\n&gt;&gt;&gt; model = np.array([1.02, 1.16, 1.3, 1.38, 1.49, 1.45, 1.32])\n&gt;&gt;&gt; willmott(obs, model)\n0.9501403174479723\n\n\n\nWillmott, C. J. 1981. “On the validation of models”. Physical Geography, 2, 184–194."
  },
  {
    "objectID": "reference/metrics.html#examples",
    "href": "reference/metrics.html#examples",
    "title": "metrics",
    "section": "",
    "text": "&gt;&gt;&gt; obs = np.array([0.3, 2.1, -1.0])\n&gt;&gt;&gt; mod = np.array([0.0, 2.3, 1.0])\n&gt;&gt;&gt; bias(obs, mod)\n0.6333333333333332\n&gt;&gt;&gt; max_error(obs, mod)\n2.0\n&gt;&gt;&gt; rmse(obs, mod)\n1.173314393786536\n&gt;&gt;&gt; urmse(obs, mod)\n0.9877021593352702\n&gt;&gt;&gt; mae(obs, mod)\n0.8333333333333331\n&gt;&gt;&gt; mape(obs, mod)\n103.17460317460316\n&gt;&gt;&gt; nse(obs, mod)\n0.14786795048143053\n&gt;&gt;&gt; r2(obs, mod)\n0.14786795048143053\n&gt;&gt;&gt; mef(obs, mod)\n0.9231099877688299\n&gt;&gt;&gt; si(obs, mod)\n0.8715019052958266\n&gt;&gt;&gt; spearmanr(obs, mod)\n0.5\n&gt;&gt;&gt; willmott(obs, mod)\n0.7484604452865941\n&gt;&gt;&gt; hit_ratio(obs, mod, a=0.5)\n0.6666666666666666\n&gt;&gt;&gt; ev(obs, mod)\n0.39614855570839064"
  },
  {
    "objectID": "reference/metrics.html#functions",
    "href": "reference/metrics.html#functions",
    "title": "metrics",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_metric\nAdds a metric to the metric list. Useful for custom metrics.\n\n\nbias\nBias (mean error)\n\n\nc_bias\nCircular bias (mean error)\n\n\nc_mae\nalias for circular mean absolute error\n\n\nc_max_error\nCircular max error\n\n\nc_mean_absolute_error\nCircular mean absolute error\n\n\nc_rmse\nalias for circular root mean squared error\n\n\nc_root_mean_squared_error\nCircular root mean squared error\n\n\nc_unbiased_root_mean_squared_error\nCircular unbiased root mean squared error\n\n\nc_urmse\nalias for circular unbiased root mean squared error\n\n\ncc\nalias for corrcoef\n\n\ncorrcoef\nPearson’s Correlation coefficient (CC)\n\n\nev\nalias for explained_variance\n\n\nexplained_variance\nEV: Explained variance\n\n\nhit_ratio\nFraction within obs ± acceptable deviation\n\n\nkge\nalias for kling_gupta_efficiency\n\n\nkling_gupta_efficiency\nKling-Gupta Efficiency (KGE)\n\n\nlin_slope\nSlope of the regression line.\n\n\nmae\nalias for mean_absolute_error\n\n\nmape\nalias for mean_absolute_percentage_error\n\n\nmax_error\nMax (absolute) error\n\n\nmean_absolute_error\nMean Absolute Error (MAE)\n\n\nmean_absolute_percentage_error\nMean Absolute Percentage Error (MAPE)\n\n\nmef\nalias for model_efficiency_factor\n\n\nmetric_has_units\nCheck if a metric has units (dimension).\n\n\nmodel_efficiency_factor\nModel Efficiency Factor (MEF)\n\n\nnash_sutcliffe_efficiency\nNash-Sutcliffe Efficiency (NSE)\n\n\nnse\nalias for nash_sutcliffe_efficiency\n\n\npeak_ratio\nPeak Ratio\n\n\npr\nalias for peak_ratio\n\n\nr2\nCoefficient of determination (R2)\n\n\nrho\nalias for spearmanr\n\n\nrmse\nalias for root_mean_squared_error\n\n\nroot_mean_squared_error\nRoot Mean Squared Error (RMSE)\n\n\nscatter_index\nScatter index (SI)\n\n\nscatter_index2\nAlternative formulation of the scatter index (SI)\n\n\nsi\nalias for scatter_index\n\n\nspearmanr\nSpearman rank correlation coefficient\n\n\nurmse\nUnbiased Root Mean Squared Error (uRMSE)\n\n\nwillmott\nWillmott’s Index of Agreement\n\n\n\n\n\nmetrics.add_metric(metric, has_units=False)\nAdds a metric to the metric list. Useful for custom metrics.\nSome metrics are dimensionless, others have the same dimension as the observations.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetric\nstr or callable\nMetric name or function\nrequired\n\n\nhas_units\nbool\nTrue if metric has a dimension, False otherwise. Default:False\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNone\n\n\n\n\n\n\n\n&gt;&gt;&gt; add_metric(hit_ratio)\n&gt;&gt;&gt; add_metric(rmse,True)\n\n\n\n\nmetrics.bias(obs, model)\nBias (mean error)\n\\[\nbias=\\frac{1}{n}\\sum_{i=1}^n (model_i - obs_i)\n\\]\nRange: \\((-\\infty, \\infty)\\); Best: 0\n\n\n\nmetrics.c_bias(obs, model)\nCircular bias (mean error)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\n\n\n\n\nRange: \\([-180., 180.]\\); Best: 0.\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular bias\n\n\n\n\n\n\n&gt;&gt;&gt; obs = np.array([10., 355., 170.])\n&gt;&gt;&gt; mod = np.array([20., 5., -180.])\n&gt;&gt;&gt; c_bias(obs, mod)\n10.0\n\n\n\n\nmetrics.c_mae(obs, model, weights=None)\nalias for circular mean absolute error\n\n\n\nmetrics.c_max_error(obs, model)\nCircular max error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\n\n\n\n\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular max error\n\n\n\n\n\n\n&gt;&gt;&gt; obs = np.array([10., 350., 10.])\n&gt;&gt;&gt; mod = np.array([20., 10., 350.])\n&gt;&gt;&gt; c_max_error(obs, mod)\n20.0\n\n\n\n\nmetrics.c_mean_absolute_error(obs, model, weights=None)\nCircular mean absolute error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular mean absolute error\n\n\n\n\n\n\n\nmetrics.c_rmse(obs, model, weights=None)\nalias for circular root mean squared error\n\n\n\nmetrics.c_root_mean_squared_error(obs, model, weights=None)\nCircular root mean squared error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular root mean squared error\n\n\n\n\n\n\n\nmetrics.c_unbiased_root_mean_squared_error(obs, model, weights=None)\nCircular unbiased root mean squared error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular unbiased root mean squared error\n\n\n\n\n\n\n\nmetrics.c_urmse(obs, model, weights=None)\nalias for circular unbiased root mean squared error\n\n\n\nmetrics.cc(obs, model, weights=None)\nalias for corrcoef\n\n\n\nmetrics.corrcoef(obs, model, weights=None)\nPearson’s Correlation coefficient (CC)\n\\[\nCC = \\frac{\\sum_{i=1}^n (model_i - \\overline{model})(obs_i - \\overline{obs}) }\n               {\\sqrt{\\sum_{i=1}^n (model_i - \\overline{model})^2}\n                \\sqrt{\\sum_{i=1}^n (obs_i - \\overline{obs})^2} }\n\\]\nRange: [-1, 1]; Best: 1\n\n\nspearmanr np.corrcoef\n\n\n\n\nmetrics.ev(obs, model)\nalias for explained_variance\n\n\n\nmetrics.explained_variance(obs, model)\nEV: Explained variance\nEV is the explained variance and measures the proportion [0 - 1] to which the model accounts for the variation (dispersion) of the observations.\nIn cases with no bias, EV is equal to r2\n\\[\n\\frac{ \\sum_{i=1}^n (obs_i - \\overline{obs})^2 -\n\\sum_{i=1}^n \\left( (obs_i - \\overline{obs}) -\n(model_i - \\overline{model}) \\right)^2}{\\sum_{i=1}^n\n(obs_i - \\overline{obs})^2}\n\\]\nRange: [0, 1]; Best: 1\n\n\nr2\n\n\n\n\nmetrics.hit_ratio(obs, model, a=0.1)\nFraction within obs ± acceptable deviation\n\\[\nHR = \\frac{1}{n}\\sum_{i=1}^n I_{|(model_i - obs_i)|} &lt; a\n\\]\nRange: [0, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.array([1.0, 1.1, 1.2, 1.3, 1.4, 1.4, 1.3])\n&gt;&gt;&gt; model = np.array([1.02, 1.16, 1.3, 1.38, 1.49, 1.45, 1.32])\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.05)\n0.2857142857142857\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.1)\n0.8571428571428571\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.15)\n1.0\n\n\n\n\nmetrics.kge(obs, model)\nalias for kling_gupta_efficiency\n\n\n\nmetrics.kling_gupta_efficiency(obs, model)\nKling-Gupta Efficiency (KGE)\n\\[\nKGE = 1 - \\sqrt{(r-1)^2 + \\left(\\frac{\\sigma_{mod}}{\\sigma_{obs}} - 1\\right)^2 +\n                            \\left(\\frac{\\mu_{mod}}{\\mu_{obs}} - 1\\right)^2 }\n\\]\nwhere \\(r\\) is the pearson correlation coefficient, \\(\\mu_{obs},\\mu_{mod}\\) and \\(\\sigma_{obs},\\sigma_{mod}\\) is the mean and standard deviation of observations and model.\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nGupta, H. V., Kling, H., Yilmaz, K. K. and Martinez, G. F., (2009), Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling, J. Hydrol., 377(1-2), 80-91 https://doi.org/10.1016/j.jhydrol.2009.08.003\nKnoben, W. J. M., Freer, J. E., and Woods, R. A. (2019) Technical note: Inherent benchmark or not? Comparing Nash–Sutcliffe and Kling–Gupta efficiency scores, Hydrol. Earth Syst. Sci., 23, 4323-4331 https://doi.org/10.5194/hess-23-4323-2019\n\n\n\n\nmetrics.lin_slope(obs, model, reg_method='ols')\nSlope of the regression line.\n\\[\nslope = \\frac{\\sum_{i=1}^n (model_i - \\overline {model})(obs_i - \\overline {obs})}\n                {\\sum_{i=1}^n (obs_i - \\overline {obs})^2}\n\\]\nRange: \\((-\\infty, \\infty )\\); Best: 1\n\n\n\nmetrics.mae(obs, model, weights=None)\nalias for mean_absolute_error\n\n\n\nmetrics.mape(obs, model)\nalias for mean_absolute_percentage_error\n\n\n\nmetrics.max_error(obs, model)\nMax (absolute) error\n\\[\nmax_{error} = max(|model_i - obs_i|)\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mean_absolute_error(obs, model, weights=None)\nMean Absolute Error (MAE)\n\\[\nMAE=\\frac{1}{n}\\sum_{i=1}^n|model_i - obs_i|\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mean_absolute_percentage_error(obs, model)\nMean Absolute Percentage Error (MAPE)\n\\[\nMAPE=\\frac{1}{n}\\sum_{i=1}^n\\frac{|model_i - obs_i|}{obs_i}*100\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mef(obs, model)\nalias for model_efficiency_factor\n\n\n\nmetrics.metric_has_units(metric)\nCheck if a metric has units (dimension).\nSome metrics are dimensionless, others have the same dimension as the observations.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetric\nstr or callable\nMetric name or function\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbool\nTrue if metric has a dimension, False otherwise\n\n\n\n\n\n\n&gt;&gt;&gt; metric_has_units(\"rmse\")\nTrue\n&gt;&gt;&gt; metric_has_units(\"kge\")\nFalse\n\n\n\n\nmetrics.model_efficiency_factor(obs, model)\nModel Efficiency Factor (MEF)\nScale independent RMSE, standardized by Stdev of observations\n\\[\nMEF = \\frac{RMSE}{STDEV}=\\frac{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(model_i - obs_i)^2}}\n                                {\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(obs_i - \\overline{obs})^2}}=\\sqrt{1-NSE}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\nnash_sutcliffe_efficiency root_mean_squared_error\n\n\n\n\nmetrics.nash_sutcliffe_efficiency(obs, model)\nNash-Sutcliffe Efficiency (NSE)\n\\[\nNSE = 1 - \\frac {\\sum _{i=1}^{n}\\left(model_{i} - obs_{i}\\right)^{2}}\n                {\\sum_{i=1}^{n}\\left(obs_{i} - {\\overline{obs}}\\right)^{2}}\n\\]\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nr2 = nash_sutcliffe_efficiency(nse)\n\n\n\nNash, J. E.; Sutcliffe, J. V. (1970). “River flow forecasting through conceptual models part I — A discussion of principles”. Journal of Hydrology. 10 (3): 282–290. https://doi.org/10.1016/0022-1694(70)90255-6\n\n\n\n\nmetrics.nse(obs, model)\nalias for nash_sutcliffe_efficiency\n\n\n\nmetrics.peak_ratio(obs, model, inter_event_level=0.7, AAP=2, inter_event_time='36h')\nPeak Ratio\nPR is the mean of the individual ratios of identified peaks in the model / identified peaks in the measurements. PR is calculated only for the joint-events, ie, events that ocurr simulateneously within a window +/- 0.5*inter_event_time.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninter_event_level\nfloat\nInter-event level threshold (default: 0.7).\n0.7\n\n\nAAP\nint\nAverage Annual Peaks (ie, Number of peaks per year, on average). (default: 2)\n2\n\n\ninter_event_time\n\nMaximum time interval between peaks (default: 36 hours).\n'36h'\n\n\n\n\n\n\n\\(\\frac{\\sum_{i=1}^{N_{joint-peaks}} (\\frac{Peak_{model_i}}{Peak_{obs_i}} )}{N_{joint-peaks}}\\)\nRange: \\([0, \\infty)\\); Best: 1.0\n\n\n\n\nmetrics.pr(obs, model, inter_event_level=0.7, AAP=2, inter_event_time='36h')\nalias for peak_ratio\n\n\n\nmetrics.r2(obs, model)\nCoefficient of determination (R2)\nPronounced ‘R-squared’; the proportion of the variation in the dependent variable that is predictable from the independent variable(s), i.e. the proportion of explained variance.\n\\[\nR^2 = 1 - \\frac{\\sum_{i=1}^n (model_i - obs_i)^2}\n                {\\sum_{i=1}^n (obs_i - \\overline {obs})^2}\n\\]\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nr2 = nash_sutcliffe_efficiency(nse)\n\n\n\n&gt;&gt;&gt; obs = np.array([1.0,1.1,1.2,1.3,1.4])\n&gt;&gt;&gt; model = np.array([1.09, 1.16, 1.3 , 1.38, 1.49])\n&gt;&gt;&gt; r2(obs,model)\n0.6379999999999998\n\n\n\n\nmetrics.rho(obs, model)\nalias for spearmanr\n\n\n\nmetrics.rmse(obs, model, weights=None, unbiased=False)\nalias for root_mean_squared_error\n\n\n\nmetrics.root_mean_squared_error(obs, model, weights=None, unbiased=False)\nRoot Mean Squared Error (RMSE)\n\\[\nres_i = model_i - obs_i\n\\]\n\\[\nRMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_i^2}\n\\]\nUnbiased version:\n\\[\nres_{u,i} = res_i - \\overline {res}\n\\]\n\\[\nuRMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_{u,i}^2}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.scatter_index(obs, model)\nScatter index (SI)\nWhich is the same as the unbiased-RMSE normalized by the absolute mean of the observations.\n\\[\n\\frac{ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left( (model_i - \\overline {model}) - (obs_i - \\overline {obs}) \\right)^2} }\n{\\frac{1}{n} \\sum_{i=1}^n | obs_i | }\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.scatter_index2(obs, model)\nAlternative formulation of the scatter index (SI)\n\\[\n\\sqrt {\\frac{\\sum_{i=1}^n \\left( (model_i - \\overline {model}) - (obs_i - \\overline {obs}) \\right)^2}\n{\\sum_{i=1}^n obs_i^2}}\n\\]\nRange: [0, 100]; Best: 0\n\n\n\nmetrics.si(obs, model)\nalias for scatter_index\n\n\n\nmetrics.spearmanr(obs, model)\nSpearman rank correlation coefficient\nThe rank correlation coefficient is similar to the Pearson correlation coefficient but applied to ranked quantities and is useful to quantify a monotonous relationship\n\\[\n\\rho = \\frac{\\sum_{i=1}^n (rmodel_i - \\overline{rmodel})(robs_i - \\overline{robs}) }\n                {\\sqrt{\\sum_{i=1}^n (rmodel_i - \\overline{rmodel})^2}\n                \\sqrt{\\sum_{i=1}^n (robs_i - \\overline{robs})^2} }\n\\]\nRange: [-1, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.linspace(-20, 20, 100)\n&gt;&gt;&gt; mod = np.tanh(obs)\n&gt;&gt;&gt; rho(obs, mod)\n0.9999759973116955\n&gt;&gt;&gt; spearmanr(obs, mod)\n0.9999759973116955\n\n\n\ncorrcoef\n\n\n\n\nmetrics.urmse(obs, model, weights=None)\nUnbiased Root Mean Squared Error (uRMSE)\n\\[\nres_i = model_i - obs_i\n\\]\n\\[\nres_{u,i} = res_i - \\overline {res}\n\\]\n\\[\nuRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_{u,i}^2}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\nroot_mean_squared_error\n\n\n\n\nmetrics.willmott(obs, model)\nWillmott’s Index of Agreement\nA scaled representation of the predictive accuracy of the model against observations. A value of 1 indicates a perfect match, and 0 indicates no agreement at all.\n\\[\nwillmott = 1 - \\frac{\\frac{1}{n} \\sum_{i=1}^n(model_i - obs_i)^2}\n                    {\\frac{1}{n} \\sum_{i=1}^n(|model_i - \\overline{obs}| + |obs_i - \\overline{obs}|)^2}\n\\]\nRange: [0, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.array([1.0, 1.1, 1.2, 1.3, 1.4, 1.4, 1.3])\n&gt;&gt;&gt; model = np.array([1.02, 1.16, 1.3, 1.38, 1.49, 1.45, 1.32])\n&gt;&gt;&gt; willmott(obs, model)\n0.9501403174479723\n\n\n\nWillmott, C. J. 1981. “On the validation of models”. Physical Geography, 2, 184–194."
  },
  {
    "objectID": "reference/model_result.html",
    "href": "reference/model_result.html",
    "title": "model_result",
    "section": "",
    "text": "model_result(data, *, aux_items=None, gtype=None, **kwargs)\nA factory function for creating an appropriate object based on the data input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the ModelResult object.\nrequired\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’, ‘unstructured’, ‘grid’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments to be passed to the ModelResult constructor.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; ms.model_result(\"Oresund2D.dfsu\", item=0)\n&lt;DfsuModelResult&gt; 'Oresund2D'\n&gt;&gt;&gt; ms.model_result(\"ERA5_DutchCoast.nc\", item=\"swh\", name=\"ERA5\")\n&lt;GridModelResult&gt; 'ERA5'"
  },
  {
    "objectID": "reference/model_result.html#parameters",
    "href": "reference/model_result.html#parameters",
    "title": "model_result",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the ModelResult object.\nrequired\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’, ‘unstructured’, ‘grid’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments to be passed to the ModelResult constructor.\n{}"
  },
  {
    "objectID": "reference/model_result.html#examples",
    "href": "reference/model_result.html#examples",
    "title": "model_result",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; ms.model_result(\"Oresund2D.dfsu\", item=0)\n&lt;DfsuModelResult&gt; 'Oresund2D'\n&gt;&gt;&gt; ms.model_result(\"ERA5_DutchCoast.nc\", item=\"swh\", name=\"ERA5\")\n&lt;GridModelResult&gt; 'ERA5'"
  },
  {
    "objectID": "reference/TrackModelResult.html",
    "href": "reference/TrackModelResult.html",
    "title": "TrackModelResult",
    "section": "",
    "text": "TrackModelResult(self, data, *, name=None, item=None, quantity=None, x_item=0, y_item=1, keep_duplicates='first', aux_items=None)\nConstruct a TrackModelResult from a dfs0 file, mikeio.Dataset, pandas.DataFrame or a xarray.Datasets\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.TrackType\nThe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nx_item\nstr | int | None\nItem of the first coordinate of positions, by default None\n0\n\n\ny_item\nstr | int | None\nItem of the second coordinate of positions, by default None\n1\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\nkeep_duplicates\n(str, bool)\nStrategy for handling duplicate timestamps (wraps xarray.Dataset.drop_duplicates) “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "reference/TrackModelResult.html#parameters",
    "href": "reference/TrackModelResult.html#parameters",
    "title": "TrackModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.TrackType\nThe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nx_item\nstr | int | None\nItem of the first coordinate of positions, by default None\n0\n\n\ny_item\nstr | int | None\nItem of the second coordinate of positions, by default None\n1\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\nkeep_duplicates\n(str, bool)\nStrategy for handling duplicate timestamps (wraps xarray.Dataset.drop_duplicates) “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "reference/observation.html",
    "href": "reference/observation.html",
    "title": "observation",
    "section": "",
    "text": "observation(data, *, gtype=None, **kwargs)\nA factory function for creating an appropriate observation object based on the data and args.\nIf ‘x’ or ‘y’ is given, a PointObservation is created. If ‘x_item’ or ‘y_item’ is given, a TrackObservation is created.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the Observation object.\nrequired\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\n\nAdditional keyword arguments to be passed to the Observation constructor.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o_pt = ms.observation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o_tr = ms.observation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)"
  },
  {
    "objectID": "reference/observation.html#parameters",
    "href": "reference/observation.html#parameters",
    "title": "observation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the Observation object.\nrequired\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\n\nAdditional keyword arguments to be passed to the Observation constructor.\n{}"
  },
  {
    "objectID": "reference/observation.html#examples",
    "href": "reference/observation.html#examples",
    "title": "observation",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o_pt = ms.observation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o_tr = ms.observation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)"
  },
  {
    "objectID": "reference/GridModelResult.html",
    "href": "reference/GridModelResult.html",
    "title": "GridModelResult",
    "section": "",
    "text": "GridModelResult(self, data, *, name=None, item=None, quantity=None, aux_items=None)\nConstruct a GridModelResult from a file or xarray.Dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.GridType\nthe input data or file path\nrequired\n\n\nname\nstr\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr or int\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "reference/GridModelResult.html#parameters",
    "href": "reference/GridModelResult.html#parameters",
    "title": "GridModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.GridType\nthe input data or file path\nrequired\n\n\nname\nstr\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr or int\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/PointModelResult.html",
    "href": "api/PointModelResult.html",
    "title": "PointModelResult",
    "section": "",
    "text": "PointModelResult(self, data, *, name=None, x=None, y=None, z=None, item=None, quantity=None, aux_items=None)\nConstruct a PointModelResult from a 0d data source: dfs0 file, mikeio.Dataset/DataArray, pandas.DataFrame/Series or xarray.Dataset/DataArray\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nx\nfloat\nfirst coordinate of point position, inferred from data if not given, else None\nNone\n\n\ny\nfloat\nsecond coordinate of point position, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nthird coordinate of point position, inferred from data if not given, else None\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/PointModelResult.html#parameters",
    "href": "api/PointModelResult.html#parameters",
    "title": "PointModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nx\nfloat\nfirst coordinate of point position, inferred from data if not given, else None\nNone\n\n\ny\nfloat\nsecond coordinate of point position, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nthird coordinate of point position, inferred from data if not given, else None\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/index.html#observation",
    "href": "api/index.html#observation",
    "title": "Function reference",
    "section": "",
    "text": "Observation classes and functions\n\n\n\nobservation\nA factory function for creating an appropriate observation object\n\n\nPointObservation\nClass for observations of fixed locations\n\n\nTrackObservation\nClass for observation with locations moving in space, e.g. satellite altimetry"
  },
  {
    "objectID": "api/index.html#model-result",
    "href": "api/index.html#model-result",
    "title": "Function reference",
    "section": "",
    "text": "Model result classes and functions\n\n\n\nmodel_result\nA factory function for creating an appropriate object based on the data input.\n\n\nPointModelResult\nConstruct a PointModelResult from a 0d data source:\n\n\nTrackModelResult\nConstruct a TrackModelResult from a dfs0 file,\n\n\nDfsuModelResult\nConstruct a DfsuModelResult from a dfsu file or mikeio.Dataset/DataArray.\n\n\nGridModelResult\nConstruct a GridModelResult from a file or xarray.Dataset.\n\n\nDummyModelResult"
  },
  {
    "objectID": "api/index.html#matching",
    "href": "api/index.html#matching",
    "title": "Function reference",
    "section": "",
    "text": "Matching classes and functions\n\n\n\nmatching"
  },
  {
    "objectID": "api/index.html#comparer",
    "href": "api/index.html#comparer",
    "title": "Function reference",
    "section": "",
    "text": "Comparer classes and functions\n\n\n\nComparer\nComparer class for comparing model and observation data.\n\n\nComparerCollection\nCollection of comparers, constructed by calling the modelskill.match"
  },
  {
    "objectID": "api/index.html#metrics",
    "href": "api/index.html#metrics",
    "title": "Function reference",
    "section": "",
    "text": "Metrics classes and functions\n\n\n\nmetrics\nThe metrics module contains different skill metrics for evaluating the"
  },
  {
    "objectID": "api/DfsuModelResult.html",
    "href": "api/DfsuModelResult.html",
    "title": "DfsuModelResult",
    "section": "",
    "text": "DfsuModelResult(self, data, *, name=None, item=None, quantity=None, aux_items=None)\nConstruct a DfsuModelResult from a dfsu file or mikeio.Dataset/DataArray.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.UnstructuredType\nthe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/DfsuModelResult.html#parameters",
    "href": "api/DfsuModelResult.html#parameters",
    "title": "DfsuModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.UnstructuredType\nthe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/TrackObservation.html",
    "href": "api/TrackObservation.html",
    "title": "TrackObservation",
    "section": "",
    "text": "TrackObservation(self, data, *, item=None, name=None, weight=1.0, x_item=0, y_item=1, keep_duplicates='first', offset_duplicates=0.001, quantity=None, aux_items=None, attrs=None)\nClass for observation with locations moving in space, e.g. satellite altimetry\nThe data needs in addition to the datetime of each single observation point also, x and y coordinates.\nCreate TrackObservation from dfs0 or DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, pandas.pandas.DataFrame, xarray.xarray.Dataset)\npath to dfs0 file or object with track data\nrequired\n\n\nitem\n(str, int)\nitem name or index of values, by default None if data contains more than one item, item must be given\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nx_item\n(str, int)\nitem name or index of x-coordinate, by default 0\n0\n\n\ny_item\n(str, int)\nitem name or index of y-coordinate, by default 1\n1\n\n\nkeep_duplicates\n(str, bool)\nstrategy for handling duplicate timestamps (xarray.Dataset.drop_duplicates): “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=2, name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=\"wind_speed\", name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track_wl.dfs0\", item=\"wl\", x_item=\"lon\", y_item=\"lat\")\n&gt;&gt;&gt; df = pd.DataFrame(\n...         {\n...             \"t\": pd.date_range(\"2010-01-01\", freq=\"10s\", periods=n),\n...             \"x\": np.linspace(0, 10, n),\n...             \"y\": np.linspace(45000, 45100, n),\n...             \"swh\": [0.1, 0.3, 0.4, 0.5, 0.3],\n...         }\n... )\n&gt;&gt;&gt; df = df.set_index(\"t\")\n&gt;&gt;&gt; df\n                    x        y  swh\nt\n2010-01-01 00:00:00   0.0  45000.0  0.1\n2010-01-01 00:00:10   2.5  45025.0  0.3\n2010-01-01 00:00:20   5.0  45050.0  0.4\n2010-01-01 00:00:30   7.5  45075.0  0.5\n2010-01-01 00:00:40  10.0  45100.0  0.3\n&gt;&gt;&gt; t1 = TrackObservation(df, name=\"fake\")\n&gt;&gt;&gt; t1.n_points\n5\n&gt;&gt;&gt; t1.values\narray([0.1, 0.3, 0.4, 0.5, 0.3])\n&gt;&gt;&gt; t1.time\nDatetimeIndex(['2010-01-01 00:00:00', '2010-01-01 00:00:10',\n           '2010-01-01 00:00:20', '2010-01-01 00:00:30',\n           '2010-01-01 00:00:40'],\n          dtype='datetime64[ns]', name='t', freq=None)\n&gt;&gt;&gt; t1.x\narray([ 0. ,  2.5,  5. ,  7.5, 10. ])\n&gt;&gt;&gt; t1.y\narray([45000., 45025., 45050., 45075., 45100.])"
  },
  {
    "objectID": "api/TrackObservation.html#parameters",
    "href": "api/TrackObservation.html#parameters",
    "title": "TrackObservation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, pandas.pandas.DataFrame, xarray.xarray.Dataset)\npath to dfs0 file or object with track data\nrequired\n\n\nitem\n(str, int)\nitem name or index of values, by default None if data contains more than one item, item must be given\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nx_item\n(str, int)\nitem name or index of x-coordinate, by default 0\n0\n\n\ny_item\n(str, int)\nitem name or index of y-coordinate, by default 1\n1\n\n\nkeep_duplicates\n(str, bool)\nstrategy for handling duplicate timestamps (xarray.Dataset.drop_duplicates): “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0"
  },
  {
    "objectID": "api/TrackObservation.html#examples",
    "href": "api/TrackObservation.html#examples",
    "title": "TrackObservation",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=2, name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track.dfs0\", item=\"wind_speed\", name=\"c2\")\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)\n&gt;&gt;&gt; o1 = ms.TrackObservation(\"track_wl.dfs0\", item=\"wl\", x_item=\"lon\", y_item=\"lat\")\n&gt;&gt;&gt; df = pd.DataFrame(\n...         {\n...             \"t\": pd.date_range(\"2010-01-01\", freq=\"10s\", periods=n),\n...             \"x\": np.linspace(0, 10, n),\n...             \"y\": np.linspace(45000, 45100, n),\n...             \"swh\": [0.1, 0.3, 0.4, 0.5, 0.3],\n...         }\n... )\n&gt;&gt;&gt; df = df.set_index(\"t\")\n&gt;&gt;&gt; df\n                    x        y  swh\nt\n2010-01-01 00:00:00   0.0  45000.0  0.1\n2010-01-01 00:00:10   2.5  45025.0  0.3\n2010-01-01 00:00:20   5.0  45050.0  0.4\n2010-01-01 00:00:30   7.5  45075.0  0.5\n2010-01-01 00:00:40  10.0  45100.0  0.3\n&gt;&gt;&gt; t1 = TrackObservation(df, name=\"fake\")\n&gt;&gt;&gt; t1.n_points\n5\n&gt;&gt;&gt; t1.values\narray([0.1, 0.3, 0.4, 0.5, 0.3])\n&gt;&gt;&gt; t1.time\nDatetimeIndex(['2010-01-01 00:00:00', '2010-01-01 00:00:10',\n           '2010-01-01 00:00:20', '2010-01-01 00:00:30',\n           '2010-01-01 00:00:40'],\n          dtype='datetime64[ns]', name='t', freq=None)\n&gt;&gt;&gt; t1.x\narray([ 0. ,  2.5,  5. ,  7.5, 10. ])\n&gt;&gt;&gt; t1.y\narray([45000., 45025., 45050., 45075., 45100.])"
  },
  {
    "objectID": "api/model_result.html",
    "href": "api/model_result.html",
    "title": "model_result",
    "section": "",
    "text": "model_result(data, *, aux_items=None, gtype=None, **kwargs)\nA factory function for creating an appropriate object based on the data input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the ModelResult object.\nrequired\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’, ‘unstructured’, ‘grid’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments to be passed to the ModelResult constructor.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; ms.model_result(\"Oresund2D.dfsu\", item=0)\n&lt;DfsuModelResult&gt; 'Oresund2D'\n&gt;&gt;&gt; ms.model_result(\"ERA5_DutchCoast.nc\", item=\"swh\", name=\"ERA5\")\n&lt;GridModelResult&gt; 'ERA5'"
  },
  {
    "objectID": "api/model_result.html#parameters",
    "href": "api/model_result.html#parameters",
    "title": "model_result",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the ModelResult object.\nrequired\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’, ‘unstructured’, ‘grid’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments to be passed to the ModelResult constructor.\n{}"
  },
  {
    "objectID": "api/model_result.html#examples",
    "href": "api/model_result.html#examples",
    "title": "model_result",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; ms.model_result(\"Oresund2D.dfsu\", item=0)\n&lt;DfsuModelResult&gt; 'Oresund2D'\n&gt;&gt;&gt; ms.model_result(\"ERA5_DutchCoast.nc\", item=\"swh\", name=\"ERA5\")\n&lt;GridModelResult&gt; 'ERA5'"
  },
  {
    "objectID": "api/matching.html#functions",
    "href": "api/matching.html#functions",
    "title": "matching",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfrom_matched\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\nmatch\nMatch observation and model result data in space and time\n\n\nmatch_space_time\nMatch observation with one or more model results in time domain\n\n\n\n\n\nmatching.from_matched(data, *, obs_item=0, mod_items=None, aux_items=None, quantity=None, name=None, weight=1.0, x=None, y=None, z=None, x_item=None, y_item=None)\nCreate a Comparer from observation and model results that are already matched (aligned)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n[pandas.pandas.DataFrame, str, pathlib.Path, mikeio.mikeio.Dfs0, mikeio.mikeio.Dataset]\nDataFrame (or object that can be converted to a DataFrame e.g. dfs0) with columns obs_item, mod_items, aux_items\nrequired\n\n\nobs_item\n[str, int]\nName or index of observation item, by default first item\n0\n\n\nmod_items\ntyping.Iterable[str, int]\nNames or indicies of model items, if None all remaining columns are model items, by default None\nNone\n\n\naux_items\ntyping.Iterable[str, int]\nNames or indicies of auxiliary items, by default None\nNone\n\n\nquantity\nmodelskill.Quantity\nQuantity of the observation and model results, by default Quantity(name=“Undefined”, unit=“Undefined”)\nNone\n\n\nname\nstr\nName of the comparer, by default None (will be set to obs_item)\nNone\n\n\nx\nfloat\nx-coordinate of observation, by default None\nNone\n\n\ny\nfloat\ny-coordinate of observation, by default None\nNone\n\n\nz\nfloat\nz-coordinate of observation, by default None\nNone\n\n\nx_item\nstr | int | None\nName of x item, only relevant for track data\nNone\n\n\ny_item\nstr | int | None\nName of y item, only relevant for track data\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a') # remaining columns are model results\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n Model: local, rmse=0.100\n&gt;&gt;&gt; df = pd.DataFrame({'stn_a': [1,2,3], 'local': [1.1,2.1,3.1], 'global': [1.2,2.2,3.2], 'nonsense':[1,2,3]}, index=pd.date_range('2010-01-01', periods=3))\n&gt;&gt;&gt; cmp = ms.from_matched(df, obs_item='stn_a', mod_items=['local', 'global'])\n&gt;&gt;&gt; cmp\n&lt;Comparer&gt;\nQuantity: Undefined [Undefined]\nObservation: stn_a, n_points=3\n    Model: local, rmse=0.100\n    Model: global, rmse=0.200\n\n\n\n\nmatching.match(obs, mod, *, obs_item=None, mod_item=None, gtype=None, max_model_gap=None, spatial_method=None)\nMatch observation and model result data in space and time\nNOTE: In case of multiple model results with different time coverage, only the overlapping time period will be used! (intersection)\nNOTE: In case of multiple observations, multiple models can only be matched if they are all of SpatialField type, e.g. DfsuModelResult or GridModelResult.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.obs.Observation, typing.Sequence[modelskill.obs.Observation])\nObservation(s) to be compared\nrequired\n\n\nmod\n(str, pathlib.Path, pandas.pandas.DataFrame, modelskill.model.factory.ModelResult, typing.Sequence[modelskill.model.factory.ModelResult])\nModel result(s) to be compared\nrequired\n\n\nobs_item\nint or str\nobservation item if obs is a file/dataframe, by default None\nNone\n\n\nmod_item\n(int, str)\nmodel item if mod is a file/dataframe, by default None\nNone\n\n\ngtype\n(str, optional)\nGeometry type of the model result (if mod is a file/dataframe). If not specified, it will be guessed.\nNone\n\n\nmax_model_gap\n(float, optional)\nMaximum time gap (s) in the model result (e.g. for event-based model results), by default None\nNone\n\n\nspatial_method\nstr\nFor Dfsu- and GridModelResult, spatial interpolation/selection method. - For DfsuModelResult, one of: ‘contained’ (=isel), ‘nearest’, ‘inverse_distance’ (with 5 nearest points), by default “inverse_distance”. - For GridModelResult, passed to xarray.interp() as method argument, by default ‘linear’.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nmodelskill.comparison.Comparer\nIn case of a single observation\n\n\nmodelskill.comparison.ComparerCollection\nIn case of multiple observations\n\n\n\n\n\n\n[from_matched][modelskill.from_matched] Create a Comparer from observation and model results that are already matched\n\n\n\n\nmatching.match_space_time(observation, raw_mod_data, max_model_gap=None, spatial_tolerance=0.001)\nMatch observation with one or more model results in time domain and return as xr.Dataset in the format used by modelskill.Comparer\nWill interpolate model results to observation time.\nNote: assumes that observation and model data are already matched in space. But positions of track observations will be checked.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobservation\nmodelskill.obs.Observation\nObservation to be matched\nrequired\n\n\nraw_mod_data\ntyping.Dict[str, modelskill.model.point.PointModelResult | modelskill.model.track.TrackModelResult]\nDictionary of model results ready for interpolation\nrequired\n\n\nmax_model_gap\ntyping.Optional[modelskill.matching.TimeDeltaTypes]\nIn case of non-equidistant model results (e.g. event data), max_model_gap can be given e.g. as seconds, by default None\nNone\n\n\nspatial_tolerance\nfloat\nTolerance for spatial matching, by default 1e-3\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.xarray.Dataset\nMatched data in the format used by modelskill.Comparer"
  },
  {
    "objectID": "api/ComparerCollection.html",
    "href": "api/ComparerCollection.html",
    "title": "ComparerCollection",
    "section": "",
    "text": "ComparerCollection(self, comparers)\nCollection of comparers, constructed by calling the modelskill.match method or by initializing with a list of comparers.\nNOTE: In case of multiple model results with different time coverage, only the overlapping time period will be used! (intersection)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncomparers\nlist of Comparer\nlist of comparers\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; mr = ms.DfsuModelResult(\"Oresund2D.dfsu\", item=0)\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"drogden.dfs0\", item=0, x=355568.0, y=6156863.0)\n&gt;&gt;&gt; cmp1 = ms.match(o1, mr)  # Comparer\n&gt;&gt;&gt; cmp2 = ms.match(o2, mr)  # Comparer\n&gt;&gt;&gt; ccA = ms.ComparerCollection([cmp1, cmp2])\n&gt;&gt;&gt; ccB = ms.match(obs=[o1, o2], mod=mr)\n&gt;&gt;&gt; sk = ccB.skill()\n&gt;&gt;&gt; ccB[\"Klagshamn\"].plot.timeseries()"
  },
  {
    "objectID": "api/ComparerCollection.html#examples",
    "href": "api/ComparerCollection.html#examples",
    "title": "ComparerCollection",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; mr = ms.DfsuModelResult(\"Oresund2D.dfsu\", item=0)\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"drogden.dfs0\", item=0, x=355568.0, y=6156863.0)\n&gt;&gt;&gt; cmp1 = ms.match(o1, mr)  # Comparer\n&gt;&gt;&gt; cmp2 = ms.match(o2, mr)  # Comparer\n&gt;&gt;&gt; ccA = ms.ComparerCollection([cmp1, cmp2])\n&gt;&gt;&gt; ccB = ms.match(obs=[o1, o2], mod=mr)\n&gt;&gt;&gt; sk = ccB.skill()\n&gt;&gt;&gt; ccB[\"Klagshamn\"].plot.timeseries()"
  },
  {
    "objectID": "api/Comparer.html",
    "href": "api/Comparer.html",
    "title": "Comparer",
    "section": "",
    "text": "Comparer(self, matched_data, raw_mod_data=None)\nComparer class for comparing model and observation data.\nTypically, the Comparer is part of a ComparerCollection, created with the match function.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatched_data\nxarray.xarray.Dataset\nMatched data\nrequired\n\n\nraw_mod_data\ndict of modelskill.TimeSeries\nRaw model data. If None, observation and modeldata must be provided.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; cmp1 = ms.match(observation, modeldata)\n&gt;&gt;&gt; cmp2 = ms.from_matched(matched_data)\n\n\n\nmodelskill.match, modelskill.from_matched"
  },
  {
    "objectID": "api/Comparer.html#parameters",
    "href": "api/Comparer.html#parameters",
    "title": "Comparer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatched_data\nxarray.xarray.Dataset\nMatched data\nrequired\n\n\nraw_mod_data\ndict of modelskill.TimeSeries\nRaw model data. If None, observation and modeldata must be provided.\nNone"
  },
  {
    "objectID": "api/Comparer.html#examples",
    "href": "api/Comparer.html#examples",
    "title": "Comparer",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; cmp1 = ms.match(observation, modeldata)\n&gt;&gt;&gt; cmp2 = ms.from_matched(matched_data)"
  },
  {
    "objectID": "api/Comparer.html#see-also",
    "href": "api/Comparer.html#see-also",
    "title": "Comparer",
    "section": "",
    "text": "modelskill.match, modelskill.from_matched"
  },
  {
    "objectID": "api/metrics.html#examples",
    "href": "api/metrics.html#examples",
    "title": "metrics",
    "section": "",
    "text": "&gt;&gt;&gt; obs = np.array([0.3, 2.1, -1.0])\n&gt;&gt;&gt; mod = np.array([0.0, 2.3, 1.0])\n&gt;&gt;&gt; bias(obs, mod)\n0.6333333333333332\n&gt;&gt;&gt; max_error(obs, mod)\n2.0\n&gt;&gt;&gt; rmse(obs, mod)\n1.173314393786536\n&gt;&gt;&gt; urmse(obs, mod)\n0.9877021593352702\n&gt;&gt;&gt; mae(obs, mod)\n0.8333333333333331\n&gt;&gt;&gt; mape(obs, mod)\n103.17460317460316\n&gt;&gt;&gt; nse(obs, mod)\n0.14786795048143053\n&gt;&gt;&gt; r2(obs, mod)\n0.14786795048143053\n&gt;&gt;&gt; mef(obs, mod)\n0.9231099877688299\n&gt;&gt;&gt; si(obs, mod)\n0.8715019052958266\n&gt;&gt;&gt; spearmanr(obs, mod)\n0.5\n&gt;&gt;&gt; willmott(obs, mod)\n0.7484604452865941\n&gt;&gt;&gt; hit_ratio(obs, mod, a=0.5)\n0.6666666666666666\n&gt;&gt;&gt; ev(obs, mod)\n0.39614855570839064"
  },
  {
    "objectID": "api/metrics.html#functions",
    "href": "api/metrics.html#functions",
    "title": "metrics",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_metric\nAdds a metric to the metric list. Useful for custom metrics.\n\n\nbias\nBias (mean error)\n\n\nc_bias\nCircular bias (mean error)\n\n\nc_mae\nalias for circular mean absolute error\n\n\nc_max_error\nCircular max error\n\n\nc_mean_absolute_error\nCircular mean absolute error\n\n\nc_rmse\nalias for circular root mean squared error\n\n\nc_root_mean_squared_error\nCircular root mean squared error\n\n\nc_unbiased_root_mean_squared_error\nCircular unbiased root mean squared error\n\n\nc_urmse\nalias for circular unbiased root mean squared error\n\n\ncc\nalias for corrcoef\n\n\ncorrcoef\nPearson’s Correlation coefficient (CC)\n\n\nev\nalias for explained_variance\n\n\nexplained_variance\nEV: Explained variance\n\n\nhit_ratio\nFraction within obs ± acceptable deviation\n\n\nkge\nalias for kling_gupta_efficiency\n\n\nkling_gupta_efficiency\nKling-Gupta Efficiency (KGE)\n\n\nlin_slope\nSlope of the regression line.\n\n\nmae\nalias for mean_absolute_error\n\n\nmape\nalias for mean_absolute_percentage_error\n\n\nmax_error\nMax (absolute) error\n\n\nmean_absolute_error\nMean Absolute Error (MAE)\n\n\nmean_absolute_percentage_error\nMean Absolute Percentage Error (MAPE)\n\n\nmef\nalias for model_efficiency_factor\n\n\nmetric_has_units\nCheck if a metric has units (dimension).\n\n\nmodel_efficiency_factor\nModel Efficiency Factor (MEF)\n\n\nnash_sutcliffe_efficiency\nNash-Sutcliffe Efficiency (NSE)\n\n\nnse\nalias for nash_sutcliffe_efficiency\n\n\npeak_ratio\nPeak Ratio\n\n\npr\nalias for peak_ratio\n\n\nr2\nCoefficient of determination (R2)\n\n\nrho\nalias for spearmanr\n\n\nrmse\nalias for root_mean_squared_error\n\n\nroot_mean_squared_error\nRoot Mean Squared Error (RMSE)\n\n\nscatter_index\nScatter index (SI)\n\n\nscatter_index2\nAlternative formulation of the scatter index (SI)\n\n\nsi\nalias for scatter_index\n\n\nspearmanr\nSpearman rank correlation coefficient\n\n\nurmse\nUnbiased Root Mean Squared Error (uRMSE)\n\n\nwillmott\nWillmott’s Index of Agreement\n\n\n\n\n\nmetrics.add_metric(metric, has_units=False)\nAdds a metric to the metric list. Useful for custom metrics.\nSome metrics are dimensionless, others have the same dimension as the observations.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetric\nstr or callable\nMetric name or function\nrequired\n\n\nhas_units\nbool\nTrue if metric has a dimension, False otherwise. Default:False\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNone\n\n\n\n\n\n\n\n&gt;&gt;&gt; add_metric(hit_ratio)\n&gt;&gt;&gt; add_metric(rmse,True)\n\n\n\n\nmetrics.bias(obs, model)\nBias (mean error)\n\\[\nbias=\\frac{1}{n}\\sum_{i=1}^n (model_i - obs_i)\n\\]\nRange: \\((-\\infty, \\infty)\\); Best: 0\n\n\n\nmetrics.c_bias(obs, model)\nCircular bias (mean error)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\n\n\n\n\nRange: \\([-180., 180.]\\); Best: 0.\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular bias\n\n\n\n\n\n\n&gt;&gt;&gt; obs = np.array([10., 355., 170.])\n&gt;&gt;&gt; mod = np.array([20., 5., -180.])\n&gt;&gt;&gt; c_bias(obs, mod)\n10.0\n\n\n\n\nmetrics.c_mae(obs, model, weights=None)\nalias for circular mean absolute error\n\n\n\nmetrics.c_max_error(obs, model)\nCircular max error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\n\n\n\n\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular max error\n\n\n\n\n\n\n&gt;&gt;&gt; obs = np.array([10., 350., 10.])\n&gt;&gt;&gt; mod = np.array([20., 10., 350.])\n&gt;&gt;&gt; c_max_error(obs, mod)\n20.0\n\n\n\n\nmetrics.c_mean_absolute_error(obs, model, weights=None)\nCircular mean absolute error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular mean absolute error\n\n\n\n\n\n\n\nmetrics.c_rmse(obs, model, weights=None)\nalias for circular root mean squared error\n\n\n\nmetrics.c_root_mean_squared_error(obs, model, weights=None)\nCircular root mean squared error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular root mean squared error\n\n\n\n\n\n\n\nmetrics.c_unbiased_root_mean_squared_error(obs, model, weights=None)\nCircular unbiased root mean squared error\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobs\nnumpy.numpy.ndarray\nObservation in degrees (0, 360)\nrequired\n\n\nmodel\nnumpy.numpy.ndarray\nModel in degrees (0, 360)\nrequired\n\n\nweights\nnumpy.numpy.ndarray\nWeights, by default None\nNone\n\n\n\n\n\n\nRange: [0, 180]; Best: 0\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nCircular unbiased root mean squared error\n\n\n\n\n\n\n\nmetrics.c_urmse(obs, model, weights=None)\nalias for circular unbiased root mean squared error\n\n\n\nmetrics.cc(obs, model, weights=None)\nalias for corrcoef\n\n\n\nmetrics.corrcoef(obs, model, weights=None)\nPearson’s Correlation coefficient (CC)\n\\[\nCC = \\frac{\\sum_{i=1}^n (model_i - \\overline{model})(obs_i - \\overline{obs}) }\n               {\\sqrt{\\sum_{i=1}^n (model_i - \\overline{model})^2}\n                \\sqrt{\\sum_{i=1}^n (obs_i - \\overline{obs})^2} }\n\\]\nRange: [-1, 1]; Best: 1\n\n\nspearmanr np.corrcoef\n\n\n\n\nmetrics.ev(obs, model)\nalias for explained_variance\n\n\n\nmetrics.explained_variance(obs, model)\nEV: Explained variance\nEV is the explained variance and measures the proportion [0 - 1] to which the model accounts for the variation (dispersion) of the observations.\nIn cases with no bias, EV is equal to r2\n\\[\n\\frac{ \\sum_{i=1}^n (obs_i - \\overline{obs})^2 -\n\\sum_{i=1}^n \\left( (obs_i - \\overline{obs}) -\n(model_i - \\overline{model}) \\right)^2}{\\sum_{i=1}^n\n(obs_i - \\overline{obs})^2}\n\\]\nRange: [0, 1]; Best: 1\n\n\nr2\n\n\n\n\nmetrics.hit_ratio(obs, model, a=0.1)\nFraction within obs ± acceptable deviation\n\\[\nHR = \\frac{1}{n}\\sum_{i=1}^n I_{|(model_i - obs_i)|} &lt; a\n\\]\nRange: [0, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.array([1.0, 1.1, 1.2, 1.3, 1.4, 1.4, 1.3])\n&gt;&gt;&gt; model = np.array([1.02, 1.16, 1.3, 1.38, 1.49, 1.45, 1.32])\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.05)\n0.2857142857142857\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.1)\n0.8571428571428571\n&gt;&gt;&gt; hit_ratio(obs, model, a=0.15)\n1.0\n\n\n\n\nmetrics.kge(obs, model)\nalias for kling_gupta_efficiency\n\n\n\nmetrics.kling_gupta_efficiency(obs, model)\nKling-Gupta Efficiency (KGE)\n\\[\nKGE = 1 - \\sqrt{(r-1)^2 + \\left(\\frac{\\sigma_{mod}}{\\sigma_{obs}} - 1\\right)^2 +\n                            \\left(\\frac{\\mu_{mod}}{\\mu_{obs}} - 1\\right)^2 }\n\\]\nwhere \\(r\\) is the pearson correlation coefficient, \\(\\mu_{obs},\\mu_{mod}\\) and \\(\\sigma_{obs},\\sigma_{mod}\\) is the mean and standard deviation of observations and model.\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nGupta, H. V., Kling, H., Yilmaz, K. K. and Martinez, G. F., (2009), Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling, J. Hydrol., 377(1-2), 80-91 https://doi.org/10.1016/j.jhydrol.2009.08.003\nKnoben, W. J. M., Freer, J. E., and Woods, R. A. (2019) Technical note: Inherent benchmark or not? Comparing Nash–Sutcliffe and Kling–Gupta efficiency scores, Hydrol. Earth Syst. Sci., 23, 4323-4331 https://doi.org/10.5194/hess-23-4323-2019\n\n\n\n\nmetrics.lin_slope(obs, model, reg_method='ols')\nSlope of the regression line.\n\\[\nslope = \\frac{\\sum_{i=1}^n (model_i - \\overline {model})(obs_i - \\overline {obs})}\n                {\\sum_{i=1}^n (obs_i - \\overline {obs})^2}\n\\]\nRange: \\((-\\infty, \\infty )\\); Best: 1\n\n\n\nmetrics.mae(obs, model, weights=None)\nalias for mean_absolute_error\n\n\n\nmetrics.mape(obs, model)\nalias for mean_absolute_percentage_error\n\n\n\nmetrics.max_error(obs, model)\nMax (absolute) error\n\\[\nmax_{error} = max(|model_i - obs_i|)\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mean_absolute_error(obs, model, weights=None)\nMean Absolute Error (MAE)\n\\[\nMAE=\\frac{1}{n}\\sum_{i=1}^n|model_i - obs_i|\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mean_absolute_percentage_error(obs, model)\nMean Absolute Percentage Error (MAPE)\n\\[\nMAPE=\\frac{1}{n}\\sum_{i=1}^n\\frac{|model_i - obs_i|}{obs_i}*100\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.mef(obs, model)\nalias for model_efficiency_factor\n\n\n\nmetrics.metric_has_units(metric)\nCheck if a metric has units (dimension).\nSome metrics are dimensionless, others have the same dimension as the observations.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetric\nstr or callable\nMetric name or function\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbool\nTrue if metric has a dimension, False otherwise\n\n\n\n\n\n\n&gt;&gt;&gt; metric_has_units(\"rmse\")\nTrue\n&gt;&gt;&gt; metric_has_units(\"kge\")\nFalse\n\n\n\n\nmetrics.model_efficiency_factor(obs, model)\nModel Efficiency Factor (MEF)\nScale independent RMSE, standardized by Stdev of observations\n\\[\nMEF = \\frac{RMSE}{STDEV}=\\frac{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(model_i - obs_i)^2}}\n                                {\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(obs_i - \\overline{obs})^2}}=\\sqrt{1-NSE}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\nnash_sutcliffe_efficiency root_mean_squared_error\n\n\n\n\nmetrics.nash_sutcliffe_efficiency(obs, model)\nNash-Sutcliffe Efficiency (NSE)\n\\[\nNSE = 1 - \\frac {\\sum _{i=1}^{n}\\left(model_{i} - obs_{i}\\right)^{2}}\n                {\\sum_{i=1}^{n}\\left(obs_{i} - {\\overline{obs}}\\right)^{2}}\n\\]\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nr2 = nash_sutcliffe_efficiency(nse)\n\n\n\nNash, J. E.; Sutcliffe, J. V. (1970). “River flow forecasting through conceptual models part I — A discussion of principles”. Journal of Hydrology. 10 (3): 282–290. https://doi.org/10.1016/0022-1694(70)90255-6\n\n\n\n\nmetrics.nse(obs, model)\nalias for nash_sutcliffe_efficiency\n\n\n\nmetrics.peak_ratio(obs, model, inter_event_level=0.7, AAP=2, inter_event_time='36h')\nPeak Ratio\nPR is the mean of the individual ratios of identified peaks in the model / identified peaks in the measurements. PR is calculated only for the joint-events, ie, events that ocurr simulateneously within a window +/- 0.5*inter_event_time.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninter_event_level\nfloat\nInter-event level threshold (default: 0.7).\n0.7\n\n\nAAP\nint\nAverage Annual Peaks (ie, Number of peaks per year, on average). (default: 2)\n2\n\n\ninter_event_time\n\nMaximum time interval between peaks (default: 36 hours).\n'36h'\n\n\n\n\n\n\n\\(\\frac{\\sum_{i=1}^{N_{joint-peaks}} (\\frac{Peak_{model_i}}{Peak_{obs_i}} )}{N_{joint-peaks}}\\)\nRange: \\([0, \\infty)\\); Best: 1.0\n\n\n\n\nmetrics.pr(obs, model, inter_event_level=0.7, AAP=2, inter_event_time='36h')\nalias for peak_ratio\n\n\n\nmetrics.r2(obs, model)\nCoefficient of determination (R2)\nPronounced ‘R-squared’; the proportion of the variation in the dependent variable that is predictable from the independent variable(s), i.e. the proportion of explained variance.\n\\[\nR^2 = 1 - \\frac{\\sum_{i=1}^n (model_i - obs_i)^2}\n                {\\sum_{i=1}^n (obs_i - \\overline {obs})^2}\n\\]\nRange: \\((-\\infty, 1]\\); Best: 1\n\n\nr2 = nash_sutcliffe_efficiency(nse)\n\n\n\n&gt;&gt;&gt; obs = np.array([1.0,1.1,1.2,1.3,1.4])\n&gt;&gt;&gt; model = np.array([1.09, 1.16, 1.3 , 1.38, 1.49])\n&gt;&gt;&gt; r2(obs,model)\n0.6379999999999998\n\n\n\n\nmetrics.rho(obs, model)\nalias for spearmanr\n\n\n\nmetrics.rmse(obs, model, weights=None, unbiased=False)\nalias for root_mean_squared_error\n\n\n\nmetrics.root_mean_squared_error(obs, model, weights=None, unbiased=False)\nRoot Mean Squared Error (RMSE)\n\\[\nres_i = model_i - obs_i\n\\]\n\\[\nRMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_i^2}\n\\]\nUnbiased version:\n\\[\nres_{u,i} = res_i - \\overline {res}\n\\]\n\\[\nuRMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_{u,i}^2}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.scatter_index(obs, model)\nScatter index (SI)\nWhich is the same as the unbiased-RMSE normalized by the absolute mean of the observations.\n\\[\n\\frac{ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left( (model_i - \\overline {model}) - (obs_i - \\overline {obs}) \\right)^2} }\n{\\frac{1}{n} \\sum_{i=1}^n | obs_i | }\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\n\nmetrics.scatter_index2(obs, model)\nAlternative formulation of the scatter index (SI)\n\\[\n\\sqrt {\\frac{\\sum_{i=1}^n \\left( (model_i - \\overline {model}) - (obs_i - \\overline {obs}) \\right)^2}\n{\\sum_{i=1}^n obs_i^2}}\n\\]\nRange: [0, 100]; Best: 0\n\n\n\nmetrics.si(obs, model)\nalias for scatter_index\n\n\n\nmetrics.spearmanr(obs, model)\nSpearman rank correlation coefficient\nThe rank correlation coefficient is similar to the Pearson correlation coefficient but applied to ranked quantities and is useful to quantify a monotonous relationship\n\\[\n\\rho = \\frac{\\sum_{i=1}^n (rmodel_i - \\overline{rmodel})(robs_i - \\overline{robs}) }\n                {\\sqrt{\\sum_{i=1}^n (rmodel_i - \\overline{rmodel})^2}\n                \\sqrt{\\sum_{i=1}^n (robs_i - \\overline{robs})^2} }\n\\]\nRange: [-1, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.linspace(-20, 20, 100)\n&gt;&gt;&gt; mod = np.tanh(obs)\n&gt;&gt;&gt; rho(obs, mod)\n0.9999759973116955\n&gt;&gt;&gt; spearmanr(obs, mod)\n0.9999759973116955\n\n\n\ncorrcoef\n\n\n\n\nmetrics.urmse(obs, model, weights=None)\nUnbiased Root Mean Squared Error (uRMSE)\n\\[\nres_i = model_i - obs_i\n\\]\n\\[\nres_{u,i} = res_i - \\overline {res}\n\\]\n\\[\nuRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n res_{u,i}^2}\n\\]\nRange: \\([0, \\infty)\\); Best: 0\n\n\nroot_mean_squared_error\n\n\n\n\nmetrics.willmott(obs, model)\nWillmott’s Index of Agreement\nA scaled representation of the predictive accuracy of the model against observations. A value of 1 indicates a perfect match, and 0 indicates no agreement at all.\n\\[\nwillmott = 1 - \\frac{\\frac{1}{n} \\sum_{i=1}^n(model_i - obs_i)^2}\n                    {\\frac{1}{n} \\sum_{i=1}^n(|model_i - \\overline{obs}| + |obs_i - \\overline{obs}|)^2}\n\\]\nRange: [0, 1]; Best: 1\n\n\n&gt;&gt;&gt; obs = np.array([1.0, 1.1, 1.2, 1.3, 1.4, 1.4, 1.3])\n&gt;&gt;&gt; model = np.array([1.02, 1.16, 1.3, 1.38, 1.49, 1.45, 1.32])\n&gt;&gt;&gt; willmott(obs, model)\n0.9501403174479723\n\n\n\nWillmott, C. J. 1981. “On the validation of models”. Physical Geography, 2, 184–194."
  },
  {
    "objectID": "api/PointObservation.html",
    "href": "api/PointObservation.html",
    "title": "PointObservation",
    "section": "",
    "text": "PointObservation(self, data, *, item=None, x=None, y=None, z=None, name=None, weight=1.0, quantity=None, aux_items=None, attrs=None)\nClass for observations of fixed locations\nCreate a PointObservation from a dfs0 file or a pd.DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nitem\n(int, str)\nindex or name of the wanted item/column, by default None if data contains more than one item, item must be given\nNone\n\n\nx\nfloat\nx-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\ny\nfloat\ny-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nz-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"klagshamn.dfs0\", item=\"Water Level\", x=366844, y=6154291)\n&gt;&gt;&gt; o3 = ms.PointObservation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o4 = ms.PointObservation(df[\"Water Level\"], x=366844, y=6154291)"
  },
  {
    "objectID": "api/PointObservation.html#parameters",
    "href": "api/PointObservation.html#parameters",
    "title": "PointObservation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n(str, pathlib.Path, mikeio.mikeio.Dataset, mikeio.mikeio.DataArray, pandas.pandas.DataFrame, pandas.pandas.Series, xarray.xarray.Dataset or xarray.xarray.DataArray)\nfilename (.dfs0 or .nc) or object with the data\nrequired\n\n\nitem\n(int, str)\nindex or name of the wanted item/column, by default None if data contains more than one item, item must be given\nNone\n\n\nx\nfloat\nx-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\ny\nfloat\ny-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nz\nfloat\nz-coordinate of the observation point, inferred from data if not given, else None\nNone\n\n\nname\nstr\nuser-defined name for easy identification in plots etc, by default file basename\nNone\n\n\nquantity\nmodelskill.Quantity\nThe quantity of the observation, for validation with model results For MIKE dfs files this is inferred from the EUM information\nNone\n\n\naux_items\nlist\nlist of names or indices of auxiliary items, by default None\nNone\n\n\nattrs\ndict\nadditional attributes to be added to the data, by default None\nNone\n\n\nweight\nfloat\nweighting factor for skill scores, by default 1.0\n1.0"
  },
  {
    "objectID": "api/PointObservation.html#examples",
    "href": "api/PointObservation.html#examples",
    "title": "PointObservation",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o1 = ms.PointObservation(\"klagshamn.dfs0\", item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o2 = ms.PointObservation(\"klagshamn.dfs0\", item=\"Water Level\", x=366844, y=6154291)\n&gt;&gt;&gt; o3 = ms.PointObservation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o4 = ms.PointObservation(df[\"Water Level\"], x=366844, y=6154291)"
  },
  {
    "objectID": "api/DummyModelResult.html",
    "href": "api/DummyModelResult.html",
    "title": "DummyModelResult",
    "section": "",
    "text": "DummyModelResult\nDummyModelResult(name='dummy', data=None, strategy='constant')"
  },
  {
    "objectID": "api/TrackModelResult.html",
    "href": "api/TrackModelResult.html",
    "title": "TrackModelResult",
    "section": "",
    "text": "TrackModelResult(self, data, *, name=None, item=None, quantity=None, x_item=0, y_item=1, keep_duplicates='first', aux_items=None)\nConstruct a TrackModelResult from a dfs0 file, mikeio.Dataset, pandas.DataFrame or a xarray.Datasets\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.TrackType\nThe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nx_item\nstr | int | None\nItem of the first coordinate of positions, by default None\n0\n\n\ny_item\nstr | int | None\nItem of the second coordinate of positions, by default None\n1\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\nkeep_duplicates\n(str, bool)\nStrategy for handling duplicate timestamps (wraps xarray.Dataset.drop_duplicates) “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/TrackModelResult.html#parameters",
    "href": "api/TrackModelResult.html#parameters",
    "title": "TrackModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.TrackType\nThe input data or file path\nrequired\n\n\nname\ntyping.Optional[str]\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr | int | None\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nx_item\nstr | int | None\nItem of the first coordinate of positions, by default None\n0\n\n\ny_item\nstr | int | None\nItem of the second coordinate of positions, by default None\n1\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\nkeep_duplicates\n(str, bool)\nStrategy for handling duplicate timestamps (wraps xarray.Dataset.drop_duplicates) “first” to keep first occurrence, “last” to keep last occurrence, False to drop all duplicates, “offset” to add milliseconds to consecutive duplicates, by default “first”\n'first'\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/observation.html",
    "href": "api/observation.html",
    "title": "observation",
    "section": "",
    "text": "observation(data, *, gtype=None, **kwargs)\nA factory function for creating an appropriate observation object based on the data and args.\nIf ‘x’ or ‘y’ is given, a PointObservation is created. If ‘x_item’ or ‘y_item’ is given, a TrackObservation is created.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the Observation object.\nrequired\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\n\nAdditional keyword arguments to be passed to the Observation constructor.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o_pt = ms.observation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o_tr = ms.observation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)"
  },
  {
    "objectID": "api/observation.html#parameters",
    "href": "api/observation.html#parameters",
    "title": "observation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.DataInputType\nThe data to be used for creating the Observation object.\nrequired\n\n\ngtype\ntyping.Optional[typing.Literal[‘point’, ‘track’]]\nThe geometry type of the data. If not specified, it will be guessed from the data.\nNone\n\n\n**kwargs\n\nAdditional keyword arguments to be passed to the Observation constructor.\n{}"
  },
  {
    "objectID": "api/observation.html#examples",
    "href": "api/observation.html#examples",
    "title": "observation",
    "section": "",
    "text": "&gt;&gt;&gt; import modelskill as ms\n&gt;&gt;&gt; o_pt = ms.observation(df, item=0, x=366844, y=6154291, name=\"Klagshamn\")\n&gt;&gt;&gt; o_tr = ms.observation(\"lon_after_lat.dfs0\", item=\"wl\", x_item=1, y_item=0)"
  },
  {
    "objectID": "api/GridModelResult.html",
    "href": "api/GridModelResult.html",
    "title": "GridModelResult",
    "section": "",
    "text": "GridModelResult(self, data, *, name=None, item=None, quantity=None, aux_items=None)\nConstruct a GridModelResult from a file or xarray.Dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.GridType\nthe input data or file path\nrequired\n\n\nname\nstr\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr or int\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "api/GridModelResult.html#parameters",
    "href": "api/GridModelResult.html#parameters",
    "title": "GridModelResult",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nmodelskill.types.modelskill.types.GridType\nthe input data or file path\nrequired\n\n\nname\nstr\nThe name of the model result, by default None (will be set to file name or item name)\nNone\n\n\nitem\nstr or int\nIf multiple items/arrays are present in the input an item must be given (as either an index or a string), by default None\nNone\n\n\nquantity\nmodelskill.quantity.Quantity\nModel quantity, for MIKE files this is inferred from the EUM information\nNone\n\n\naux_items\ntyping.Optional[list[int | str]]\nAuxiliary items, by default None\nNone"
  },
  {
    "objectID": "examples/MIKE21HD_dfsu.html",
    "href": "examples/MIKE21HD_dfsu.html",
    "title": "MIKE21 HD",
    "section": "",
    "text": "import modelskill as ms\n\n\nmr = ms.model_result('../tests/testdata/Oresund2D.dfsu',\n                     item='Surface elevation')\nmr\n\n&lt;DfsuModelResult&gt;: Oresund2D\nTime: 2018-03-04 00:00:00 - 2018-03-10 22:40:00\nQuantity: Surface Elevation [m]\n\n\n\nfn = '../tests/testdata/smhi_2095_klagshamn.dfs0'\no1 = ms.PointObservation(fn, x=366844.15, y=6154291.6, item=0) \no1\n\n&lt;PointObservation&gt;: smhi_2095_klagshamn\nLocation: 366844.15, 6154291.6\nTime: 2015-01-01 01:00:00 - 2020-09-28 00:00:00\nQuantity: Water Level [m]\n\n\n\nms.plotting.spatial_overview(o1, mr, figsize=(8, 8));\n\n\n\n\n\n\n\n\n\ncmp = ms.match(o1, mr)\ncmp\n\n&lt;Comparer&gt;\nQuantity: Water Level [m]\nObservation: smhi_2095_klagshamn, n_points=167\nModel(s):\n0: Oresund2D\n\n\nMost use cases will compare many observed locations to a one or more models.\nIn this case we only have one observed location.\n\ncmp.plot.timeseries(figsize=(10,4));\n\n\n\n\n\n\n\n\n\nub_cmp = cmp.remove_bias()\nub_cmp.plot.timeseries(figsize=(10,4));\n\n\n\n\n\n\n\n\n\nub_cmp.score(\"bias\")\n\n{'Oresund2D': -2.9251385079944247e-17}\n\n\n\ncmp.score(\"bias\")\n\n{'Oresund2D': 0.1868744370781926}\n\n\n\ncmp.plot.timeseries()\n\n\n\n\n\n\n\n\nGet skill for a commonly used set of metrics\n\ncmp.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\nsmhi_2095_klagshamn\n167\n0.186874\n0.191303\n0.040924\n0.186874\n0.838306\n0.378995\n-5.505521\n\n\n\n\n\n\n\nOr choose specific metrics\n\ncmp.metrics = [\"bias\",\"rmse\"]\ncmp.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\nsmhi_2095_klagshamn\n167\n0.186874\n0.191303\n0.040924\n0.186874\n0.838306\n0.378995\n-5.505521\n\n\n\n\n\n\n\n\ncmp.plot.scatter(bins=0.02, cmap='YlOrRd', show_points=True);\n\n\n\n\n\n\n\n\n\ncmp.plot.scatter(skill_table=True, show_points=True);",
    "crumbs": [
      "Home",
      "Examples",
      "MIKE21 HD"
    ]
  },
  {
    "objectID": "api/ComparerCollection.html#parameters",
    "href": "api/ComparerCollection.html#parameters",
    "title": "ComparerCollection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncomparers\nlist of Comparer\nlist of comparers\nrequired"
  }
]