{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \n",
        "\n",
        "# Terminology\n",
        "\n",
        "ModelSkill is a library for assessing the skill of numerical models. It\n",
        "provides tools for comparing model results with observations, plotting\n",
        "the results and calculating validation metrics. This page defines some\n",
        "of the key terms used in the documentation.\n",
        "\n",
        "##  Skill\n",
        "\n",
        "**Skill** refers to the ability of a numerical model to accurately\n",
        "represent the real-world phenomenon it aims to simulate. It is a measure\n",
        "of how well the model performs in reproducing the observed system. Skill\n",
        "can be assessed using various metrics, such as accuracy, precision, and\n",
        "reliability, depending on the specific goals of the model and the nature\n",
        "of the data. In ModelSkill,\n",
        "[`skill`](../api/Comparer.html#modelskill.Comparer.skill) is also a\n",
        "specific method on [Comparer](../api/Comparer.html#modelskill.Comparer)\n",
        "objects that returns a\n",
        "[`SkillTable`](../api/SkillTable.html#modelskill.SkillTable) with\n",
        "aggregated skill scores per observation and model for a list of selected\n",
        "[metrics](../api/metrics.html#modelskill.metrics).\n",
        "\n",
        "##  Validation\n",
        "\n",
        "**Validation** is the process of assessing the model’s performance by\n",
        "comparing its output to real-world observations or data collected from\n",
        "the system being modeled. It helps ensure that the model accurately\n",
        "represents the system it simulates. Validation is typically performed\n",
        "before the model is used for prediction or decision-making.\n",
        "\n",
        "##  Calibration\n",
        "\n",
        "**Calibration** is the process of adjusting the model’s parameters or\n",
        "settings to improve its performance. It involves fine-tuning the model\n",
        "to better match observed data. Calibration aims to reduce discrepancies\n",
        "between model predictions and actual measurements. At the end of the\n",
        "calibration process, the calibrated model should be validated with\n",
        "independent data.\n",
        "\n",
        "##  Performance\n",
        "\n",
        "**Performance** is a measure of how well a numerical model operates in\n",
        "reproducing the observed system. It can be assessed using various\n",
        "metrics, such as accuracy, precision, and reliability, depending on the\n",
        "specific goals of the model and the nature of the data. In this context,\n",
        "**performance** is synonymous with **skill**.\n",
        "\n",
        "##  Timeseries\n",
        "\n",
        "A **timeseries** is a sequence of data points in time. In ModelSkill,\n",
        "The data can either be from [observations](#observation) or [model\n",
        "results](#model-result). Timeseries can univariate or multivariate;\n",
        "ModelSkill primarily supports univariate timeseries. Multivariate\n",
        "timeseries can be assessed one variable at a time. Timeseries can also\n",
        "have different spatial dimensions, such as point, track, line, or area.\n",
        "\n",
        "##  Observation\n",
        "\n",
        "An **observation** refers to real-world data or measurements collected\n",
        "from the system you are modeling. Observations serve as a reference for\n",
        "assessing the model’s performance. These data points are used to compare\n",
        "with the model’s predictions during validation and calibration.\n",
        "Observations are usually based on field measurements or laboratory\n",
        "experiments, but for the purposes of model validation, they can also be\n",
        "derived from other models (e.g. a reference model). ModelSkill supports\n",
        "[point](../api/PointModelResult.html#modelskill.PointModelResult) and\n",
        "[track](../api/TrackModelResult.html#modelskill.TrackModelResult)\n",
        "observation types.\n",
        "\n",
        "##  Measurement\n",
        "\n",
        "A **measurement** is called [observation](#observation) in ModelSkill.\n",
        "\n",
        "##  Model result\n",
        "\n",
        "A **model result** is the output of any type of numerical model. It is\n",
        "the data generated by the model during a simulation. Model results can\n",
        "be compared with observations to assess the model’s performance. In the\n",
        "context of validation, the term “model result” is often used\n",
        "interchangeably with “model output” or “model prediction”. ModelSkill\n",
        "supports\n",
        "[point](../api/PointModelResult.html#modelskill.PointModelResult),\n",
        "[track](../api/TrackModelResult.html#modelskill.TrackModelResult),\n",
        "[dfsu](../api/DfsuModelResult.html#modelskill.DfsuModelResult) and\n",
        "[grid](../api/GridModelResult.html#modelskill.GridModelResult) model\n",
        "result types.\n",
        "\n",
        "##  Metric\n",
        "\n",
        "A **metric** is a quantitative measure (a mathematical expression) used\n",
        "to evaluate the performance of a numerical model. Metrics provide a\n",
        "standardized way to assess the model’s accuracy, precision, and other\n",
        "attributes. A metric aggregates the skill of a model into a single\n",
        "number. See list of [metrics](../api/metrics.html#modelskill.metrics)\n",
        "supported by ModelSkill.\n",
        "\n",
        "##  Score\n",
        "\n",
        "A **score** is a numerical value that summarizes the model’s performance\n",
        "based on chosen metrics. Scores can be used to rank or compare different\n",
        "models or model configurations. In the context of validation, the “skill\n",
        "score” or “validation score” often quantifies the model’s overall\n",
        "performance. The score of a model is a single number, calculated as a\n",
        "weighted average for all time-steps, observations and variables. If you\n",
        "want to perform automated calibration, you can use the score as the\n",
        "objective function. In ModelSkill,\n",
        "[`score`](../api/ComparerCollection.html#modelskill.ComparerCollection.score)\n",
        "is also a specific method on\n",
        "[Comparer](../api/Comparer.html#modelskill.Comparer) objects that\n",
        "returns a single number aggregated score using a specific\n",
        "[metric](#metric).\n",
        "\n",
        "## Matched data\n",
        "\n",
        "In ModelSkill, observations and model results are *matched* when they\n",
        "refer to the same positions in space and time. If the\n",
        "[observations](#observation) and [model results](#model-result) are\n",
        "already matched, the\n",
        "[`from_matched`](../api/from_matched.html#modelskill.from_matched)\n",
        "function can be used to create a [Comparer](#comparer) directly.\n",
        "Otherwise, the [match](#match) function can be used to match the\n",
        "observations and model results in space and time.\n",
        "\n",
        "##  match()\n",
        "\n",
        "The function [`match`](../api/match.html#modelskill.match) is used to\n",
        "match a model result with observations. It returns a\n",
        "[`Comparer`](../api/Comparer.html#modelskill.Comparer) object or a\n",
        "[`ComparerCollection`](../api/ComparerCollection.html#modelskill.ComparerCollection)\n",
        "object.\n",
        "\n",
        "## Comparer\n",
        "\n",
        "A [**Comparer**](../api/Comparer.html#modelskill.Comparer) is an object\n",
        "that stores the matched observation and model result data for a *single*\n",
        "observation. It is used to calculate validation metrics and generate\n",
        "plots. A Comparer can be created using the\n",
        "[`match`](../api/match.html#modelskill.match) function.\n",
        "\n",
        "## ComparerCollection\n",
        "\n",
        "A\n",
        "[**ComparerCollection**](../api/ComparerCollection.html#modelskill.ComparerCollection)\n",
        "is a collection of [Comparer](#comparer)s. It is used to compare\n",
        "*multiple* observations with one or more model results. A\n",
        "ComparerCollection can be created using the\n",
        "[`match`](../api/match.html#modelskill.match) function or by passing a\n",
        "list of Comparers to the\n",
        "[`ComparerCollection`](../api/ComparerCollection.html#modelskill.ComparerCollection)\n",
        "constructor.\n",
        "\n",
        "## Connector\n",
        "\n",
        "In past versions of FMSkill/ModelSkill, the Connector class was used to\n",
        "connect observations and model results. This class has been deprecated\n",
        "and is no longer in use.\n",
        "\n",
        "## Abbreviations\n",
        "\n",
        "| Abbreviation  | Meaning              |\n",
        "|---------------|----------------------|\n",
        "| `ms`          | ModelSkill           |\n",
        "| `o` or `obs`  | Observation          |\n",
        "| `mr` or `mod` | Model result         |\n",
        "| `cmp`         | `Comparer`           |\n",
        "| `cc`          | `ComparerCollection` |\n",
        "| `sk`          | `SkillTable`         |\n",
        "| `mtr`         | Metric               |\n",
        "| `q`           | `Quantity`           |"
      ],
      "id": "a393056d-1b83-4f2a-bff8-21c5a31d4280"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}