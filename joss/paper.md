---
title: "`modelskill` : A Python package for model skill assessment"
tags: 
    - "Python"
    - "model skill assessment"
    - "model evaluation"
    - "model validation"
authors:
    - name: Jesper Sandvig Mariegaard
      orcid: 0009-0000-0118-3302
      affiliation: "1"
    - name: Henrik Andersson
      orcid: 0000-0003-1752-8584
      affiliation: "1"
    - name: Daniel Caichac
      orcid: 0000-0000-0000-0000
      affiliation: "1"
    - name: Ryan Walter Murray
      orcid: 0000-0000-0000-0000
      affiliation: "1"
affiliations:
 - name: DHI
   index: 1
date: 1 January 2024
---

# Summary

'`modelskill`' is a Python package for model skill assessment. It is designed to be used in conjunction with the MIKE FM modelling software, but can also be used for other modelling software. The package is designed to be easy to use and to provide a wide range of functionality for model skill assessment. The package is open source and available on GitHub.

# Statement of need

Quantifying the skill of a model is an important part of the modelling workflow. The skill of a model can be assessed in many different ways, and there are many different metrics that can be used. The '`modelskill`' package provides a wide range of functionality for model skill assessment, including many different metrics and plots.

The package is designed to be easy to use to encourage modelers to use it. 
Many of the functions are designed to be used with a single line of code.
For speficic use cases, the evaluation can be easily be extended with custom metrics and configurable plots.

